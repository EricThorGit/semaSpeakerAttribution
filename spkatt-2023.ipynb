{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set gpus for qlora training\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0aae6f7aa91c407eb821a1b8fd9622f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device map for 7b model\n",
    "device_map = {\n",
    "    \"model.embed_tokens\": 0,\n",
    "    \"model.layers.0\": 0,\n",
    "    \"model.layers.1\": 0,\n",
    "    \"model.layers.2\": 0,\n",
    "    \"model.layers.3\": 0,\n",
    "    \"model.layers.4\": 0,\n",
    "    \"model.layers.5\": 0,\n",
    "    \"model.layers.6\": 0,\n",
    "    \"model.layers.7\": 0,\n",
    "    \"model.layers.8\": 0,\n",
    "    \"model.layers.9\": 0,\n",
    "    \"model.layers.10\": 0,\n",
    "    \"model.layers.11\": 0,\n",
    "    \"model.layers.12\": 0,\n",
    "    \"model.layers.13\": 0,\n",
    "    \"model.layers.14\": 0,\n",
    "    \"model.layers.15\": 0,\n",
    "    \"model.layers.16\": 0,\n",
    "    \"model.layers.17\": 0,\n",
    "    \"model.layers.18\": 0,\n",
    "    \"model.layers.19\": 0,\n",
    "    \"model.layers.20\": 0,\n",
    "    \"model.layers.21\": 0,\n",
    "    \"model.layers.22\": 0,\n",
    "    \"model.layers.23\": 0,\n",
    "    \"model.layers.24\": 0,\n",
    "    \"model.layers.25\": 0,\n",
    "    \"model.layers.26\": 0,\n",
    "    \"model.layers.27\": 0,\n",
    "    \"model.layers.28\": 0,\n",
    "    \"model.layers.29\": 0,\n",
    "    \"model.layers.30\": 0,\n",
    "    \"model.layers.31\": 0,\n",
    "    \"model.norm\": 0,\n",
    "    \"lm_head\": 0,\n",
    "}\n",
    "\n",
    "# device map for 70b model\n",
    "# device_map = {\n",
    "#     \"model.embed_tokens\": 0,\n",
    "#     \"model.layers.0\": 0,\n",
    "#     \"model.layers.1\": 0,\n",
    "#     \"model.layers.2\": 0,\n",
    "#     \"model.layers.3\": 0,\n",
    "#     \"model.layers.4\": 0,\n",
    "#     \"model.layers.5\": 0,\n",
    "#     \"model.layers.6\": 0,\n",
    "#     \"model.layers.7\": 0,\n",
    "#     \"model.layers.8\": 0,\n",
    "#     \"model.layers.9\": 0,\n",
    "#     \"model.layers.10\": 0,\n",
    "#     \"model.layers.11\": 0,\n",
    "#     \"model.layers.12\": 0,\n",
    "#     \"model.layers.13\": 0,\n",
    "#     \"model.layers.14\": 0,\n",
    "#     \"model.layers.15\": 0,\n",
    "#     \"model.layers.16\": 0,\n",
    "#     \"model.layers.17\": 0,\n",
    "#     \"model.layers.18\": 1,\n",
    "#     \"model.layers.19\": 1,\n",
    "#     \"model.layers.20\": 1,\n",
    "#     \"model.layers.21\": 1,\n",
    "#     \"model.layers.22\": 1,\n",
    "#     \"model.layers.23\": 1,\n",
    "#     \"model.layers.24\": 1,\n",
    "#     \"model.layers.25\": 1,\n",
    "#     \"model.layers.26\": 1,\n",
    "#     \"model.layers.27\": 1,\n",
    "#     \"model.layers.28\": 1,\n",
    "#     \"model.layers.29\": 1,\n",
    "#     \"model.layers.30\": 1,\n",
    "#     \"model.layers.31\": 1,\n",
    "#     \"model.layers.32\": 1,\n",
    "#     \"model.layers.33\": 1,\n",
    "#     \"model.layers.34\": 1,\n",
    "#     \"model.layers.35\": 1,\n",
    "#     \"model.layers.36\": 1,\n",
    "#     \"model.layers.37\": 1,\n",
    "#     \"model.layers.38\": 1,\n",
    "#     \"model.layers.39\": 2,\n",
    "#     \"model.layers.40\": 2,\n",
    "#     \"model.layers.41\": 2,\n",
    "#     \"model.layers.42\": 2,\n",
    "#     \"model.layers.43\": 2,\n",
    "#     \"model.layers.44\": 2,\n",
    "#     \"model.layers.45\": 2,\n",
    "#     \"model.layers.46\": 2,\n",
    "#     \"model.layers.47\": 2,\n",
    "#     \"model.layers.48\": 2,\n",
    "#     \"model.layers.49\": 2,\n",
    "#     \"model.layers.50\": 2,\n",
    "#     \"model.layers.51\": 2,\n",
    "#     \"model.layers.52\": 2,\n",
    "#     \"model.layers.53\": 2,\n",
    "#     \"model.layers.54\": 2,\n",
    "#     \"model.layers.55\": 2,\n",
    "#     \"model.layers.56\": 2,\n",
    "#     \"model.layers.57\": 2,\n",
    "#     \"model.layers.58\": 2,\n",
    "#     \"model.layers.59\": 2,\n",
    "#     \"model.layers.60\": 3,\n",
    "#     \"model.layers.61\": 3,\n",
    "#     \"model.layers.62\": 3,\n",
    "#     \"model.layers.63\": 3,\n",
    "#     \"model.layers.64\": 3,\n",
    "#     \"model.layers.65\": 3,\n",
    "#     \"model.layers.66\": 3,\n",
    "#     \"model.layers.67\": 3,\n",
    "#     \"model.layers.68\": 3,\n",
    "#     \"model.layers.69\": 3,\n",
    "#     \"model.layers.70\": 3,\n",
    "#     \"model.layers.71\": 3,\n",
    "#     \"model.layers.72\": 3,\n",
    "#     \"model.layers.73\": 3,\n",
    "#     \"model.layers.74\": 3,\n",
    "#     \"model.layers.75\": 3,\n",
    "#     \"model.layers.76\": 3,\n",
    "#     \"model.layers.77\": 3,\n",
    "#     \"model.layers.78\": 3,\n",
    "#     \"model.layers.79\": 3,\n",
    "#     \"model.norm\": 3,\n",
    "#     \"lm_head\": 3,\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eth/semaSpeakerAttribution/venv/lib/python3.8/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import warnings\n",
    "import gc\n",
    "import torch\n",
    "import json\n",
    "import Levenshtein\n",
    "import shutil\n",
    "\n",
    "from datasets import (\n",
    "    load_dataset,\n",
    "    concatenate_datasets,\n",
    "    load_from_disk,\n",
    "    Features,\n",
    "    Sequence,\n",
    "    Value,\n",
    ")\n",
    "from transformers import AutoTokenizer, pipeline, AutoModelForCausalLM, LlamaTokenizer\n",
    "from langchain import HuggingFacePipeline, PromptTemplate, LLMChain\n",
    "from peft import PeftModel\n",
    "\n",
    "from datasets import logging as ds_logging\n",
    "from transformers import logging as trans_logging\n",
    "\n",
    "from qlora import train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_logging.set_verbosity_error()\n",
    "ds_logging.disable_progress_bar()\n",
    "trans_logging.set_verbosity_error()\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_annotations_from_file(path: str, file: str):\n",
    "    features = Features(\n",
    "        {\n",
    "            \"PTC\": Sequence(feature=Value(dtype=\"string\", id=None), length=-1, id=None),\n",
    "            \"Evidence\": Sequence(\n",
    "                feature=Value(dtype=\"string\", id=None), length=-1, id=None\n",
    "            ),\n",
    "            \"Medium\": Sequence(\n",
    "                feature=Value(dtype=\"string\", id=None), length=-1, id=None\n",
    "            ),\n",
    "            \"Topic\": Sequence(\n",
    "                feature=Value(dtype=\"string\", id=None), length=-1, id=None\n",
    "            ),\n",
    "            \"Cue\": Sequence(feature=Value(dtype=\"string\", id=None), length=-1, id=None),\n",
    "            \"Addr\": Sequence(\n",
    "                feature=Value(dtype=\"string\", id=None), length=-1, id=None\n",
    "            ),\n",
    "            \"Message\": Sequence(\n",
    "                feature=Value(dtype=\"string\", id=None), length=-1, id=None\n",
    "            ),\n",
    "            \"Source\": Sequence(\n",
    "                feature=Value(dtype=\"string\", id=None), length=-1, id=None\n",
    "            ),\n",
    "        }\n",
    "    )\n",
    "    ds = load_dataset(\n",
    "        \"json\",\n",
    "        data_files=os.path.join(path, file),\n",
    "        field=\"Annotations\",\n",
    "        split=\"train\",\n",
    "        features=features,\n",
    "    )\n",
    "    ds = ds.add_column(\"FileName\", [file] * len(ds))\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sentences_from_file(path: str, file: str):\n",
    "    ds = load_dataset(\n",
    "        \"json\", data_files=os.path.join(path, file), field=\"Sentences\", split=\"train\"\n",
    "    )\n",
    "    ds = ds.add_column(\"FileName\", [file] * len(ds))\n",
    "    ds = ds.add_column(\"Sentence\", [\" \".join(t) for t in ds[\"Tokens\"]])\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_annotations_from_path(path: str):\n",
    "    dataset = None\n",
    "\n",
    "    for file in tqdm(sorted(os.listdir(path))):\n",
    "        if not dataset:\n",
    "            dataset = read_annotations_from_file(path, file)\n",
    "        else:\n",
    "            dataset = concatenate_datasets(\n",
    "                [dataset, read_annotations_from_file(path, file)]\n",
    "            )\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sentences_from_path(path: str):\n",
    "    dataset = None\n",
    "\n",
    "    for file in tqdm(sorted(os.listdir(path))):\n",
    "        if not dataset:\n",
    "            dataset = read_sentences_from_file(path, file)\n",
    "        else:\n",
    "            dataset = concatenate_datasets(\n",
    "                [dataset, read_sentences_from_file(path, file)]\n",
    "            )\n",
    "\n",
    "    dataset = dataset.add_column(\"id\", range(len(dataset)))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sentences_dataset(ds_name: str):\n",
    "    path_to_dataset = \"./transformed_datasets/\" + ds_name + \"/sentences\"\n",
    "\n",
    "    if os.path.isdir(path_to_dataset):\n",
    "        result = load_from_disk(path_to_dataset)\n",
    "    else:\n",
    "        result = read_sentences_from_path(\n",
    "            \"./SpkAtt-2023/data/\"\n",
    "            + ds_name\n",
    "            + \"/task1\"\n",
    "            + (\"_test/\" if ds_name == \"eval\" else \"/\")\n",
    "        )\n",
    "        os.makedirs(path_to_dataset, exist_ok=True)\n",
    "        result.save_to_disk(path_to_dataset)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_annotations_dataset(ds_name: str):\n",
    "    path_to_dataset = \"./transformed_datasets/\" + ds_name + \"/annotations\"\n",
    "\n",
    "    if os.path.isdir(path_to_dataset):\n",
    "        return load_from_disk(path_to_dataset)\n",
    "\n",
    "    result = read_annotations_from_path(\n",
    "        \"./SpkAtt-2023/data/\"\n",
    "        + ds_name\n",
    "        + \"/task1\"\n",
    "        + (\"_test/\" if ds_name == \"eval\" else \"/\")\n",
    "    )\n",
    "    os.makedirs(path_to_dataset, exist_ok=True)\n",
    "    result.save_to_disk(path_to_dataset)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences_dataset = read_sentences_dataset(\"train\")\n",
    "val_sentences_dataset = read_sentences_dataset(\"dev\")\n",
    "test_sentences_dataset = read_sentences_dataset(\"eval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_annotations_dataset = read_annotations_dataset(\"train\")\n",
    "val_annotations_dataset = read_annotations_dataset(\"dev\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format datasets for usage in langchain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_from_label(train_sentences_dataset, row, annotations):\n",
    "    tokens = []\n",
    "    for anno in annotations:\n",
    "        if int(anno.split(\":\")[0]) == row[\"SentenceId\"]:\n",
    "            tokens.append(row[\"Tokens\"][int(anno.split(\":\")[1])])\n",
    "        else:\n",
    "            temp_row = train_sentences_dataset.filter(\n",
    "                lambda r: r[\"FileName\"] == row[\"FileName\"]\n",
    "                and r[\"SentenceId\"] == int(anno.split(\":\")[0])\n",
    "            )[0]\n",
    "            tokens.append(temp_row[\"Tokens\"][int(anno.split(\":\")[1])])\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_complete_dataset(sentences_dataset, annotations_dataset, dataset_name):\n",
    "    path_to_dataset = \"./transformed_datasets/\" + dataset_name + \"/complete\"\n",
    "    if os.path.isdir(path_to_dataset):\n",
    "        return load_from_disk(path_to_dataset)\n",
    "\n",
    "    ptc, ptc_temp, ptc_mapped, ptc_mapped_temp = [], [], [], []\n",
    "    evidence, evidence_temp, evidence_mapped, evidence_mapped_temp = [], [], [], []\n",
    "    medium, medium_temp, medium_mapped, medium_mapped_temp = [], [], [], []\n",
    "    topic, topic_temp, topic_mapped, topic_mapped_temp = [], [], [], []\n",
    "    cue, cue_temp, cue_mapped, cue_mapped_temp = [], [], [], []\n",
    "    addr, addr_temp, addr_mapped, addr_mapped_temp = [], [], [], []\n",
    "    message, message_temp, message_mapped, message_mapped_temp = [], [], [], []\n",
    "    source, source_temp, source_mapped, source_mapped_temp = [], [], [], []\n",
    "    (\n",
    "        sentence_extended,\n",
    "        tokens_extended,\n",
    "        sentence_extended_ids,\n",
    "    ) = (\n",
    "        [],\n",
    "        [],\n",
    "        [],\n",
    "    )\n",
    "\n",
    "    index_in_anno_ds = 0\n",
    "\n",
    "    for i, row in tqdm(enumerate(sentences_dataset)):\n",
    "        context = row[\"Sentence\"]\n",
    "        tokens = row[\"Tokens\"]\n",
    "        ids = [row[\"SentenceId\"]] * len(row[\"Tokens\"])\n",
    "        if (\n",
    "            i + 1 < len(sentences_dataset)\n",
    "            and sentences_dataset[i + 1][\"FileName\"] == row[\"FileName\"]\n",
    "        ):\n",
    "            context = context + \" \" + sentences_dataset[i + 1][\"Sentence\"]\n",
    "            tokens.extend(sentences_dataset[i + 1][\"Tokens\"])\n",
    "            ids.extend(\n",
    "                [sentences_dataset[i + 1][\"SentenceId\"]]\n",
    "                * len(sentences_dataset[i + 1][\"Tokens\"])\n",
    "            )\n",
    "        if (\n",
    "            i + 2 < len(sentences_dataset)\n",
    "            and sentences_dataset[i + 2][\"FileName\"] == row[\"FileName\"]\n",
    "        ):\n",
    "            context = context + \" \" + sentences_dataset[i + 2][\"Sentence\"]\n",
    "            tokens.extend(sentences_dataset[i + 2][\"Tokens\"])\n",
    "            ids.extend(\n",
    "                [sentences_dataset[i + 2][\"SentenceId\"]]\n",
    "                * len(sentences_dataset[i + 2][\"Tokens\"])\n",
    "            )\n",
    "        sentence_extended.append(context)\n",
    "        tokens_extended.append(tokens)\n",
    "        sentence_extended_ids.append(ids)\n",
    "\n",
    "        if annotations_dataset is not None:\n",
    "            id_of_next_sentence_with_annotation = (\n",
    "                int(annotations_dataset[index_in_anno_ds][\"Cue\"][0].split(\":\")[0])\n",
    "                if index_in_anno_ds != len(annotations_dataset)\n",
    "                else -1\n",
    "            )\n",
    "\n",
    "            if row[\"SentenceId\"] != id_of_next_sentence_with_annotation:\n",
    "                ptc.append([])\n",
    "                ptc_mapped.append([])\n",
    "                evidence.append([])\n",
    "                evidence_mapped.append([])\n",
    "                medium.append([])\n",
    "                medium_mapped.append([])\n",
    "                topic.append([])\n",
    "                topic_mapped.append([])\n",
    "                cue.append([])\n",
    "                cue_mapped.append([])\n",
    "                addr.append([])\n",
    "                addr_mapped.append([])\n",
    "                message.append([])\n",
    "                message_mapped.append([])\n",
    "                source.append([])\n",
    "                source_mapped.append([])\n",
    "                continue\n",
    "\n",
    "            while row[\"SentenceId\"] == id_of_next_sentence_with_annotation:\n",
    "                ptc_temp.append(annotations_dataset[index_in_anno_ds][\"PTC\"])\n",
    "                evidence_temp.append(annotations_dataset[index_in_anno_ds][\"Evidence\"])\n",
    "                medium_temp.append(annotations_dataset[index_in_anno_ds][\"Medium\"])\n",
    "                topic_temp.append(annotations_dataset[index_in_anno_ds][\"Topic\"])\n",
    "                cue_temp.append(annotations_dataset[index_in_anno_ds][\"Cue\"])\n",
    "                addr_temp.append(annotations_dataset[index_in_anno_ds][\"Addr\"])\n",
    "                message_temp.append(annotations_dataset[index_in_anno_ds][\"Message\"])\n",
    "                source_temp.append(annotations_dataset[index_in_anno_ds][\"Source\"])\n",
    "\n",
    "                ptc_mapped_temp.append(\n",
    "                    get_text_from_label(sentences_dataset, row, ptc_temp[-1])\n",
    "                )\n",
    "                evidence_mapped_temp.append(\n",
    "                    get_text_from_label(sentences_dataset, row, evidence_temp[-1])\n",
    "                )\n",
    "                medium_mapped_temp.append(\n",
    "                    get_text_from_label(sentences_dataset, row, medium_temp[-1])\n",
    "                )\n",
    "                topic_mapped_temp.append(\n",
    "                    get_text_from_label(sentences_dataset, row, topic_temp[-1])\n",
    "                )\n",
    "                cue_mapped_temp.append(\n",
    "                    get_text_from_label(sentences_dataset, row, cue_temp[-1])\n",
    "                )\n",
    "                addr_mapped_temp.append(\n",
    "                    get_text_from_label(sentences_dataset, row, addr_temp[-1])\n",
    "                )\n",
    "                message_mapped_temp.append(\n",
    "                    get_text_from_label(sentences_dataset, row, message_temp[-1])\n",
    "                )\n",
    "                source_mapped_temp.append(\n",
    "                    get_text_from_label(sentences_dataset, row, source_temp[-1])\n",
    "                )\n",
    "\n",
    "                index_in_anno_ds += 1\n",
    "                if index_in_anno_ds == len(annotations_dataset):\n",
    "                    break\n",
    "                id_of_next_sentence_with_annotation = int(\n",
    "                    annotations_dataset[index_in_anno_ds][\"Cue\"][0].split(\":\")[0]\n",
    "                )\n",
    "\n",
    "            ptc.append(ptc_temp)\n",
    "            ptc_mapped.append(ptc_mapped_temp)\n",
    "            evidence.append(evidence_temp)\n",
    "            evidence_mapped.append(evidence_mapped_temp)\n",
    "            medium.append(medium_temp)\n",
    "            medium_mapped.append(medium_mapped_temp)\n",
    "            topic.append(topic_temp)\n",
    "            topic_mapped.append(topic_mapped_temp)\n",
    "            cue.append(cue_temp)\n",
    "            cue_mapped.append(cue_mapped_temp)\n",
    "            addr.append(addr_temp)\n",
    "            addr_mapped.append(addr_mapped_temp)\n",
    "            message.append(message_temp)\n",
    "            message_mapped.append(message_mapped_temp)\n",
    "            source.append(source_temp)\n",
    "            source_mapped.append(source_mapped_temp)\n",
    "\n",
    "            ptc_temp, ptc_mapped_temp = [], []\n",
    "            evidence_temp, evidence_mapped_temp = [], []\n",
    "            medium_temp, medium_mapped_temp = [], []\n",
    "            topic_temp, topic_mapped_temp = [], []\n",
    "            cue_temp, cue_mapped_temp = [], []\n",
    "            addr_temp, addr_mapped_temp = [], []\n",
    "            message_temp, message_mapped_temp = [], []\n",
    "            source_temp, source_mapped_temp = [], []\n",
    "\n",
    "    res = sentences_dataset.add_column(\"sentence_extended\", sentence_extended)\n",
    "    res = res.add_column(\"tokens_extended\", tokens_extended)\n",
    "    res = res.add_column(\"sentence_extended_ids\", sentence_extended_ids)\n",
    "\n",
    "    if annotations_dataset is not None:\n",
    "        res = res.add_column(\"ptc\", ptc)\n",
    "        res = res.add_column(\"ptc_mapped\", ptc_mapped)\n",
    "        res = res.add_column(\"evidence\", evidence)\n",
    "        res = res.add_column(\"evidence_mapped\", evidence_mapped)\n",
    "        res = res.add_column(\"medium\", medium)\n",
    "        res = res.add_column(\"medium_mapped\", medium_mapped)\n",
    "        res = res.add_column(\"topic\", topic)\n",
    "        res = res.add_column(\"topic_mapped\", topic_mapped)\n",
    "        res = res.add_column(\"cue\", cue)\n",
    "        res = res.add_column(\"cue_mapped\", cue_mapped)\n",
    "        res = res.add_column(\"addr\", addr)\n",
    "        res = res.add_column(\"addr_mapped\", addr_mapped)\n",
    "        res = res.add_column(\"message\", message)\n",
    "        res = res.add_column(\"message_mapped\", message_mapped)\n",
    "        res = res.add_column(\"source\", source)\n",
    "        res = res.add_column(\"source_mapped\", source_mapped)\n",
    "\n",
    "    os.makedirs(path_to_dataset, exist_ok=True)\n",
    "    res.save_to_disk(path_to_dataset)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = build_complete_dataset(\n",
    "    train_sentences_dataset, train_annotations_dataset, \"train\"\n",
    ")\n",
    "val_ds = build_complete_dataset(val_sentences_dataset, val_annotations_dataset, \"dev\")\n",
    "test_ds = build_complete_dataset(test_sentences_dataset, None, \"eval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = test_sentences_dataset.rename_column(\"Sentence\", \"Satz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Showcase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Tokens': ['-',\n",
       "  'Letzter',\n",
       "  'Redner',\n",
       "  'in',\n",
       "  'der',\n",
       "  'Debatte',\n",
       "  ':',\n",
       "  'Bernd',\n",
       "  'Westphal',\n",
       "  'fÃ¼r',\n",
       "  'die',\n",
       "  'SPD-Fraktion',\n",
       "  '.'],\n",
       " 'SentenceId': 52,\n",
       " 'FileName': '19002_Zusatzpunkt_3_CDUCSU_Jung_ID19209800_21.11.2017.json',\n",
       " 'Sentence': '- Letzter Redner in der Debatte : Bernd Westphal fÃ¼r die SPD-Fraktion .',\n",
       " 'id': 52,\n",
       " 'sentence_extended': '- Letzter Redner in der Debatte : Bernd Westphal fÃ¼r die SPD-Fraktion .',\n",
       " 'tokens_extended': ['-',\n",
       "  'Letzter',\n",
       "  'Redner',\n",
       "  'in',\n",
       "  'der',\n",
       "  'Debatte',\n",
       "  ':',\n",
       "  'Bernd',\n",
       "  'Westphal',\n",
       "  'fÃ¼r',\n",
       "  'die',\n",
       "  'SPD-Fraktion',\n",
       "  '.'],\n",
       " 'sentence_extended_ids': [52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52],\n",
       " 'ptc': [[]],\n",
       " 'ptc_mapped': [[]],\n",
       " 'evidence': [[]],\n",
       " 'evidence_mapped': [[]],\n",
       " 'medium': [[]],\n",
       " 'medium_mapped': [[]],\n",
       " 'topic': [[]],\n",
       " 'topic_mapped': [[]],\n",
       " 'cue': [['52:5']],\n",
       " 'cue_mapped': [['Debatte']],\n",
       " 'addr': [[]],\n",
       " 'addr_mapped': [[]],\n",
       " 'message': [[]],\n",
       " 'message_mapped': [[]],\n",
       " 'source': [[]],\n",
       " 'source_mapped': [[]]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[52]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Tokens': ['Dazu',\n",
       "  'muss',\n",
       "  'man',\n",
       "  'nur',\n",
       "  'mit',\n",
       "  'den',\n",
       "  'Landwirten',\n",
       "  'sprechen',\n",
       "  ',',\n",
       "  'die',\n",
       "  'sagen',\n",
       "  ':',\n",
       "  'Ja',\n",
       "  ',',\n",
       "  'auch',\n",
       "  'frÃ¼her',\n",
       "  'gab',\n",
       "  'es',\n",
       "  'extreme',\n",
       "  'Ereignisse',\n",
       "  ',',\n",
       "  'auch',\n",
       "  'frÃ¼her',\n",
       "  'gab',\n",
       "  'es',\n",
       "  'Naturkatastrophen',\n",
       "  ',',\n",
       "  'aber',\n",
       "  'in',\n",
       "  'einem',\n",
       "  'Jahr',\n",
       "  'den',\n",
       "  'Hagel',\n",
       "  ',',\n",
       "  'im',\n",
       "  'anderen',\n",
       "  'Jahr',\n",
       "  'eine',\n",
       "  'DÃ¼rre',\n",
       "  'und',\n",
       "  'im',\n",
       "  'dritten',\n",
       "  'Jahr',\n",
       "  ',',\n",
       "  'wie',\n",
       "  'in',\n",
       "  'diesem',\n",
       "  'Jahr',\n",
       "  ',',\n",
       "  'die',\n",
       "  'FrostschÃ¤den',\n",
       "  ',',\n",
       "  'unter',\n",
       "  'denen',\n",
       "  'die',\n",
       "  'Obstbauern',\n",
       "  'zu',\n",
       "  'leiden',\n",
       "  'hatten',\n",
       "  ',',\n",
       "  'diese',\n",
       "  'HÃ¤ufung',\n",
       "  'hatten',\n",
       "  'wir',\n",
       "  'frÃ¼her',\n",
       "  'so',\n",
       "  'nicht',\n",
       "  ',',\n",
       "  'also',\n",
       "  'tut',\n",
       "  'etwas',\n",
       "  'gegen',\n",
       "  'den',\n",
       "  'Klimawandel',\n",
       "  '.'],\n",
       " 'SentenceId': 15,\n",
       " 'FileName': '19002_Zusatzpunkt_3_CDUCSU_Jung_ID19209800_21.11.2017.json',\n",
       " 'Sentence': 'Dazu muss man nur mit den Landwirten sprechen , die sagen : Ja , auch frÃ¼her gab es extreme Ereignisse , auch frÃ¼her gab es Naturkatastrophen , aber in einem Jahr den Hagel , im anderen Jahr eine DÃ¼rre und im dritten Jahr , wie in diesem Jahr , die FrostschÃ¤den , unter denen die Obstbauern zu leiden hatten , diese HÃ¤ufung hatten wir frÃ¼her so nicht , also tut etwas gegen den Klimawandel .',\n",
       " 'id': 15,\n",
       " 'sentence_extended': 'Dazu muss man nur mit den Landwirten sprechen , die sagen : Ja , auch frÃ¼her gab es extreme Ereignisse , auch frÃ¼her gab es Naturkatastrophen , aber in einem Jahr den Hagel , im anderen Jahr eine DÃ¼rre und im dritten Jahr , wie in diesem Jahr , die FrostschÃ¤den , unter denen die Obstbauern zu leiden hatten , diese HÃ¤ufung hatten wir frÃ¼her so nicht , also tut etwas gegen den Klimawandel . Es geht um unsere wirtschaftlichen Existenzen . Damit ist die Aufgabe , vor der wir stehen , beschrieben .',\n",
       " 'tokens_extended': ['Dazu',\n",
       "  'muss',\n",
       "  'man',\n",
       "  'nur',\n",
       "  'mit',\n",
       "  'den',\n",
       "  'Landwirten',\n",
       "  'sprechen',\n",
       "  ',',\n",
       "  'die',\n",
       "  'sagen',\n",
       "  ':',\n",
       "  'Ja',\n",
       "  ',',\n",
       "  'auch',\n",
       "  'frÃ¼her',\n",
       "  'gab',\n",
       "  'es',\n",
       "  'extreme',\n",
       "  'Ereignisse',\n",
       "  ',',\n",
       "  'auch',\n",
       "  'frÃ¼her',\n",
       "  'gab',\n",
       "  'es',\n",
       "  'Naturkatastrophen',\n",
       "  ',',\n",
       "  'aber',\n",
       "  'in',\n",
       "  'einem',\n",
       "  'Jahr',\n",
       "  'den',\n",
       "  'Hagel',\n",
       "  ',',\n",
       "  'im',\n",
       "  'anderen',\n",
       "  'Jahr',\n",
       "  'eine',\n",
       "  'DÃ¼rre',\n",
       "  'und',\n",
       "  'im',\n",
       "  'dritten',\n",
       "  'Jahr',\n",
       "  ',',\n",
       "  'wie',\n",
       "  'in',\n",
       "  'diesem',\n",
       "  'Jahr',\n",
       "  ',',\n",
       "  'die',\n",
       "  'FrostschÃ¤den',\n",
       "  ',',\n",
       "  'unter',\n",
       "  'denen',\n",
       "  'die',\n",
       "  'Obstbauern',\n",
       "  'zu',\n",
       "  'leiden',\n",
       "  'hatten',\n",
       "  ',',\n",
       "  'diese',\n",
       "  'HÃ¤ufung',\n",
       "  'hatten',\n",
       "  'wir',\n",
       "  'frÃ¼her',\n",
       "  'so',\n",
       "  'nicht',\n",
       "  ',',\n",
       "  'also',\n",
       "  'tut',\n",
       "  'etwas',\n",
       "  'gegen',\n",
       "  'den',\n",
       "  'Klimawandel',\n",
       "  '.',\n",
       "  'Es',\n",
       "  'geht',\n",
       "  'um',\n",
       "  'unsere',\n",
       "  'wirtschaftlichen',\n",
       "  'Existenzen',\n",
       "  '.',\n",
       "  'Damit',\n",
       "  'ist',\n",
       "  'die',\n",
       "  'Aufgabe',\n",
       "  ',',\n",
       "  'vor',\n",
       "  'der',\n",
       "  'wir',\n",
       "  'stehen',\n",
       "  ',',\n",
       "  'beschrieben',\n",
       "  '.'],\n",
       " 'sentence_extended_ids': [15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  16,\n",
       "  16,\n",
       "  16,\n",
       "  16,\n",
       "  16,\n",
       "  16,\n",
       "  16,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17],\n",
       " 'ptc': [[], []],\n",
       " 'ptc_mapped': [[], []],\n",
       " 'evidence': [[], []],\n",
       " 'evidence_mapped': [[], []],\n",
       " 'medium': [[], []],\n",
       " 'medium_mapped': [[], []],\n",
       " 'topic': [[], []],\n",
       " 'topic_mapped': [[], []],\n",
       " 'cue': [['15:7'], ['15:10']],\n",
       " 'cue_mapped': [['sprechen'], ['sagen']],\n",
       " 'addr': [['15:4', '15:5', '15:6'], []],\n",
       " 'addr_mapped': [['mit', 'den', 'Landwirten'], []],\n",
       " 'message': [[],\n",
       "  ['15:12',\n",
       "   '15:13',\n",
       "   '15:14',\n",
       "   '15:15',\n",
       "   '15:16',\n",
       "   '15:17',\n",
       "   '15:18',\n",
       "   '15:19',\n",
       "   '15:20',\n",
       "   '15:21',\n",
       "   '15:22',\n",
       "   '15:23',\n",
       "   '15:24',\n",
       "   '15:25',\n",
       "   '15:26',\n",
       "   '15:27',\n",
       "   '15:28',\n",
       "   '15:29',\n",
       "   '15:30',\n",
       "   '15:31',\n",
       "   '15:32',\n",
       "   '15:33',\n",
       "   '15:34',\n",
       "   '15:35',\n",
       "   '15:36',\n",
       "   '15:37',\n",
       "   '15:38',\n",
       "   '15:39',\n",
       "   '15:40',\n",
       "   '15:41',\n",
       "   '15:42',\n",
       "   '15:43',\n",
       "   '15:44',\n",
       "   '15:45',\n",
       "   '15:46',\n",
       "   '15:47',\n",
       "   '15:48',\n",
       "   '15:49',\n",
       "   '15:50',\n",
       "   '15:51',\n",
       "   '15:52',\n",
       "   '15:53',\n",
       "   '15:54',\n",
       "   '15:55',\n",
       "   '15:56',\n",
       "   '15:57',\n",
       "   '15:58',\n",
       "   '15:59',\n",
       "   '15:60',\n",
       "   '15:61',\n",
       "   '15:62',\n",
       "   '15:63',\n",
       "   '15:64',\n",
       "   '15:65',\n",
       "   '15:66',\n",
       "   '15:67',\n",
       "   '15:68',\n",
       "   '15:69',\n",
       "   '15:70',\n",
       "   '15:71',\n",
       "   '15:72',\n",
       "   '15:73',\n",
       "   '15:74',\n",
       "   '16:0',\n",
       "   '16:1',\n",
       "   '16:2',\n",
       "   '16:3',\n",
       "   '16:4',\n",
       "   '16:5']],\n",
       " 'message_mapped': [[],\n",
       "  ['Ja',\n",
       "   ',',\n",
       "   'auch',\n",
       "   'frÃ¼her',\n",
       "   'gab',\n",
       "   'es',\n",
       "   'extreme',\n",
       "   'Ereignisse',\n",
       "   ',',\n",
       "   'auch',\n",
       "   'frÃ¼her',\n",
       "   'gab',\n",
       "   'es',\n",
       "   'Naturkatastrophen',\n",
       "   ',',\n",
       "   'aber',\n",
       "   'in',\n",
       "   'einem',\n",
       "   'Jahr',\n",
       "   'den',\n",
       "   'Hagel',\n",
       "   ',',\n",
       "   'im',\n",
       "   'anderen',\n",
       "   'Jahr',\n",
       "   'eine',\n",
       "   'DÃ¼rre',\n",
       "   'und',\n",
       "   'im',\n",
       "   'dritten',\n",
       "   'Jahr',\n",
       "   ',',\n",
       "   'wie',\n",
       "   'in',\n",
       "   'diesem',\n",
       "   'Jahr',\n",
       "   ',',\n",
       "   'die',\n",
       "   'FrostschÃ¤den',\n",
       "   ',',\n",
       "   'unter',\n",
       "   'denen',\n",
       "   'die',\n",
       "   'Obstbauern',\n",
       "   'zu',\n",
       "   'leiden',\n",
       "   'hatten',\n",
       "   ',',\n",
       "   'diese',\n",
       "   'HÃ¤ufung',\n",
       "   'hatten',\n",
       "   'wir',\n",
       "   'frÃ¼her',\n",
       "   'so',\n",
       "   'nicht',\n",
       "   ',',\n",
       "   'also',\n",
       "   'tut',\n",
       "   'etwas',\n",
       "   'gegen',\n",
       "   'den',\n",
       "   'Klimawandel',\n",
       "   '.',\n",
       "   'Es',\n",
       "   'geht',\n",
       "   'um',\n",
       "   'unsere',\n",
       "   'wirtschaftlichen',\n",
       "   'Existenzen']],\n",
       " 'source': [['15:2'], ['15:9']],\n",
       " 'source_mapped': [['man'], ['die']]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build lmsys format json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_cues_to_string(mapped):\n",
    "    if mapped == []:\n",
    "        return \"#UNK#\"\n",
    "    return \", \".join([\"[\" + \", \".join(val) + \"]\" for val in mapped])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_roles_to_string(mapped):\n",
    "    if mapped == []:\n",
    "        return \"#UNK#\"\n",
    "    return \", \".join(mapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmsys_data_path = \"./lmsys.json\"\n",
    "\n",
    "\n",
    "def build_lmsys_format(train_ds, val_ds):\n",
    "    result = []\n",
    "\n",
    "    index = 0\n",
    "    for row in concatenate_datasets([train_ds, val_ds]):\n",
    "        if len(row[\"cue_mapped\"]) == 0:\n",
    "            element = {\"id\": \"identity_\" + str(index)}\n",
    "            index += 1\n",
    "            conversations = [\n",
    "                {\n",
    "                    \"from\": \"human\",\n",
    "                    \"value\": 'A cue is the lexical items in a sentence that indicate that speech, writing, or thought is being reproduced.\\nI want you to extract all cues in the text below.\\nIf you find multiple words for one cue, you output them separated by commas.\\nIf no cue can be found in the given text, you output the string #UNK# as cue.\\nNow extract all cues from the following sentence.\\nUse the prefix \"Cues: \".\\nSentence: '\n",
    "                    + row[\"Sentence\"],\n",
    "                },\n",
    "                {\n",
    "                    \"from\": \"gpt\",\n",
    "                    \"value\": \"Cues: \" + map_cues_to_string(row[\"cue_mapped\"]),\n",
    "                },\n",
    "            ]\n",
    "            element[\"conversations\"] = conversations\n",
    "            result.append(element)\n",
    "            continue\n",
    "        for i, cue in enumerate(row[\"cue_mapped\"]):\n",
    "            element = {\"id\": \"identity_\" + str(index)}\n",
    "            index += 1\n",
    "            conversations = [\n",
    "                {\n",
    "                    \"from\": \"human\",\n",
    "                    \"value\": 'A cue is the lexical items in a sentence that indicate that speech, writing, or thought is being reproduced.\\nI want you to extract all cues in the text below.\\nIf you find multiple words for one cue, you output them separated by commas.\\nIf no cue can be found in the given text, you output the string #UNK# as cue.\\nNow extract all cues from the following sentence.\\nUse the prefix \"Cues: \".\\nSentence: '\n",
    "                    + row[\"Sentence\"],\n",
    "                },\n",
    "                {\n",
    "                    \"from\": \"gpt\",\n",
    "                    \"value\": \"Cues: \" + map_cues_to_string(row[\"cue_mapped\"]),\n",
    "                },\n",
    "                {\n",
    "                    \"from\": \"human\",\n",
    "                    \"value\": \"Now I give you again the sentence only in addition with the two following sentences, because the roles can be partially contained in the following sentences.\\nText: \"\n",
    "                    + row[\"sentence_extended\"]\n",
    "                    + \"\\n\\nNow find all roles in the sentence associated with the cue '\"\n",
    "                    + \", \".join(cue)\n",
    "                    + \"' you found in the beginning sentence.\",\n",
    "                },\n",
    "                {\n",
    "                    \"from\": \"gpt\",\n",
    "                    \"value\": \"cue: \"\n",
    "                    + \", \".join(cue)\n",
    "                    + \"\\nptc: \"\n",
    "                    + map_roles_to_string(row[\"ptc_mapped\"][i])\n",
    "                    + \"\\nevidence: \"\n",
    "                    + map_roles_to_string(row[\"evidence_mapped\"][i])\n",
    "                    + \"\\nmedium: \"\n",
    "                    + map_roles_to_string(row[\"medium_mapped\"][i])\n",
    "                    + \"\\ntopic: \"\n",
    "                    + map_roles_to_string(row[\"topic_mapped\"][i])\n",
    "                    + \"\\naddr: \"\n",
    "                    + map_roles_to_string(row[\"addr_mapped\"][i])\n",
    "                    + \"\\nmessage: \"\n",
    "                    + map_roles_to_string(row[\"message_mapped\"][i])\n",
    "                    + \"\\nsource: \"\n",
    "                    + map_roles_to_string(row[\"source_mapped\"][i]),\n",
    "                },\n",
    "            ]\n",
    "            element[\"conversations\"] = conversations\n",
    "            result.append(element)\n",
    "\n",
    "    with open(lmsys_data_path, \"w\", encoding=\"utf8\") as outfile:\n",
    "        json.dump(result, outfile, indent=3, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_lmsys_format(train_ds, val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QLoRA Fine-Tuning\n",
    "\n",
    "## Parse data into required format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parsed_cues_file = \"./transformed_datasets/prompts_training/parsed_data_cues.jsonl\"\n",
    "parsed_roles_file = \"./transformed_datasets/prompts_training/parsed_data_roles.jsonl\"\n",
    "os.makedirs(os.path.dirname(parsed_cues_file), exist_ok=True)\n",
    "os.makedirs(os.path.dirname(parsed_roles_file), exist_ok=True)\n",
    "\n",
    "# token to signal the end of the assistant's response\n",
    "separator = \"</s>\"\n",
    "\n",
    "# reload parsed data\n",
    "with open(lmsys_data_path) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# save parsed prompts separately\n",
    "all_prompts_cues = []\n",
    "all_prompts_roles = []\n",
    "for conversation in data:\n",
    "    # keep track of the complete conversation in order to generate the input of the prompts\n",
    "    complete_prompt = \"\"\n",
    "\n",
    "    for i, turn in enumerate(conversation[\"conversations\"]):\n",
    "        if turn[\"from\"] == \"human\":\n",
    "            complete_prompt += \"User: \"\n",
    "            complete_prompt += turn[\"value\"]\n",
    "        elif turn[\"from\"] == \"gpt\":\n",
    "            complete_prompt += \"Assistant: \"\n",
    "\n",
    "            # idea\n",
    "            # turn 0: user prompt for cues\n",
    "            # turn 1: assistant response with cues\n",
    "            #   --> create sample with the conversation up to this point as input and the cues as output\n",
    "            # turn 2: user prompt for roles for one specific cue\n",
    "            # turn 3: assistant response with roles\n",
    "            #   --> create sample with the conversation up to this point as input and the roles as output\n",
    "            # there should be no further turns because we split all conversations with multiple cues into separate conversations\n",
    "\n",
    "            sample = json.dumps(\n",
    "                {\"input\": complete_prompt, \"output\": turn[\"value\"] + separator}\n",
    "            )\n",
    "\n",
    "            if i == 1 and sample not in all_prompts_cues:\n",
    "                # turn 1: assistant response with cues\n",
    "                all_prompts_cues.append(sample)\n",
    "            elif i == 3 and sample not in all_prompts_cues:\n",
    "                # turn 3: assistant response with roles\n",
    "                all_prompts_roles.append(sample)\n",
    "            elif i != 1 and i != 3:\n",
    "                print(\n",
    "                    \"ERROR: each conversation should maximally contain 4 turns\"\n",
    "                    \" and only turn 1 and 3 should be responses by the assistant\"\n",
    "                )\n",
    "\n",
    "            complete_prompt += turn[\"value\"] + separator\n",
    "        complete_prompt += \"\\n\"\n",
    "\n",
    "# write parsed prompts to files\n",
    "with open(parsed_cues_file, \"w\") as f:\n",
    "    f.write(\"\\n\".join(all_prompts_cues))\n",
    "\n",
    "with open(parsed_roles_file, \"w\") as f:\n",
    "    f.write(\"\\n\".join(all_prompts_roles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check that the file with the cue prompts was written correctly\n",
    "with open(parsed_cues_file) as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "print(f\"Number of samples: {len(lines)}\\n\")\n",
    "\n",
    "print(\"First 5 samples:\")\n",
    "for l in lines[:5]:\n",
    "    print(\"=== in: ===\\n\" + json.loads(l)[\"input\"] + \"\\n\")\n",
    "    print(\"=== out: ===\\n\" + json.loads(l)[\"output\"] + \"\\n\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check that the file with the role prompts was written correctly\n",
    "with open(parsed_roles_file) as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "print(f\"Number of samples: {len(lines)}\\n\")\n",
    "\n",
    "print(\"First 5 samples:\")\n",
    "for l in lines[:5]:\n",
    "    print(\"=== in: ===\\n\" + json.loads(l)[\"input\"] + \"\\n\")\n",
    "    print(\"=== out: ===\\n\" + json.loads(l)[\"output\"] + \"\\n\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Check optimal source and target lengths\n",
    "\n",
    "This step is only required if you want to use your own data. If you use the original GermEval 2023 task 1 data, you can skip this step and use the source and target lengths that are already defined in the configurations below at the start of the training code (parameters `source_max_len` and `target_max_len`).\n",
    "\n",
    "If you want to change the maximum source or target lengths, keep in mind that longer prompts mean longer training times and more memory requirements. While it would be best to set the maximum source/target lengths to the maximum lengths of the inputs/outputs, this is not always feasible due to memory constraints. In this case, we recommend choosing maximum lengths that only truncate few samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# encode all prompt inputs with the Llama 1 tokenizer (same as the Llama 2 tokenizer)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"huggyllama/llama-7b\", padding_side=\"right\", use_fast=False, tokenizer_type=\"llama\"\n",
    ")\n",
    "\n",
    "encoded_inputs_cues = []\n",
    "encoded_inputs_roles = []\n",
    "encoded_outputs_cues = []\n",
    "encoded_outputs_roles = []\n",
    "with open(parsed_cues_file) as f:\n",
    "    for l in f.readlines():\n",
    "        enc_in = tokenizer.encode(json.loads(l)[\"input\"])\n",
    "        encoded_inputs_cues.append(enc_in)\n",
    "        enc_out = tokenizer.encode(json.loads(l)[\"output\"])\n",
    "        encoded_outputs_cues.append(enc_out)\n",
    "with open(parsed_roles_file) as f:\n",
    "    for l in f.readlines():\n",
    "        enc_in = tokenizer.encode(json.loads(l)[\"input\"])\n",
    "        encoded_inputs_roles.append(enc_in)\n",
    "        enc_out = tokenizer.encode(json.loads(l)[\"output\"])\n",
    "        encoded_outputs_roles.append(enc_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# maximum source lengths taken from the config files\n",
    "max_length_source_cues = 256\n",
    "max_length_source_roles = 640\n",
    "\n",
    "print(\"cues source lengths\")\n",
    "len_enc = [len(e) for e in encoded_inputs_cues]\n",
    "print(f\"max length: {max(len_enc)}\")\n",
    "print(f\"mean length: {np.mean(len_enc)}\")\n",
    "print(\n",
    "    f\"number of samples longer than {max_length_source_cues}: {sum(np.array(len_enc) > max_length_source_cues)}\"\n",
    ")\n",
    "print()\n",
    "\n",
    "print(\"roles source lengths\")\n",
    "len_enc = [len(e) for e in encoded_inputs_roles]\n",
    "print(f\"max length: {max(len_enc)}\")\n",
    "print(f\"mean length: {np.mean(len_enc)}\")\n",
    "print(\n",
    "    f\"number of samples longer than {max_length_source_roles}: {sum(np.array(len_enc) > max_length_source_roles)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# maximum target lengths taken from the config files\n",
    "max_length_target_cues = 64\n",
    "max_length_target_roles = 256\n",
    "\n",
    "print(\"cues target lengths\")\n",
    "len_enc = [len(e) for e in encoded_outputs_cues]\n",
    "print(f\"max length: {max(len_enc)}\")\n",
    "print(f\"mean length: {np.mean(len_enc)}\")\n",
    "print(\n",
    "    f\"number of samples longer than {max_length_target_cues}: {sum(np.array(len_enc) > max_length_target_cues)}\"\n",
    ")\n",
    "print()\n",
    "\n",
    "print(\"roles target lengths\")\n",
    "len_enc = [len(e) for e in encoded_outputs_roles]\n",
    "print(f\"max length: {max(len_enc)}\")\n",
    "print(f\"mean length: {np.mean(len_enc)}\")\n",
    "print(\n",
    "    f\"number of samples longer than {max_length_target_roles}: {sum(np.array(len_enc) > max_length_target_roles)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Train models\n",
    "\n",
    "This step can be skipped if you already have trained models.\n",
    "\n",
    "For training, you first have to prepare the Llama 2 models and adapt the configuration. To prepare the Llama 2 models, you will have to make them accessible in HF (Huggingface) format. You can either use the models directly from Huggingface or prepare them yourself by first downloading the model weights from [the official Llama repo](https://github.com/facebookresearch/llama) and then converting these weights using their [conversion manual](https://github.com/facebookresearch/llama-recipes/#model-conversion-to-hugging-face). When using the models from Huggingface, you should add the parameter `use_auth_token` with your Huggingface token to the training configs in the code cell below. If you don't want to use the models from Huggingface, once you have prepared the models yourself, update the path to the models in the config (parameter `model_name_or_path`) so the paths point to the folder containing the `pytorch_model-000xx-of-00015.bin` files.\n",
    "\n",
    "Further configuration parameters:\n",
    "\n",
    "- `per_device_train_batch_size` and `gradient_accumulation_steps`: With these two parameters you can control the batch size and the number of accumulation steps when calculating the gradients during training. Larger batch sizes should speed up training, but increase memory requirements considerably. We recommend choosing the parameters so that their product `per_device_train_batch_size * gradient_accumulation_steps` is a multiple of 16.\n",
    "- `save_steps` and `max_steps`: set `max_steps` to control the length of training (`save_steps` determines when checkpoints are created)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define config files for training\n",
    "# 7B models\n",
    "cues_training_config = {\n",
    "    \"model_name_or_path\": \"meta-llama/Llama-2-7b-hf\",\n",
    "    \"output_dir\": \"./output/spkatt-13b-cues-leolm\",\n",
    "    \"data_seed\": 42,\n",
    "    \"save_steps\": 500,\n",
    "    \"evaluation_strategy\": \"no\",\n",
    "    \"dataloader_num_workers\": 4,\n",
    "    \"lora_modules\": \"all\",\n",
    "    \"bf16\": True,\n",
    "    \"dataset\": \"transformed_datasets/prompts_training/parsed_data_cues.jsonl\",\n",
    "    \"dataset_format\": \"input-output\",\n",
    "    \"source_max_len\": 256,\n",
    "    \"target_max_len\": 64,\n",
    "    \"per_device_train_batch_size\": 16,\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"max_steps\": 2000,\n",
    "    \"learning_rate\": 0.0002,\n",
    "    \"lora_dropout\": 0.1,\n",
    "    \"seed\": 0,\n",
    "}\n",
    "roles_training_config = {\n",
    "    \"model_name_or_path\": \"meta-llama/Llama-2-7b-hf\",\n",
    "    \"output_dir\": \"./output/spkatt-13b-roles-leolm\",\n",
    "    \"data_seed\": 42,\n",
    "    \"save_steps\": 500,\n",
    "    \"evaluation_strategy\": \"no\",\n",
    "    \"dataloader_num_workers\": 4,\n",
    "    \"lora_modules\": \"all\",\n",
    "    \"bf16\": True,\n",
    "    \"dataset\": \"transformed_datasets/prompts_training/parsed_data_roles.jsonl\",\n",
    "    \"dataset_format\": \"input-output\",\n",
    "    \"source_max_len\": 640,\n",
    "    \"target_max_len\": 256,\n",
    "    \"per_device_train_batch_size\": 16,\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"max_steps\": 2000,\n",
    "    \"learning_rate\": 0.0002,\n",
    "    \"lora_dropout\": 0.1,\n",
    "    \"seed\": 0,\n",
    "}\n",
    "\n",
    "# 70B models\n",
    "# cues_training_config = {\"model_name_or_path\": \"meta-llama/Llama-2-70b-hf\",\n",
    "#                         \"output_dir\": \"./output/spkatt-70b-cues\",\n",
    "#                         \"data_seed\": 42,\n",
    "#                         \"save_steps\": 500,\n",
    "#                         \"evaluation_strategy\": \"no\",\n",
    "#                         \"dataloader_num_workers\": 4,\n",
    "#                         \"lora_modules\": \"all\",\n",
    "#                         \"bf16\": True,\n",
    "#                         \"dataset\": \"transformed_datasets/prompts_training/parsed_data_cues.jsonl\",\n",
    "#                         \"dataset_format\": \"input-output\",\n",
    "#                         \"source_max_len\": 256,\n",
    "#                         \"target_max_len\": 64,\n",
    "#                         \"per_device_train_batch_size\": 16,\n",
    "#                         \"gradient_accumulation_steps\": 1,\n",
    "#                         \"max_steps\": 2000,\n",
    "#                         \"learning_rate\": 0.0001,\n",
    "#                         \"lora_dropout\": 0.05,\n",
    "#                         \"seed\": 0,\n",
    "#                         }\n",
    "# roles_training_config = {\"model_name_or_path\": \"meta-llama/Llama-2-70b-hf\",\n",
    "#                          \"output_dir\": \"./output/spkatt-70b-roles\",\n",
    "#                          \"data_seed\": 42,\n",
    "#                          \"save_steps\": 500,\n",
    "#                          \"evaluation_strategy\": \"no\",\n",
    "#                          \"dataloader_num_workers\": 4,\n",
    "#                          \"lora_modules\": \"all\",\n",
    "#                          \"bf16\": True,\n",
    "#                          \"dataset\": \"transformed_datasets/prompts_training/parsed_data_roles.jsonl\",\n",
    "#                          \"dataset_format\": \"input-output\",\n",
    "#                          \"source_max_len\": 640,\n",
    "#                          \"target_max_len\": 256,\n",
    "#                          \"per_device_train_batch_size\": 8,\n",
    "#                          \"gradient_accumulation_steps\": 2,\n",
    "#                          \"max_steps\": 2500,\n",
    "#                          \"learning_rate\": 0.0001,\n",
    "#                          \"lora_dropout\": 0.05,\n",
    "#                          \"seed\": 0,\n",
    "#                          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "meta-llama/Llama-2-7b-hf is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/semaSpeakerAttribution/venv/lib/python3.8/site-packages/huggingface_hub/utils/_errors.py:269\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 269\u001b[0m     response\u001b[39m.\u001b[39;49mraise_for_status()\n\u001b[1;32m    270\u001b[0m \u001b[39mexcept\u001b[39;00m HTTPError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/semaSpeakerAttribution/venv/lib/python3.8/site-packages/requests/models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m \u001b[39mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1021\u001b[0m     \u001b[39mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mGatedRepoError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/semaSpeakerAttribution/venv/lib/python3.8/site-packages/transformers/utils/hub.py:417\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    416\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 417\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[1;32m    418\u001b[0m         path_or_repo_id,\n\u001b[1;32m    419\u001b[0m         filename,\n\u001b[1;32m    420\u001b[0m         subfolder\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m \u001b[39mif\u001b[39;49;00m \u001b[39mlen\u001b[39;49m(subfolder) \u001b[39m==\u001b[39;49m \u001b[39m0\u001b[39;49m \u001b[39melse\u001b[39;49;00m subfolder,\n\u001b[1;32m    421\u001b[0m         repo_type\u001b[39m=\u001b[39;49mrepo_type,\n\u001b[1;32m    422\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    423\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    424\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    425\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    426\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    427\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    428\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m    429\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    430\u001b[0m     )\n\u001b[1;32m    432\u001b[0m \u001b[39mexcept\u001b[39;00m RepositoryNotFoundError:\n",
      "File \u001b[0;32m~/semaSpeakerAttribution/venv/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/semaSpeakerAttribution/venv/lib/python3.8/site-packages/huggingface_hub/file_download.py:1360\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, endpoint, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[1;32m   1358\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(head_call_error, RepositoryNotFoundError) \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(head_call_error, GatedRepoError):\n\u001b[1;32m   1359\u001b[0m     \u001b[39m# Repo not found => let's raise the actual error\u001b[39;00m\n\u001b[0;32m-> 1360\u001b[0m     \u001b[39mraise\u001b[39;00m head_call_error\n\u001b[1;32m   1361\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1362\u001b[0m     \u001b[39m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
      "File \u001b[0;32m~/semaSpeakerAttribution/venv/lib/python3.8/site-packages/huggingface_hub/file_download.py:1233\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, endpoint, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[1;32m   1232\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1233\u001b[0m     metadata \u001b[39m=\u001b[39m get_hf_file_metadata(\n\u001b[1;32m   1234\u001b[0m         url\u001b[39m=\u001b[39;49murl,\n\u001b[1;32m   1235\u001b[0m         token\u001b[39m=\u001b[39;49mtoken,\n\u001b[1;32m   1236\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m   1237\u001b[0m         timeout\u001b[39m=\u001b[39;49metag_timeout,\n\u001b[1;32m   1238\u001b[0m     )\n\u001b[1;32m   1239\u001b[0m \u001b[39mexcept\u001b[39;00m EntryNotFoundError \u001b[39mas\u001b[39;00m http_error:\n\u001b[1;32m   1240\u001b[0m     \u001b[39m# Cache the non-existence of the file and raise\u001b[39;00m\n",
      "File \u001b[0;32m~/semaSpeakerAttribution/venv/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/semaSpeakerAttribution/venv/lib/python3.8/site-packages/huggingface_hub/file_download.py:1622\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout)\u001b[0m\n\u001b[1;32m   1613\u001b[0m r \u001b[39m=\u001b[39m _request_wrapper(\n\u001b[1;32m   1614\u001b[0m     method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mHEAD\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1615\u001b[0m     url\u001b[39m=\u001b[39murl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1620\u001b[0m     timeout\u001b[39m=\u001b[39mtimeout,\n\u001b[1;32m   1621\u001b[0m )\n\u001b[0;32m-> 1622\u001b[0m hf_raise_for_status(r)\n\u001b[1;32m   1624\u001b[0m \u001b[39m# Return\u001b[39;00m\n",
      "File \u001b[0;32m~/semaSpeakerAttribution/venv/lib/python3.8/site-packages/huggingface_hub/utils/_errors.py:285\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    282\u001b[0m     message \u001b[39m=\u001b[39m (\n\u001b[1;32m    283\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mresponse\u001b[39m.\u001b[39mstatus_code\u001b[39m}\u001b[39;00m\u001b[39m Client Error.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCannot access gated repo for url \u001b[39m\u001b[39m{\u001b[39;00mresponse\u001b[39m.\u001b[39murl\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m     )\n\u001b[0;32m--> 285\u001b[0m     \u001b[39mraise\u001b[39;00m GatedRepoError(message, response) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[39melif\u001b[39;00m (\n\u001b[1;32m    288\u001b[0m     response\u001b[39m.\u001b[39mstatus_code \u001b[39m==\u001b[39m \u001b[39m401\u001b[39m\n\u001b[1;32m    289\u001b[0m     \u001b[39mand\u001b[39;00m response\u001b[39m.\u001b[39mrequest\u001b[39m.\u001b[39murl \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    292\u001b[0m     \u001b[39m# Collection not found. We don't raise a custom error for this.\u001b[39;00m\n\u001b[1;32m    293\u001b[0m     \u001b[39m# This prevent from raising a misleading `RepositoryNotFoundError` (see below).\u001b[39;00m\n",
      "\u001b[0;31mGatedRepoError\u001b[0m: 401 Client Error. (Request ID: Root=1-654b5e26-0fba916535d91fd07f32318a;043a990c-b639-40af-8f87-a68c3188e950)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json.\nRepo model meta-llama/Llama-2-7b-hf is gated. You must be authenticated to access it.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m/home/eth/semaSpeakerAttribution/spkatt-2023.ipynb Cell 43\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bdgx01.fb09.fh-aachen.de/home/eth/semaSpeakerAttribution/spkatt-2023.ipynb#X56sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m train(cues_training_config)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdgx01.fb09.fh-aachen.de/home/eth/semaSpeakerAttribution/spkatt-2023.ipynb#X56sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# free vram after training\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdgx01.fb09.fh-aachen.de/home/eth/semaSpeakerAttribution/spkatt-2023.ipynb#X56sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m gc\u001b[39m.\u001b[39mcollect()\n",
      "File \u001b[0;32m~/semaSpeakerAttribution/qlora.py:674\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(custom_args)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[39mif\u001b[39;00m completed_training:\n\u001b[1;32m    672\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mDetected that training was already completed!\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 674\u001b[0m model, tokenizer \u001b[39m=\u001b[39m get_accelerate_model(args, checkpoint_dir)\n\u001b[1;32m    676\u001b[0m model\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_cache \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    677\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mloaded model\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/semaSpeakerAttribution/qlora.py:280\u001b[0m, in \u001b[0;36mget_accelerate_model\u001b[0;34m(args, checkpoint_dir)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mloading base model \u001b[39m\u001b[39m{\u001b[39;00margs\u001b[39m.\u001b[39mmodel_name_or_path\u001b[39m}\u001b[39;00m\u001b[39m...\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    279\u001b[0m compute_dtype \u001b[39m=\u001b[39m (torch\u001b[39m.\u001b[39mfloat16 \u001b[39mif\u001b[39;00m args\u001b[39m.\u001b[39mfp16 \u001b[39melse\u001b[39;00m (torch\u001b[39m.\u001b[39mbfloat16 \u001b[39mif\u001b[39;00m args\u001b[39m.\u001b[39mbf16 \u001b[39melse\u001b[39;00m torch\u001b[39m.\u001b[39mfloat32))\n\u001b[0;32m--> 280\u001b[0m model \u001b[39m=\u001b[39m AutoModelForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    281\u001b[0m     args\u001b[39m.\u001b[39;49mmodel_name_or_path,\n\u001b[1;32m    282\u001b[0m     cache_dir\u001b[39m=\u001b[39;49margs\u001b[39m.\u001b[39;49mcache_dir,\n\u001b[1;32m    283\u001b[0m     load_in_4bit\u001b[39m=\u001b[39;49margs\u001b[39m.\u001b[39;49mbits \u001b[39m==\u001b[39;49m \u001b[39m4\u001b[39;49m,\n\u001b[1;32m    284\u001b[0m     load_in_8bit\u001b[39m=\u001b[39;49margs\u001b[39m.\u001b[39;49mbits \u001b[39m==\u001b[39;49m \u001b[39m8\u001b[39;49m,\n\u001b[1;32m    285\u001b[0m     device_map\u001b[39m=\u001b[39;49mdevice_map,\n\u001b[1;32m    286\u001b[0m     max_memory\u001b[39m=\u001b[39;49mmax_memory,\n\u001b[1;32m    287\u001b[0m     quantization_config\u001b[39m=\u001b[39;49mBitsAndBytesConfig(\n\u001b[1;32m    288\u001b[0m         load_in_4bit\u001b[39m=\u001b[39;49margs\u001b[39m.\u001b[39;49mbits \u001b[39m==\u001b[39;49m \u001b[39m4\u001b[39;49m,\n\u001b[1;32m    289\u001b[0m         load_in_8bit\u001b[39m=\u001b[39;49margs\u001b[39m.\u001b[39;49mbits \u001b[39m==\u001b[39;49m \u001b[39m8\u001b[39;49m,\n\u001b[1;32m    290\u001b[0m         llm_int8_threshold\u001b[39m=\u001b[39;49m\u001b[39m6.0\u001b[39;49m,\n\u001b[1;32m    291\u001b[0m         llm_int8_has_fp16_weight\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    292\u001b[0m         bnb_4bit_compute_dtype\u001b[39m=\u001b[39;49mcompute_dtype,\n\u001b[1;32m    293\u001b[0m         bnb_4bit_use_double_quant\u001b[39m=\u001b[39;49margs\u001b[39m.\u001b[39;49mdouble_quant,\n\u001b[1;32m    294\u001b[0m         bnb_4bit_quant_type\u001b[39m=\u001b[39;49margs\u001b[39m.\u001b[39;49mquant_type,\n\u001b[1;32m    295\u001b[0m     ),\n\u001b[1;32m    296\u001b[0m     torch_dtype\u001b[39m=\u001b[39;49m(torch\u001b[39m.\u001b[39;49mfloat32 \u001b[39mif\u001b[39;49;00m args\u001b[39m.\u001b[39;49mfp16 \u001b[39melse\u001b[39;49;00m (torch\u001b[39m.\u001b[39;49mbfloat16 \u001b[39mif\u001b[39;49;00m args\u001b[39m.\u001b[39;49mbf16 \u001b[39melse\u001b[39;49;00m torch\u001b[39m.\u001b[39;49mfloat32)),\n\u001b[1;32m    297\u001b[0m     trust_remote_code\u001b[39m=\u001b[39;49margs\u001b[39m.\u001b[39;49mtrust_remote_code,\n\u001b[1;32m    298\u001b[0m     use_auth_token\u001b[39m=\u001b[39;49margs\u001b[39m.\u001b[39;49muse_auth_token\n\u001b[1;32m    299\u001b[0m )\n\u001b[1;32m    300\u001b[0m \u001b[39mif\u001b[39;00m compute_dtype \u001b[39m==\u001b[39m torch\u001b[39m.\u001b[39mfloat16 \u001b[39mand\u001b[39;00m args\u001b[39m.\u001b[39mbits \u001b[39m==\u001b[39m \u001b[39m4\u001b[39m:\n\u001b[1;32m    301\u001b[0m     major, minor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mget_device_capability()\n",
      "File \u001b[0;32m~/semaSpeakerAttribution/venv/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py:461\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[39mif\u001b[39;00m kwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtorch_dtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    459\u001b[0m     _ \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mtorch_dtype\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 461\u001b[0m config, kwargs \u001b[39m=\u001b[39m AutoConfig\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    462\u001b[0m     pretrained_model_name_or_path,\n\u001b[1;32m    463\u001b[0m     return_unused_kwargs\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    464\u001b[0m     trust_remote_code\u001b[39m=\u001b[39;49mtrust_remote_code,\n\u001b[1;32m    465\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs,\n\u001b[1;32m    466\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    467\u001b[0m )\n\u001b[1;32m    469\u001b[0m \u001b[39m# if torch_dtype=auto was passed here, ensure to pass it on\u001b[39;00m\n\u001b[1;32m    470\u001b[0m \u001b[39mif\u001b[39;00m kwargs_orig\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtorch_dtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/semaSpeakerAttribution/venv/lib/python3.8/site-packages/transformers/models/auto/configuration_auto.py:983\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    981\u001b[0m kwargs[\u001b[39m\"\u001b[39m\u001b[39mname_or_path\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m pretrained_model_name_or_path\n\u001b[1;32m    982\u001b[0m trust_remote_code \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mtrust_remote_code\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> 983\u001b[0m config_dict, unused_kwargs \u001b[39m=\u001b[39m PretrainedConfig\u001b[39m.\u001b[39;49mget_config_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    984\u001b[0m has_remote_code \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mauto_map\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mAutoConfig\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict[\u001b[39m\"\u001b[39m\u001b[39mauto_map\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    985\u001b[0m has_local_code \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict \u001b[39mand\u001b[39;00m config_dict[\u001b[39m\"\u001b[39m\u001b[39mmodel_type\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39min\u001b[39;00m CONFIG_MAPPING\n",
      "File \u001b[0;32m~/semaSpeakerAttribution/venv/lib/python3.8/site-packages/transformers/configuration_utils.py:617\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    615\u001b[0m original_kwargs \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(kwargs)\n\u001b[1;32m    616\u001b[0m \u001b[39m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[0;32m--> 617\u001b[0m config_dict, kwargs \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_get_config_dict(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    618\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m config_dict:\n\u001b[1;32m    619\u001b[0m     original_kwargs[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m config_dict[\u001b[39m\"\u001b[39m\u001b[39m_commit_hash\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/semaSpeakerAttribution/venv/lib/python3.8/site-packages/transformers/configuration_utils.py:672\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    668\u001b[0m configuration_file \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39m_configuration_file\u001b[39m\u001b[39m\"\u001b[39m, CONFIG_NAME)\n\u001b[1;32m    670\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    671\u001b[0m     \u001b[39m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[0;32m--> 672\u001b[0m     resolved_config_file \u001b[39m=\u001b[39m cached_file(\n\u001b[1;32m    673\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m    674\u001b[0m         configuration_file,\n\u001b[1;32m    675\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    676\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    677\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    678\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    679\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    680\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m    681\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    682\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    683\u001b[0m         subfolder\u001b[39m=\u001b[39;49msubfolder,\n\u001b[1;32m    684\u001b[0m         _commit_hash\u001b[39m=\u001b[39;49mcommit_hash,\n\u001b[1;32m    685\u001b[0m     )\n\u001b[1;32m    686\u001b[0m     commit_hash \u001b[39m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m    687\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m:\n\u001b[1;32m    688\u001b[0m     \u001b[39m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[1;32m    689\u001b[0m     \u001b[39m# the original exception.\u001b[39;00m\n",
      "File \u001b[0;32m~/semaSpeakerAttribution/venv/lib/python3.8/site-packages/transformers/utils/hub.py:433\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    417\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[1;32m    418\u001b[0m         path_or_repo_id,\n\u001b[1;32m    419\u001b[0m         filename,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    429\u001b[0m         local_files_only\u001b[39m=\u001b[39mlocal_files_only,\n\u001b[1;32m    430\u001b[0m     )\n\u001b[1;32m    432\u001b[0m \u001b[39mexcept\u001b[39;00m RepositoryNotFoundError:\n\u001b[0;32m--> 433\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    434\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m is not a local folder and is not a valid model identifier \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    435\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlisted on \u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/models\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mIf this is a private repository, make sure to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    436\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpass a token having permission to this repo with `use_auth_token` or log in with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    437\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`huggingface-cli login` and pass `use_auth_token=True`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    438\u001b[0m     )\n\u001b[1;32m    439\u001b[0m \u001b[39mexcept\u001b[39;00m RevisionNotFoundError:\n\u001b[1;32m    440\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    441\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mrevision\u001b[39m}\u001b[39;00m\u001b[39m is not a valid git identifier (branch name, tag name or commit id) that exists \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    442\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mfor this model name. Check the model page at \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    443\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo_id\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m for available revisions.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    444\u001b[0m     )\n",
      "\u001b[0;31mOSError\u001b[0m: meta-llama/Llama-2-7b-hf is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`."
     ]
    }
   ],
   "source": [
    "train(cues_training_config)\n",
    "\n",
    "# free vram after training\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(roles_training_config)\n",
    "\n",
    "# free vram after training\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference for Cues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load cue model for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    cues_training_config[\"model_name_or_path\"],\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=device_map,\n",
    ")\n",
    "\n",
    "checkpoint_dir = (\n",
    "    cues_training_config[\"output_dir\"] + \"/checkpoint-2000/\"\n",
    ")  # choose checkpoint\n",
    "model = PeftModel.from_pretrained(model, os.path.join(checkpoint_dir, \"adapter_model\"))\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\n",
    "    cues_training_config[\"model_name_or_path\"], legacy=False\n",
    ")\n",
    "tokenizer.bos_token_id = 1\n",
    "\n",
    "pipe = pipeline(\n",
    "    task=\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=300\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Cue-LLM-Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_cues = \"\"\"User: A cue is the lexical items in a sentence that indicate that speech, writing, or thought is being reproduced.\n",
    "I want you to extract all cues in the text below.\n",
    "If you find multiple words for one cue, you output them separated by commas.\n",
    "If no cue can be found in the given text, you output the string #UNK# as cue.\n",
    "Now extract all cues from the following sentence.\n",
    "Use the prefix \\\"Cues: \\\".\n",
    "Sentence: {Satz}\n",
    "Assistant:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_cues = PromptTemplate(input_variables=[\"Satz\"], template=template_cues)\n",
    "llm_chain_cues = LLMChain(prompt=prompt_cues, llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_cues = []\n",
    "for row in tqdm(inputs, desc=\"Cues\"):\n",
    "    outputs_cues.append(llm_chain_cues.apply([row])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free vram after inference for cues\n",
    "del tokenizer\n",
    "del model\n",
    "del pipe\n",
    "del llm\n",
    "del llm_chain_cues\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save and extract cues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data in correct format and insert raw cue outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_outputs_to_output_file_format(inputs, outputs):\n",
    "    result = {}\n",
    "    seen_sentences = []\n",
    "\n",
    "    for i, row in enumerate(inputs):\n",
    "        if row[\"FileName\"] not in result:\n",
    "            result[row[\"FileName\"]] = {\n",
    "                \"Sentences\": [],\n",
    "                \"Annotations\": [],\n",
    "                \"Outputs\": {\"Cues\": {}},\n",
    "            }\n",
    "\n",
    "        if row[\"FileName\"] + \"-\" + str(row[\"SentenceId\"]) not in seen_sentences:\n",
    "            seen_sentences.append(row[\"FileName\"] + \"-\" + str(row[\"SentenceId\"]))\n",
    "            result[row[\"FileName\"]][\"Sentences\"].append(\n",
    "                {\"SentenceId\": row[\"SentenceId\"], \"Tokens\": row[\"Tokens\"]}\n",
    "            )\n",
    "\n",
    "        if row[\"SentenceId\"] not in result[row[\"FileName\"]][\"Outputs\"][\"Cues\"]:\n",
    "            result[row[\"FileName\"]][\"Outputs\"][\"Cues\"][row[\"SentenceId\"]] = []\n",
    "\n",
    "        result[row[\"FileName\"]][\"Outputs\"][\"Cues\"][row[\"SentenceId\"]].append(\n",
    "            outputs[i][\"text\"]\n",
    "        )\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_outputs_to_output_files(inputs, outputs):\n",
    "    path = \"./output/data/\"\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    for key, value in map_outputs_to_output_file_format(inputs, outputs).items():\n",
    "        with open(path + key, \"w\", encoding=\"utf8\") as outfile:\n",
    "            json.dump(value, outfile, indent=3, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_outputs_to_output_files(inputs, outputs_cues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map raw cue outputs to cues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_overlap(cues):\n",
    "    for i, cue in enumerate(cues):\n",
    "        for j in range(i + 1, len(cues)):\n",
    "            if len(list(set(cue) & set(cues[j]))) > 0:\n",
    "                return True, i, j\n",
    "    return False, -1, -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_cues_from_output(output_string: str):\n",
    "    output_string = output_string.strip().split(\"\\n\")[0].strip()\n",
    "\n",
    "    if output_string.startswith(\"Cues:\"):\n",
    "        output_string = output_string[5:].strip()\n",
    "    else:\n",
    "        raise SystemError\n",
    "\n",
    "    if output_string == \"\" or output_string == \"#UNK#\":\n",
    "        return []\n",
    "\n",
    "    outputs = [v.strip() for v in output_string.strip().split(\"],\")]\n",
    "\n",
    "    cues = []\n",
    "    for i, output in enumerate(outputs):\n",
    "        if i < len(outputs) - 1:\n",
    "            output = output + \"]\"\n",
    "        if not output.startswith(\"[\") or not output.endswith(\"]\"):\n",
    "            raise LookupError\n",
    "        output = output[1:-1]\n",
    "        output = [v.strip().split(\" \")[0].strip() for v in output.strip().split(\",\")]\n",
    "\n",
    "        while \"#UNK#\" in output:\n",
    "            output.pop(output.index(\"#UNK#\"))\n",
    "\n",
    "        cues.append(output)\n",
    "\n",
    "    overlap, i, j = check_for_overlap(cues)\n",
    "    while overlap:\n",
    "        cue_2 = cues.pop(j)\n",
    "        cue_1 = cues.pop(i)\n",
    "        cue_1.extend(cue_2)\n",
    "        cue_1 = list(set(cue_1))\n",
    "        cues.append(cue_1)\n",
    "\n",
    "        overlap, i, j = check_for_overlap(cues)\n",
    "\n",
    "    return cues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_cues():\n",
    "    path = \"./output/data/\"\n",
    "    count_cues = 0\n",
    "\n",
    "    for file in sorted(os.listdir(path)):\n",
    "        if file.endswith(\".zip\"):\n",
    "            continue\n",
    "        file_content = {}\n",
    "\n",
    "        with open(os.path.join(path, file), \"r\") as f:\n",
    "            file_content = json.load(f)\n",
    "            file_content[\"Outputs\"][\"Cues_text\"] = {}\n",
    "\n",
    "            for id, output in file_content[\"Outputs\"][\"Cues\"].items():\n",
    "                try:\n",
    "                    cues = extract_cues_from_output(output[0])\n",
    "                # output does not start with \"Cues: \"\n",
    "                except SystemError:\n",
    "                    cues = []\n",
    "                # output not in [...] format\n",
    "                except LookupError:\n",
    "                    cues = []\n",
    "\n",
    "                count_cues += len(cues)\n",
    "                file_content[\"Outputs\"][\"Cues_text\"][id] = cues\n",
    "\n",
    "        with open(os.path.join(path, file), \"w\", encoding=\"utf8\") as outfile:\n",
    "            json.dump(file_content, outfile, indent=3, ensure_ascii=False)\n",
    "\n",
    "    return count_cues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_cues = extract_cues()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference for Roles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load roles model for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    roles_training_config[\"model_name_or_path\"],\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=device_map,\n",
    ")\n",
    "\n",
    "checkpoint_dir = (\n",
    "    roles_training_config[\"output_dir\"] + \"/checkpoint-2000/\"\n",
    ")  # choose checkpoint\n",
    "model = PeftModel.from_pretrained(model, os.path.join(checkpoint_dir, \"adapter_model\"))\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\n",
    "    roles_training_config[\"model_name_or_path\"], legacy=False\n",
    ")\n",
    "tokenizer.bos_token_id = 1\n",
    "\n",
    "pipe = pipeline(\n",
    "    task=\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=300\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_for_roles(ds, prompt_cues, roles_prompt):\n",
    "    path = \"./output/data/\"\n",
    "\n",
    "    pbar = tqdm(total=count_cues, desc=\"Roles\")\n",
    "    for file in sorted(os.listdir(path)):\n",
    "        if file.endswith(\".zip\"):\n",
    "            continue\n",
    "        file_content = {}\n",
    "\n",
    "        with open(os.path.join(path, file), \"r\") as f:\n",
    "            file_content = json.load(f)\n",
    "            file_content[\"Outputs\"][\"Roles\"] = {}\n",
    "\n",
    "            for id, cues in file_content[\"Outputs\"][\"Cues_text\"].items():\n",
    "                file_content[\"Outputs\"][\"Roles\"][id] = []\n",
    "\n",
    "                if cues == []:\n",
    "                    continue\n",
    "\n",
    "                sentence = ds.filter(\n",
    "                    lambda r: r[\"FileName\"] == file and r[\"SentenceId\"] == int(id)\n",
    "                )[0][\"Sentence\"]\n",
    "                text = ds.filter(\n",
    "                    lambda r: r[\"FileName\"] == file and r[\"SentenceId\"] == int(id)\n",
    "                )[0][\"sentence_extended\"]\n",
    "                if sentence.endswith(\":\"):\n",
    "                    sentence = sentence[:-1] + \".\"\n",
    "                if text.endswith(\":\"):\n",
    "                    text = text[:-1] + \".\"\n",
    "                cue_prompt = (\n",
    "                    prompt_cues.format(Satz=sentence)\n",
    "                    + \" Cues: \"\n",
    "                    + \", \".join([\"[\" + \", \".join(cue) + \"]\" for cue in cues])\n",
    "                    + \"</s>\"\n",
    "                )\n",
    "\n",
    "                for cue in cues:\n",
    "                    file_content[\"Outputs\"][\"Roles\"][id].append([])\n",
    "                    prompt = PromptTemplate(\n",
    "                        input_variables=[\"text\", \"cue\"],\n",
    "                        template=cue_prompt + \"\\nUser: \" + roles_prompt,\n",
    "                    )\n",
    "                    llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "                    output = llm_chain.apply([{\"text\": text, \"cue\": \", \".join(cue)}])[\n",
    "                        0\n",
    "                    ][\"text\"]\n",
    "                    file_content[\"Outputs\"][\"Roles\"][id][-1].append(output)\n",
    "                    pbar.update()\n",
    "\n",
    "        with open(os.path.join(path, file), \"w\", encoding=\"utf8\") as outfile:\n",
    "            json.dump(file_content, outfile, indent=3, ensure_ascii=False)\n",
    "\n",
    "    pbar.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roles_prompt = \"Now I give you again the sentence only in addition with the two following sentences, because the roles can be partially contained in the following sentences.\\nText: {text}\\n\\nNow find all roles in the sentence associated with the cue '{cue}' you found in the beginning sentence.\\nAssistant:\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_for_roles(test_ds, prompt_cues, roles_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# free vram after inference for roles\n",
    "del tokenizer\n",
    "del model\n",
    "del pipe\n",
    "del llm\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Roles and map outputs to tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_roles_from_output(output_string: str):\n",
    "    res = {\n",
    "        \"ptc\": \"\",\n",
    "        \"evidence\": \"\",\n",
    "        \"medium\": \"\",\n",
    "        \"topic\": \"\",\n",
    "        \"addr\": \"\",\n",
    "        \"message\": \"\",\n",
    "        \"source\": \"\",\n",
    "    }\n",
    "\n",
    "    output_rows = [v.strip() for v in output_string.strip().split(\"\\n\")]\n",
    "\n",
    "    try:\n",
    "        if output_rows[1].startswith(\"ptc: \"):\n",
    "            res[\"ptc\"] = [\n",
    "                v.strip().split(\" \")[0].strip()\n",
    "                for v in output_rows[1][4:].strip().split(\",\")\n",
    "            ]\n",
    "    except IndexError:\n",
    "        pass\n",
    "    try:\n",
    "        if output_rows[2].startswith(\"evidence: \"):\n",
    "            res[\"evidence\"] = [\n",
    "                v.strip().split(\" \")[0].strip()\n",
    "                for v in output_rows[2][9:].strip().split(\",\")\n",
    "            ]\n",
    "    except IndexError:\n",
    "        pass\n",
    "    try:\n",
    "        if output_rows[3].startswith(\"medium: \"):\n",
    "            res[\"medium\"] = [\n",
    "                v.strip().split(\" \")[0].strip()\n",
    "                for v in output_rows[3][7:].strip().split(\",\")\n",
    "            ]\n",
    "    except IndexError:\n",
    "        pass\n",
    "    try:\n",
    "        if output_rows[4].startswith(\"topic: \"):\n",
    "            res[\"topic\"] = [\n",
    "                v.strip().split(\" \")[0].strip()\n",
    "                for v in output_rows[4][6:].strip().split(\",\")\n",
    "            ]\n",
    "    except IndexError:\n",
    "        pass\n",
    "    try:\n",
    "        if output_rows[5].startswith(\"addr: \"):\n",
    "            res[\"addr\"] = [\n",
    "                v.strip().split(\" \")[0].strip()\n",
    "                for v in output_rows[5][5:].strip().split(\",\")\n",
    "            ]\n",
    "    except IndexError:\n",
    "        pass\n",
    "    try:\n",
    "        if output_rows[6].startswith(\"message: \"):\n",
    "            res[\"message\"] = [\n",
    "                v.strip().split(\" \")[0].strip()\n",
    "                for v in output_rows[6][8:].strip().split(\",\")\n",
    "            ]\n",
    "    except IndexError:\n",
    "        pass\n",
    "    try:\n",
    "        if output_rows[7].startswith(\"source: \"):\n",
    "            res[\"source\"] = [\n",
    "                v.strip().split(\" \")[0].strip()\n",
    "                for v in output_rows[7][7:].strip().split(\",\")\n",
    "            ]\n",
    "    except IndexError:\n",
    "        pass\n",
    "\n",
    "    for key, value in res.items():\n",
    "        if value == [\"\"] or value == [\"#UNK#\"]:\n",
    "            res[key] = \"\"\n",
    "        while \"#UNK#\" in value:\n",
    "            value.pop(value.index(\"#UNK#\"))\n",
    "        while type(value) == list and \"\" in value:\n",
    "            value.pop(value.index(\"\"))\n",
    "        res[key] = value\n",
    "\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_roles():\n",
    "    path = \"./output/data/\"\n",
    "\n",
    "    for file in sorted(os.listdir(path)):\n",
    "        if file.endswith(\".zip\"):\n",
    "            continue\n",
    "        file_content = {}\n",
    "\n",
    "        with open(os.path.join(path, file), \"r\") as f:\n",
    "            file_content = json.load(f)\n",
    "            file_content[\"Outputs\"][\"Roles_text\"] = {}\n",
    "\n",
    "            for id, roles_for_sentence in file_content[\"Outputs\"][\"Roles\"].items():\n",
    "                file_content[\"Outputs\"][\"Roles_text\"][id] = []\n",
    "\n",
    "                if roles_for_sentence == []:\n",
    "                    continue\n",
    "\n",
    "                for roles_output in roles_for_sentence:\n",
    "                    file_content[\"Outputs\"][\"Roles_text\"][id].append([])\n",
    "\n",
    "                    roles = extract_roles_from_output(roles_output[0])\n",
    "                    file_content[\"Outputs\"][\"Roles_text\"][id][-1].append(roles)\n",
    "\n",
    "        with open(os.path.join(path, file), \"w\", encoding=\"utf8\") as outfile:\n",
    "            json.dump(file_content, outfile, indent=3, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_roles()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_neighbors(i, seen, skip_index):\n",
    "    res = 0\n",
    "    if i - 2 >= 0 and i - 2 != skip_index:\n",
    "        res += 1 if seen[i - 2] else 0\n",
    "    if i - 1 >= 0 and i - 1 != skip_index:\n",
    "        res += 1 if seen[i - 1] else 0\n",
    "    if i + 1 < len(seen) and i + 1 != skip_index:\n",
    "        res += 1 if seen[i + 1] else 0\n",
    "    if i + 2 < len(seen) and i + 2 != skip_index:\n",
    "        res += 1 if seen[i + 2] else 0\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_neighborhood_swap(seen, tokens):\n",
    "    for i, v in enumerate(seen):\n",
    "        if not v:\n",
    "            continue\n",
    "\n",
    "        neigh_c_v = count_neighbors(i, seen, -1)\n",
    "        neigh = [\n",
    "            j\n",
    "            for j, t in enumerate(tokens)\n",
    "            if seen[j] == False and Levenshtein.distance(t, tokens[i]) <= 1\n",
    "        ]\n",
    "        neigh_c_other = [count_neighbors(n, seen, i) for n in neigh]\n",
    "        if len(neigh_c_other) > 0:\n",
    "            neigh_c_other_max = max(neigh_c_other)\n",
    "            if neigh_c_other_max > neigh_c_v:\n",
    "                return i, neigh[neigh_c_other.index(neigh_c_other_max)]\n",
    "\n",
    "    return -1, -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_output_list(output_list: list, ids: list, tokens: list, seen_old=None):\n",
    "    res = []\n",
    "    seen = [False] * len(tokens)\n",
    "    if seen_old == None:\n",
    "        seen_old = [False] * len(tokens)\n",
    "\n",
    "    for output in output_list:\n",
    "        indices = [\n",
    "            i\n",
    "            for i, v in enumerate(tokens)\n",
    "            if v == output and seen[i] == False and seen_old[i] == False\n",
    "        ]\n",
    "        if len(indices) > 0:\n",
    "            seen[indices[0]] = True\n",
    "        if len(indices) == 0:\n",
    "            indices = [\n",
    "                i\n",
    "                for i, v in enumerate(tokens)\n",
    "                if seen[i] == False\n",
    "                and seen_old[i] == False\n",
    "                and Levenshtein.distance(output, v) <= 1\n",
    "            ]\n",
    "            if len(indices) > 0:\n",
    "                seen[indices[0]] = True\n",
    "\n",
    "    changed = True\n",
    "    while changed:\n",
    "        changed = False\n",
    "        i, j = calculate_neighborhood_swap(seen, tokens)\n",
    "        while i != j:\n",
    "            seen[i] = False\n",
    "            seen[j] = True\n",
    "            changed = True\n",
    "            i, j = calculate_neighborhood_swap(seen, tokens)\n",
    "\n",
    "        for i in range(len(seen)):\n",
    "            if (\n",
    "                seen[i] == False\n",
    "                and i != 0\n",
    "                and i != len(seen) - 1\n",
    "                and seen[i - 1]\n",
    "                and seen[i + 1]\n",
    "                and (\n",
    "                    tokens[i] == \",\"\n",
    "                    or tokens[i] == \":\"\n",
    "                    or tokens[i] == \";\"\n",
    "                    or tokens[i] == \"-\"\n",
    "                )\n",
    "            ):\n",
    "                seen[i] = True\n",
    "                changed = True\n",
    "\n",
    "    for i in range(len(seen)):\n",
    "        if seen[i]:\n",
    "            res.append(str(ids[i]) + \":\" + str(i))\n",
    "\n",
    "    return res, [v or seen_old[i] for i, v in enumerate(seen)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_outputs(ds):\n",
    "    path = \"./output/data/\"\n",
    "\n",
    "    for file in sorted(os.listdir(path)):\n",
    "        if file.endswith(\".zip\"):\n",
    "            continue\n",
    "        file_content = {}\n",
    "\n",
    "        with open(os.path.join(path, file), \"r\") as f:\n",
    "            file_content = json.load(f)\n",
    "            file_content[\"Annotations\"] = []\n",
    "\n",
    "            for cues_text, roles_text in zip(\n",
    "                file_content[\"Outputs\"][\"Cues_text\"].items(),\n",
    "                file_content[\"Outputs\"][\"Roles_text\"].items(),\n",
    "            ):\n",
    "                id, cues = cues_text\n",
    "                id, roles_list = roles_text\n",
    "\n",
    "                if cues == []:\n",
    "                    continue\n",
    "\n",
    "                tokens = ds.filter(\n",
    "                    lambda r: r[\"FileName\"] == file and r[\"SentenceId\"] == int(id)\n",
    "                )[0][\"tokens_extended\"]\n",
    "                ids = ds.filter(\n",
    "                    lambda r: r[\"FileName\"] == file and r[\"SentenceId\"] == int(id)\n",
    "                )[0][\"sentence_extended_ids\"]\n",
    "\n",
    "                seen_cues = None\n",
    "                for cue, roles in zip(cues, roles_list):\n",
    "                    roles = roles[0]\n",
    "\n",
    "                    cue, seen_cues = map_output_list(cue, ids, tokens, seen_cues)\n",
    "\n",
    "                    if cue != []:\n",
    "                        addr, _ = map_output_list(\n",
    "                            roles[\"addr\"],\n",
    "                            ids,\n",
    "                            tokens,\n",
    "                        )\n",
    "\n",
    "                        evidence, _ = map_output_list(\n",
    "                            roles[\"evidence\"],\n",
    "                            ids,\n",
    "                            tokens,\n",
    "                        )\n",
    "\n",
    "                        medium, _ = map_output_list(\n",
    "                            roles[\"medium\"],\n",
    "                            ids,\n",
    "                            tokens,\n",
    "                        )\n",
    "\n",
    "                        message, _ = map_output_list(\n",
    "                            roles[\"message\"],\n",
    "                            ids,\n",
    "                            tokens,\n",
    "                        )\n",
    "\n",
    "                        source, _ = map_output_list(\n",
    "                            roles[\"source\"],\n",
    "                            ids,\n",
    "                            tokens,\n",
    "                        )\n",
    "\n",
    "                        topic, _ = map_output_list(\n",
    "                            roles[\"topic\"],\n",
    "                            ids,\n",
    "                            tokens,\n",
    "                        )\n",
    "\n",
    "                        ptc, _ = map_output_list(\n",
    "                            roles[\"ptc\"],\n",
    "                            ids,\n",
    "                            tokens,\n",
    "                        )\n",
    "\n",
    "                        annotation = {\n",
    "                            \"Addr\": addr,\n",
    "                            \"Evidence\": evidence,\n",
    "                            \"Medium\": medium,\n",
    "                            \"Message\": message,\n",
    "                            \"Source\": source,\n",
    "                            \"Topic\": topic,\n",
    "                            \"Cue\": cue,\n",
    "                            \"PTC\": ptc,\n",
    "                        }\n",
    "                        file_content[\"Annotations\"].append(annotation)\n",
    "\n",
    "        with open(os.path.join(path, file), \"w\", encoding=\"utf8\") as outfile:\n",
    "            json.dump(file_content, outfile, indent=3, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_outputs(test_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare zip file for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"./output/data/submission.zip\"):\n",
    "    os.remove(\"./output/data/submission.zip\")\n",
    "\n",
    "temp_path = \"./output/data/temp\"\n",
    "shutil.copytree(\"./output/data\", temp_path)\n",
    "\n",
    "for file in sorted(os.listdir(temp_path)):\n",
    "    file_content = {}\n",
    "\n",
    "    with open(os.path.join(temp_path, file), \"r\") as f:\n",
    "        file_content = json.load(f)\n",
    "        file_content.pop(\"Outputs\")\n",
    "\n",
    "    with open(os.path.join(temp_path, file), \"w\", encoding=\"utf8\") as outfile:\n",
    "        json.dump(file_content, outfile, indent=3, ensure_ascii=False)\n",
    "shutil.make_archive(temp_path, \"zip\", temp_path)\n",
    "shutil.move(\n",
    "    temp_path + \".zip\",\n",
    "    \"./output/data/submission.zip\",\n",
    ")\n",
    "shutil.rmtree(temp_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
