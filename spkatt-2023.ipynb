{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "\n",
    "from datasets import (\n",
    "    load_dataset,\n",
    "    concatenate_datasets,\n",
    "    load_from_disk,\n",
    "    Features,\n",
    "    Sequence,\n",
    "    Value,\n",
    "    logging,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.set_verbosity_error()\n",
    "logging.disable_progress_bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_annotations_from_file(path: str, file: str):\n",
    "    features = Features(\n",
    "        {\n",
    "            \"PTC\": Sequence(feature=Value(dtype=\"string\", id=None), length=-1, id=None),\n",
    "            \"Evidence\": Sequence(\n",
    "                feature=Value(dtype=\"string\", id=None), length=-1, id=None\n",
    "            ),\n",
    "            \"Medium\": Sequence(\n",
    "                feature=Value(dtype=\"string\", id=None), length=-1, id=None\n",
    "            ),\n",
    "            \"Topic\": Sequence(\n",
    "                feature=Value(dtype=\"string\", id=None), length=-1, id=None\n",
    "            ),\n",
    "            \"Cue\": Sequence(feature=Value(dtype=\"string\", id=None), length=-1, id=None),\n",
    "            \"Addr\": Sequence(\n",
    "                feature=Value(dtype=\"string\", id=None), length=-1, id=None\n",
    "            ),\n",
    "            \"Message\": Sequence(\n",
    "                feature=Value(dtype=\"string\", id=None), length=-1, id=None\n",
    "            ),\n",
    "            \"Source\": Sequence(\n",
    "                feature=Value(dtype=\"string\", id=None), length=-1, id=None\n",
    "            ),\n",
    "        }\n",
    "    )\n",
    "    ds = load_dataset(\n",
    "        \"json\",\n",
    "        data_files=os.path.join(path, file),\n",
    "        field=\"Annotations\",\n",
    "        split=\"train\",\n",
    "        features=features,\n",
    "    )\n",
    "    ds = ds.add_column(\"FileName\", [file] * len(ds))\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sentences_from_file(path: str, file: str):\n",
    "    ds = load_dataset(\n",
    "        \"json\", data_files=os.path.join(path, file), field=\"Sentences\", split=\"train\"\n",
    "    )\n",
    "    ds = ds.add_column(\"FileName\", [file] * len(ds))\n",
    "    ds = ds.add_column(\"Sentence\", [\" \".join(t) for t in ds[\"Tokens\"]])\n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_annotations_from_path(path: str):\n",
    "    dataset = None\n",
    "\n",
    "    for file in tqdm(sorted(os.listdir(path))):\n",
    "        if not dataset:\n",
    "            dataset = read_annotations_from_file(path, file)\n",
    "        else:\n",
    "            dataset = concatenate_datasets(\n",
    "                [dataset, read_annotations_from_file(path, file)]\n",
    "            )\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sentences_from_path(path: str):\n",
    "    dataset = None\n",
    "\n",
    "    for file in tqdm(sorted(os.listdir(path))):\n",
    "        if not dataset:\n",
    "            dataset = read_sentences_from_file(path, file)\n",
    "        else:\n",
    "            dataset = concatenate_datasets(\n",
    "                [dataset, read_sentences_from_file(path, file)]\n",
    "            )\n",
    "\n",
    "    dataset = dataset.add_column(\"id\", range(len(dataset)))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sentences_dataset(ds_name: str):\n",
    "    path_to_dataset = \"./transformed_datasets/\" + ds_name + \"/sentences\"\n",
    "\n",
    "    if os.path.isdir(path_to_dataset):\n",
    "        result = load_from_disk(path_to_dataset)\n",
    "    else:\n",
    "        result = read_sentences_from_path(\n",
    "            \"./SpkAtt-2023/data/\"\n",
    "            + ds_name\n",
    "            + \"/task1\"\n",
    "            + (\"_test/\" if ds_name == \"eval\" else \"/\")\n",
    "        )\n",
    "        os.makedirs(path_to_dataset, exist_ok=True)\n",
    "        result.save_to_disk(path_to_dataset)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_annotations_dataset(ds_name: str):\n",
    "    path_to_dataset = \"./transformed_datasets/\" + ds_name + \"/annotations\"\n",
    "\n",
    "    if os.path.isdir(path_to_dataset):\n",
    "        return load_from_disk(path_to_dataset)\n",
    "\n",
    "    result = read_annotations_from_path(\n",
    "        \"./SpkAtt-2023/data/\"\n",
    "        + ds_name\n",
    "        + \"/task1\"\n",
    "        + (\"_test/\" if ds_name == \"eval\" else \"/\")\n",
    "    )\n",
    "    os.makedirs(path_to_dataset, exist_ok=True)\n",
    "    result.save_to_disk(path_to_dataset)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences_dataset = read_sentences_dataset(\"train\")\n",
    "val_sentences_dataset = read_sentences_dataset(\"dev\")\n",
    "test_sentences_dataset = read_sentences_dataset(\"eval\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_annotations_dataset = read_annotations_dataset(\"train\")\n",
    "val_annotations_dataset = read_annotations_dataset(\"dev\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format datasets for usage in langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_from_label(train_sentences_dataset, row, annotations):\n",
    "    tokens = []\n",
    "    for anno in annotations:\n",
    "        if int(anno.split(\":\")[0]) == row[\"SentenceId\"]:\n",
    "            tokens.append(row[\"Tokens\"][int(anno.split(\":\")[1])])\n",
    "        else:\n",
    "            temp_row = train_sentences_dataset.filter(\n",
    "                lambda r: r[\"FileName\"] == row[\"FileName\"]\n",
    "                and r[\"SentenceId\"] == int(anno.split(\":\")[0])\n",
    "            )[0]\n",
    "            tokens.append(temp_row[\"Tokens\"][int(anno.split(\":\")[1])])\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_complete_dataset(sentences_dataset, annotations_dataset, dataset_name):\n",
    "    path_to_dataset = \"./transformed_datasets/\" + dataset_name + \"/complete\"\n",
    "    if os.path.isdir(path_to_dataset):\n",
    "        return load_from_disk(path_to_dataset)\n",
    "\n",
    "    ptc, ptc_temp, ptc_mapped, ptc_mapped_temp = [], [], [], []\n",
    "    evidence, evidence_temp, evidence_mapped, evidence_mapped_temp = [], [], [], []\n",
    "    medium, medium_temp, medium_mapped, medium_mapped_temp = [], [], [], []\n",
    "    topic, topic_temp, topic_mapped, topic_mapped_temp = [], [], [], []\n",
    "    cue, cue_temp, cue_mapped, cue_mapped_temp = [], [], [], []\n",
    "    addr, addr_temp, addr_mapped, addr_mapped_temp = [], [], [], []\n",
    "    message, message_temp, message_mapped, message_mapped_temp = [], [], [], []\n",
    "    source, source_temp, source_mapped, source_mapped_temp = [], [], [], []\n",
    "    (\n",
    "        sentence_extended,\n",
    "        tokens_extended,\n",
    "        sentence_extended_ids,\n",
    "    ) = (\n",
    "        [],\n",
    "        [],\n",
    "        [],\n",
    "    )\n",
    "\n",
    "    index_in_anno_ds = 0\n",
    "\n",
    "    for i, row in tqdm(enumerate(sentences_dataset)):\n",
    "        context = row[\"Sentence\"]\n",
    "        tokens = row[\"Tokens\"]\n",
    "        ids = [row[\"SentenceId\"]] * len(row[\"Tokens\"])\n",
    "        if (\n",
    "            i + 1 < len(sentences_dataset)\n",
    "            and sentences_dataset[i + 1][\"FileName\"] == row[\"FileName\"]\n",
    "        ):\n",
    "            context = context + \" \" + sentences_dataset[i + 1][\"Sentence\"]\n",
    "            tokens.extend(sentences_dataset[i + 1][\"Tokens\"])\n",
    "            ids.extend(\n",
    "                [sentences_dataset[i + 1][\"SentenceId\"]]\n",
    "                * len(sentences_dataset[i + 1][\"Tokens\"])\n",
    "            )\n",
    "        if (\n",
    "            i + 2 < len(sentences_dataset)\n",
    "            and sentences_dataset[i + 2][\"FileName\"] == row[\"FileName\"]\n",
    "        ):\n",
    "            context = context + \" \" + sentences_dataset[i + 2][\"Sentence\"]\n",
    "            tokens.extend(sentences_dataset[i + 2][\"Tokens\"])\n",
    "            ids.extend(\n",
    "                [sentences_dataset[i + 2][\"SentenceId\"]]\n",
    "                * len(sentences_dataset[i + 2][\"Tokens\"])\n",
    "            )\n",
    "        sentence_extended.append(context)\n",
    "        tokens_extended.append(tokens)\n",
    "        sentence_extended_ids.append(ids)\n",
    "\n",
    "        if annotations_dataset is not None:\n",
    "            id_of_next_sentence_with_annotation = (\n",
    "                int(annotations_dataset[index_in_anno_ds][\"Cue\"][0].split(\":\")[0])\n",
    "                if index_in_anno_ds != len(annotations_dataset)\n",
    "                else -1\n",
    "            )\n",
    "\n",
    "            if row[\"SentenceId\"] != id_of_next_sentence_with_annotation:\n",
    "                ptc.append([])\n",
    "                ptc_mapped.append([])\n",
    "                evidence.append([])\n",
    "                evidence_mapped.append([])\n",
    "                medium.append([])\n",
    "                medium_mapped.append([])\n",
    "                topic.append([])\n",
    "                topic_mapped.append([])\n",
    "                cue.append([])\n",
    "                cue_mapped.append([])\n",
    "                addr.append([])\n",
    "                addr_mapped.append([])\n",
    "                message.append([])\n",
    "                message_mapped.append([])\n",
    "                source.append([])\n",
    "                source_mapped.append([])\n",
    "                continue\n",
    "\n",
    "            while row[\"SentenceId\"] == id_of_next_sentence_with_annotation:\n",
    "                ptc_temp.append(annotations_dataset[index_in_anno_ds][\"PTC\"])\n",
    "                evidence_temp.append(annotations_dataset[index_in_anno_ds][\"Evidence\"])\n",
    "                medium_temp.append(annotations_dataset[index_in_anno_ds][\"Medium\"])\n",
    "                topic_temp.append(annotations_dataset[index_in_anno_ds][\"Topic\"])\n",
    "                cue_temp.append(annotations_dataset[index_in_anno_ds][\"Cue\"])\n",
    "                addr_temp.append(annotations_dataset[index_in_anno_ds][\"Addr\"])\n",
    "                message_temp.append(annotations_dataset[index_in_anno_ds][\"Message\"])\n",
    "                source_temp.append(annotations_dataset[index_in_anno_ds][\"Source\"])\n",
    "\n",
    "                ptc_mapped_temp.append(\n",
    "                    get_text_from_label(sentences_dataset, row, ptc_temp[-1])\n",
    "                )\n",
    "                evidence_mapped_temp.append(\n",
    "                    get_text_from_label(sentences_dataset, row, evidence_temp[-1])\n",
    "                )\n",
    "                medium_mapped_temp.append(\n",
    "                    get_text_from_label(sentences_dataset, row, medium_temp[-1])\n",
    "                )\n",
    "                topic_mapped_temp.append(\n",
    "                    get_text_from_label(sentences_dataset, row, topic_temp[-1])\n",
    "                )\n",
    "                cue_mapped_temp.append(\n",
    "                    get_text_from_label(sentences_dataset, row, cue_temp[-1])\n",
    "                )\n",
    "                addr_mapped_temp.append(\n",
    "                    get_text_from_label(sentences_dataset, row, addr_temp[-1])\n",
    "                )\n",
    "                message_mapped_temp.append(\n",
    "                    get_text_from_label(sentences_dataset, row, message_temp[-1])\n",
    "                )\n",
    "                source_mapped_temp.append(\n",
    "                    get_text_from_label(sentences_dataset, row, source_temp[-1])\n",
    "                )\n",
    "\n",
    "                index_in_anno_ds += 1\n",
    "                if index_in_anno_ds == len(annotations_dataset):\n",
    "                    break\n",
    "                id_of_next_sentence_with_annotation = int(\n",
    "                    annotations_dataset[index_in_anno_ds][\"Cue\"][0].split(\":\")[0]\n",
    "                )\n",
    "\n",
    "            ptc.append(ptc_temp)\n",
    "            ptc_mapped.append(ptc_mapped_temp)\n",
    "            evidence.append(evidence_temp)\n",
    "            evidence_mapped.append(evidence_mapped_temp)\n",
    "            medium.append(medium_temp)\n",
    "            medium_mapped.append(medium_mapped_temp)\n",
    "            topic.append(topic_temp)\n",
    "            topic_mapped.append(topic_mapped_temp)\n",
    "            cue.append(cue_temp)\n",
    "            cue_mapped.append(cue_mapped_temp)\n",
    "            addr.append(addr_temp)\n",
    "            addr_mapped.append(addr_mapped_temp)\n",
    "            message.append(message_temp)\n",
    "            message_mapped.append(message_mapped_temp)\n",
    "            source.append(source_temp)\n",
    "            source_mapped.append(source_mapped_temp)\n",
    "\n",
    "            ptc_temp, ptc_mapped_temp = [], []\n",
    "            evidence_temp, evidence_mapped_temp = [], []\n",
    "            medium_temp, medium_mapped_temp = [], []\n",
    "            topic_temp, topic_mapped_temp = [], []\n",
    "            cue_temp, cue_mapped_temp = [], []\n",
    "            addr_temp, addr_mapped_temp = [], []\n",
    "            message_temp, message_mapped_temp = [], []\n",
    "            source_temp, source_mapped_temp = [], []\n",
    "\n",
    "    res = sentences_dataset.add_column(\"sentence_extended\", sentence_extended)\n",
    "    res = res.add_column(\"tokens_extended\", tokens_extended)\n",
    "    res = res.add_column(\"sentence_extended_ids\", sentence_extended_ids)\n",
    "\n",
    "    if annotations_dataset is not None:\n",
    "        res = res.add_column(\"ptc\", ptc)\n",
    "        res = res.add_column(\"ptc_mapped\", ptc_mapped)\n",
    "        res = res.add_column(\"evidence\", evidence)\n",
    "        res = res.add_column(\"evidence_mapped\", evidence_mapped)\n",
    "        res = res.add_column(\"medium\", medium)\n",
    "        res = res.add_column(\"medium_mapped\", medium_mapped)\n",
    "        res = res.add_column(\"topic\", topic)\n",
    "        res = res.add_column(\"topic_mapped\", topic_mapped)\n",
    "        res = res.add_column(\"cue\", cue)\n",
    "        res = res.add_column(\"cue_mapped\", cue_mapped)\n",
    "        res = res.add_column(\"addr\", addr)\n",
    "        res = res.add_column(\"addr_mapped\", addr_mapped)\n",
    "        res = res.add_column(\"message\", message)\n",
    "        res = res.add_column(\"message_mapped\", message_mapped)\n",
    "        res = res.add_column(\"source\", source)\n",
    "        res = res.add_column(\"source_mapped\", source_mapped)\n",
    "\n",
    "    os.makedirs(path_to_dataset, exist_ok=True)\n",
    "    res.save_to_disk(path_to_dataset)\n",
    "\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = build_complete_dataset(\n",
    "    train_sentences_dataset, train_annotations_dataset, \"train\"\n",
    ")\n",
    "val_ds = build_complete_dataset(val_sentences_dataset, val_annotations_dataset, \"dev\")\n",
    "test_ds = build_complete_dataset(test_sentences_dataset, None, \"eval\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = test_sentences_dataset.rename_column(\"Sentence\", \"Satz\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Showcase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Tokens': ['-',\n",
       "  'Letzter',\n",
       "  'Redner',\n",
       "  'in',\n",
       "  'der',\n",
       "  'Debatte',\n",
       "  ':',\n",
       "  'Bernd',\n",
       "  'Westphal',\n",
       "  'für',\n",
       "  'die',\n",
       "  'SPD-Fraktion',\n",
       "  '.'],\n",
       " 'SentenceId': 52,\n",
       " 'FileName': '19002_Zusatzpunkt_3_CDUCSU_Jung_ID19209800_21.11.2017.json',\n",
       " 'Sentence': '- Letzter Redner in der Debatte : Bernd Westphal für die SPD-Fraktion .',\n",
       " 'id': 52,\n",
       " 'sentence_extended': '- Letzter Redner in der Debatte : Bernd Westphal für die SPD-Fraktion .',\n",
       " 'tokens_extended': ['-',\n",
       "  'Letzter',\n",
       "  'Redner',\n",
       "  'in',\n",
       "  'der',\n",
       "  'Debatte',\n",
       "  ':',\n",
       "  'Bernd',\n",
       "  'Westphal',\n",
       "  'für',\n",
       "  'die',\n",
       "  'SPD-Fraktion',\n",
       "  '.'],\n",
       " 'sentence_extended_ids': [52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52],\n",
       " 'ptc': [[]],\n",
       " 'ptc_mapped': [[]],\n",
       " 'evidence': [[]],\n",
       " 'evidence_mapped': [[]],\n",
       " 'medium': [[]],\n",
       " 'medium_mapped': [[]],\n",
       " 'topic': [[]],\n",
       " 'topic_mapped': [[]],\n",
       " 'cue': [['52:5']],\n",
       " 'cue_mapped': [['Debatte']],\n",
       " 'addr': [[]],\n",
       " 'addr_mapped': [[]],\n",
       " 'message': [[]],\n",
       " 'message_mapped': [[]],\n",
       " 'source': [[]],\n",
       " 'source_mapped': [[]]}"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[52]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Tokens': ['Dazu',\n",
       "  'muss',\n",
       "  'man',\n",
       "  'nur',\n",
       "  'mit',\n",
       "  'den',\n",
       "  'Landwirten',\n",
       "  'sprechen',\n",
       "  ',',\n",
       "  'die',\n",
       "  'sagen',\n",
       "  ':',\n",
       "  'Ja',\n",
       "  ',',\n",
       "  'auch',\n",
       "  'früher',\n",
       "  'gab',\n",
       "  'es',\n",
       "  'extreme',\n",
       "  'Ereignisse',\n",
       "  ',',\n",
       "  'auch',\n",
       "  'früher',\n",
       "  'gab',\n",
       "  'es',\n",
       "  'Naturkatastrophen',\n",
       "  ',',\n",
       "  'aber',\n",
       "  'in',\n",
       "  'einem',\n",
       "  'Jahr',\n",
       "  'den',\n",
       "  'Hagel',\n",
       "  ',',\n",
       "  'im',\n",
       "  'anderen',\n",
       "  'Jahr',\n",
       "  'eine',\n",
       "  'Dürre',\n",
       "  'und',\n",
       "  'im',\n",
       "  'dritten',\n",
       "  'Jahr',\n",
       "  ',',\n",
       "  'wie',\n",
       "  'in',\n",
       "  'diesem',\n",
       "  'Jahr',\n",
       "  ',',\n",
       "  'die',\n",
       "  'Frostschäden',\n",
       "  ',',\n",
       "  'unter',\n",
       "  'denen',\n",
       "  'die',\n",
       "  'Obstbauern',\n",
       "  'zu',\n",
       "  'leiden',\n",
       "  'hatten',\n",
       "  ',',\n",
       "  'diese',\n",
       "  'Häufung',\n",
       "  'hatten',\n",
       "  'wir',\n",
       "  'früher',\n",
       "  'so',\n",
       "  'nicht',\n",
       "  ',',\n",
       "  'also',\n",
       "  'tut',\n",
       "  'etwas',\n",
       "  'gegen',\n",
       "  'den',\n",
       "  'Klimawandel',\n",
       "  '.'],\n",
       " 'SentenceId': 15,\n",
       " 'FileName': '19002_Zusatzpunkt_3_CDUCSU_Jung_ID19209800_21.11.2017.json',\n",
       " 'Sentence': 'Dazu muss man nur mit den Landwirten sprechen , die sagen : Ja , auch früher gab es extreme Ereignisse , auch früher gab es Naturkatastrophen , aber in einem Jahr den Hagel , im anderen Jahr eine Dürre und im dritten Jahr , wie in diesem Jahr , die Frostschäden , unter denen die Obstbauern zu leiden hatten , diese Häufung hatten wir früher so nicht , also tut etwas gegen den Klimawandel .',\n",
       " 'id': 15,\n",
       " 'sentence_extended': 'Dazu muss man nur mit den Landwirten sprechen , die sagen : Ja , auch früher gab es extreme Ereignisse , auch früher gab es Naturkatastrophen , aber in einem Jahr den Hagel , im anderen Jahr eine Dürre und im dritten Jahr , wie in diesem Jahr , die Frostschäden , unter denen die Obstbauern zu leiden hatten , diese Häufung hatten wir früher so nicht , also tut etwas gegen den Klimawandel . Es geht um unsere wirtschaftlichen Existenzen . Damit ist die Aufgabe , vor der wir stehen , beschrieben .',\n",
       " 'tokens_extended': ['Dazu',\n",
       "  'muss',\n",
       "  'man',\n",
       "  'nur',\n",
       "  'mit',\n",
       "  'den',\n",
       "  'Landwirten',\n",
       "  'sprechen',\n",
       "  ',',\n",
       "  'die',\n",
       "  'sagen',\n",
       "  ':',\n",
       "  'Ja',\n",
       "  ',',\n",
       "  'auch',\n",
       "  'früher',\n",
       "  'gab',\n",
       "  'es',\n",
       "  'extreme',\n",
       "  'Ereignisse',\n",
       "  ',',\n",
       "  'auch',\n",
       "  'früher',\n",
       "  'gab',\n",
       "  'es',\n",
       "  'Naturkatastrophen',\n",
       "  ',',\n",
       "  'aber',\n",
       "  'in',\n",
       "  'einem',\n",
       "  'Jahr',\n",
       "  'den',\n",
       "  'Hagel',\n",
       "  ',',\n",
       "  'im',\n",
       "  'anderen',\n",
       "  'Jahr',\n",
       "  'eine',\n",
       "  'Dürre',\n",
       "  'und',\n",
       "  'im',\n",
       "  'dritten',\n",
       "  'Jahr',\n",
       "  ',',\n",
       "  'wie',\n",
       "  'in',\n",
       "  'diesem',\n",
       "  'Jahr',\n",
       "  ',',\n",
       "  'die',\n",
       "  'Frostschäden',\n",
       "  ',',\n",
       "  'unter',\n",
       "  'denen',\n",
       "  'die',\n",
       "  'Obstbauern',\n",
       "  'zu',\n",
       "  'leiden',\n",
       "  'hatten',\n",
       "  ',',\n",
       "  'diese',\n",
       "  'Häufung',\n",
       "  'hatten',\n",
       "  'wir',\n",
       "  'früher',\n",
       "  'so',\n",
       "  'nicht',\n",
       "  ',',\n",
       "  'also',\n",
       "  'tut',\n",
       "  'etwas',\n",
       "  'gegen',\n",
       "  'den',\n",
       "  'Klimawandel',\n",
       "  '.',\n",
       "  'Es',\n",
       "  'geht',\n",
       "  'um',\n",
       "  'unsere',\n",
       "  'wirtschaftlichen',\n",
       "  'Existenzen',\n",
       "  '.',\n",
       "  'Damit',\n",
       "  'ist',\n",
       "  'die',\n",
       "  'Aufgabe',\n",
       "  ',',\n",
       "  'vor',\n",
       "  'der',\n",
       "  'wir',\n",
       "  'stehen',\n",
       "  ',',\n",
       "  'beschrieben',\n",
       "  '.'],\n",
       " 'sentence_extended_ids': [15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  16,\n",
       "  16,\n",
       "  16,\n",
       "  16,\n",
       "  16,\n",
       "  16,\n",
       "  16,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17],\n",
       " 'ptc': [[], []],\n",
       " 'ptc_mapped': [[], []],\n",
       " 'evidence': [[], []],\n",
       " 'evidence_mapped': [[], []],\n",
       " 'medium': [[], []],\n",
       " 'medium_mapped': [[], []],\n",
       " 'topic': [[], []],\n",
       " 'topic_mapped': [[], []],\n",
       " 'cue': [['15:7'], ['15:10']],\n",
       " 'cue_mapped': [['sprechen'], ['sagen']],\n",
       " 'addr': [['15:4', '15:5', '15:6'], []],\n",
       " 'addr_mapped': [['mit', 'den', 'Landwirten'], []],\n",
       " 'message': [[],\n",
       "  ['15:12',\n",
       "   '15:13',\n",
       "   '15:14',\n",
       "   '15:15',\n",
       "   '15:16',\n",
       "   '15:17',\n",
       "   '15:18',\n",
       "   '15:19',\n",
       "   '15:20',\n",
       "   '15:21',\n",
       "   '15:22',\n",
       "   '15:23',\n",
       "   '15:24',\n",
       "   '15:25',\n",
       "   '15:26',\n",
       "   '15:27',\n",
       "   '15:28',\n",
       "   '15:29',\n",
       "   '15:30',\n",
       "   '15:31',\n",
       "   '15:32',\n",
       "   '15:33',\n",
       "   '15:34',\n",
       "   '15:35',\n",
       "   '15:36',\n",
       "   '15:37',\n",
       "   '15:38',\n",
       "   '15:39',\n",
       "   '15:40',\n",
       "   '15:41',\n",
       "   '15:42',\n",
       "   '15:43',\n",
       "   '15:44',\n",
       "   '15:45',\n",
       "   '15:46',\n",
       "   '15:47',\n",
       "   '15:48',\n",
       "   '15:49',\n",
       "   '15:50',\n",
       "   '15:51',\n",
       "   '15:52',\n",
       "   '15:53',\n",
       "   '15:54',\n",
       "   '15:55',\n",
       "   '15:56',\n",
       "   '15:57',\n",
       "   '15:58',\n",
       "   '15:59',\n",
       "   '15:60',\n",
       "   '15:61',\n",
       "   '15:62',\n",
       "   '15:63',\n",
       "   '15:64',\n",
       "   '15:65',\n",
       "   '15:66',\n",
       "   '15:67',\n",
       "   '15:68',\n",
       "   '15:69',\n",
       "   '15:70',\n",
       "   '15:71',\n",
       "   '15:72',\n",
       "   '15:73',\n",
       "   '15:74',\n",
       "   '16:0',\n",
       "   '16:1',\n",
       "   '16:2',\n",
       "   '16:3',\n",
       "   '16:4',\n",
       "   '16:5']],\n",
       " 'message_mapped': [[],\n",
       "  ['Ja',\n",
       "   ',',\n",
       "   'auch',\n",
       "   'früher',\n",
       "   'gab',\n",
       "   'es',\n",
       "   'extreme',\n",
       "   'Ereignisse',\n",
       "   ',',\n",
       "   'auch',\n",
       "   'früher',\n",
       "   'gab',\n",
       "   'es',\n",
       "   'Naturkatastrophen',\n",
       "   ',',\n",
       "   'aber',\n",
       "   'in',\n",
       "   'einem',\n",
       "   'Jahr',\n",
       "   'den',\n",
       "   'Hagel',\n",
       "   ',',\n",
       "   'im',\n",
       "   'anderen',\n",
       "   'Jahr',\n",
       "   'eine',\n",
       "   'Dürre',\n",
       "   'und',\n",
       "   'im',\n",
       "   'dritten',\n",
       "   'Jahr',\n",
       "   ',',\n",
       "   'wie',\n",
       "   'in',\n",
       "   'diesem',\n",
       "   'Jahr',\n",
       "   ',',\n",
       "   'die',\n",
       "   'Frostschäden',\n",
       "   ',',\n",
       "   'unter',\n",
       "   'denen',\n",
       "   'die',\n",
       "   'Obstbauern',\n",
       "   'zu',\n",
       "   'leiden',\n",
       "   'hatten',\n",
       "   ',',\n",
       "   'diese',\n",
       "   'Häufung',\n",
       "   'hatten',\n",
       "   'wir',\n",
       "   'früher',\n",
       "   'so',\n",
       "   'nicht',\n",
       "   ',',\n",
       "   'also',\n",
       "   'tut',\n",
       "   'etwas',\n",
       "   'gegen',\n",
       "   'den',\n",
       "   'Klimawandel',\n",
       "   '.',\n",
       "   'Es',\n",
       "   'geht',\n",
       "   'um',\n",
       "   'unsere',\n",
       "   'wirtschaftlichen',\n",
       "   'Existenzen']],\n",
       " 'source': [['15:2'], ['15:9']],\n",
       " 'source_mapped': [['man'], ['die']]}"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[15]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build lmsys format json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_cues_to_string(mapped):\n",
    "    if mapped == []:\n",
    "        return \"#UNK#\"\n",
    "    return \", \".join([\"[\" + \", \".join(val) + \"]\" for val in mapped])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_roles_to_string(mapped):\n",
    "    if mapped == []:\n",
    "        return \"#UNK#\"\n",
    "    return \", \".join(mapped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lmsys_format(train_ds, val_ds):\n",
    "    result = []\n",
    "\n",
    "    index = 0\n",
    "    for row in concatenate_datasets([train_ds, val_ds]):\n",
    "        if len(row[\"cue_mapped\"]) == 0:\n",
    "            element = {\"id\": \"identity_\" + str(index)}\n",
    "            index += 1\n",
    "            conversations = [\n",
    "                {\n",
    "                    \"from\": \"human\",\n",
    "                    \"value\": 'A cue is the lexical items in a sentence that indicate that speech, writing, or thought is being reproduced.\\nI want you to extract all cuee in the text below.\\nIf you find multiple words for one cue, you output them separated by commas.\\nIf no cue can be found in the given text, you output the string #UNK# as cue.\\nNow extract all cues from the following sentence.\\nUse the prefix \"Cues: \".\\nSentence: '\n",
    "                    + row[\"Sentence\"],\n",
    "                },\n",
    "                {\n",
    "                    \"from\": \"gpt\",\n",
    "                    \"value\": \"Cues: \" + map_cues_to_string(row[\"cue_mapped\"]),\n",
    "                },\n",
    "            ]\n",
    "            element[\"conversations\"] = conversations\n",
    "            result.append(element)\n",
    "            continue\n",
    "        for i, cue in enumerate(row[\"cue_mapped\"]):\n",
    "            element = {\"id\": \"identity_\" + str(index)}\n",
    "            index += 1\n",
    "            conversations = [\n",
    "                {\n",
    "                    \"from\": \"human\",\n",
    "                    \"value\": 'A cue is the lexical items in a sentence that indicate that speech, writing, or thought is being reproduced.\\nI want you to extract all cuee in the text below.\\nIf you find multiple words for one cue, you output them separated by commas.\\nIf no cue can be found in the given text, you output the string #UNK# as cue.\\nNow extract all cues from the following sentence.\\nUse the prefix \"Cues: \".\\nSentence: '\n",
    "                    + row[\"Sentence\"],\n",
    "                },\n",
    "                {\n",
    "                    \"from\": \"gpt\",\n",
    "                    \"value\": \"Cues: \" + map_cues_to_string(row[\"cue_mapped\"]),\n",
    "                },\n",
    "                {\n",
    "                    \"from\": \"human\",\n",
    "                    \"value\": \"Now I give you again the sentence only in addition with the two following sentences, because the roles can be partially contained in the following sentences.\\nText: \"\n",
    "                    + row[\"sentence_extended\"]\n",
    "                    + \"\\n\\nNow find all roles in the sentence associated with the cue '\"\n",
    "                    + \", \".join(cue)\n",
    "                    + \"' you found in the beginning sentence.\",\n",
    "                },\n",
    "                {\n",
    "                    \"from\": \"gpt\",\n",
    "                    \"value\": \"cue: \"\n",
    "                    + \", \".join(cue)\n",
    "                    + \"\\nptc: \"\n",
    "                    + map_roles_to_string(row[\"ptc_mapped\"][i])\n",
    "                    + \"\\nevidence: \"\n",
    "                    + map_roles_to_string(row[\"evidence_mapped\"][i])\n",
    "                    + \"\\nmedium: \"\n",
    "                    + map_roles_to_string(row[\"medium_mapped\"][i])\n",
    "                    + \"\\ntopic: \"\n",
    "                    + map_roles_to_string(row[\"topic_mapped\"][i])\n",
    "                    + \"\\naddr: \"\n",
    "                    + map_roles_to_string(row[\"addr_mapped\"][i])\n",
    "                    + \"\\nmessage: \"\n",
    "                    + map_roles_to_string(row[\"message_mapped\"][i])\n",
    "                    + \"\\nsource: \"\n",
    "                    + map_roles_to_string(row[\"source_mapped\"][i]),\n",
    "                },\n",
    "            ]\n",
    "            element[\"conversations\"] = conversations\n",
    "            result.append(element)\n",
    "\n",
    "    with open(\"lmsys.json\", \"w\", encoding=\"utf8\") as outfile:\n",
    "        json.dump(result, outfile, indent=3, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_lmsys_format(train_ds, val_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QLoRA Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
