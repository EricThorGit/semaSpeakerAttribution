{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "\n",
    "from datasets import (\n",
    "    load_dataset,\n",
    "    concatenate_datasets,\n",
    "    load_from_disk,\n",
    "    Features,\n",
    "    Sequence,\n",
    "    Value,\n",
    "    logging,\n",
    ")\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from qlora import train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.set_verbosity_error()\n",
    "logging.disable_progress_bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_annotations_from_file(path: str, file: str):\n",
    "    features = Features(\n",
    "        {\n",
    "            \"PTC\": Sequence(feature=Value(dtype=\"string\", id=None), length=-1, id=None),\n",
    "            \"Evidence\": Sequence(\n",
    "                feature=Value(dtype=\"string\", id=None), length=-1, id=None\n",
    "            ),\n",
    "            \"Medium\": Sequence(\n",
    "                feature=Value(dtype=\"string\", id=None), length=-1, id=None\n",
    "            ),\n",
    "            \"Topic\": Sequence(\n",
    "                feature=Value(dtype=\"string\", id=None), length=-1, id=None\n",
    "            ),\n",
    "            \"Cue\": Sequence(feature=Value(dtype=\"string\", id=None), length=-1, id=None),\n",
    "            \"Addr\": Sequence(\n",
    "                feature=Value(dtype=\"string\", id=None), length=-1, id=None\n",
    "            ),\n",
    "            \"Message\": Sequence(\n",
    "                feature=Value(dtype=\"string\", id=None), length=-1, id=None\n",
    "            ),\n",
    "            \"Source\": Sequence(\n",
    "                feature=Value(dtype=\"string\", id=None), length=-1, id=None\n",
    "            ),\n",
    "        }\n",
    "    )\n",
    "    ds = load_dataset(\n",
    "        \"json\",\n",
    "        data_files=os.path.join(path, file),\n",
    "        field=\"Annotations\",\n",
    "        split=\"train\",\n",
    "        features=features,\n",
    "    )\n",
    "    ds = ds.add_column(\"FileName\", [file] * len(ds))\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sentences_from_file(path: str, file: str):\n",
    "    ds = load_dataset(\n",
    "        \"json\", data_files=os.path.join(path, file), field=\"Sentences\", split=\"train\"\n",
    "    )\n",
    "    ds = ds.add_column(\"FileName\", [file] * len(ds))\n",
    "    ds = ds.add_column(\"Sentence\", [\" \".join(t) for t in ds[\"Tokens\"]])\n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_annotations_from_path(path: str):\n",
    "    dataset = None\n",
    "\n",
    "    for file in tqdm(sorted(os.listdir(path))):\n",
    "        if not dataset:\n",
    "            dataset = read_annotations_from_file(path, file)\n",
    "        else:\n",
    "            dataset = concatenate_datasets(\n",
    "                [dataset, read_annotations_from_file(path, file)]\n",
    "            )\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sentences_from_path(path: str):\n",
    "    dataset = None\n",
    "\n",
    "    for file in tqdm(sorted(os.listdir(path))):\n",
    "        if not dataset:\n",
    "            dataset = read_sentences_from_file(path, file)\n",
    "        else:\n",
    "            dataset = concatenate_datasets(\n",
    "                [dataset, read_sentences_from_file(path, file)]\n",
    "            )\n",
    "\n",
    "    dataset = dataset.add_column(\"id\", range(len(dataset)))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sentences_dataset(ds_name: str):\n",
    "    path_to_dataset = \"./transformed_datasets/\" + ds_name + \"/sentences\"\n",
    "\n",
    "    if os.path.isdir(path_to_dataset):\n",
    "        result = load_from_disk(path_to_dataset)\n",
    "    else:\n",
    "        result = read_sentences_from_path(\n",
    "            \"./SpkAtt-2023/data/\"\n",
    "            + ds_name\n",
    "            + \"/task1\"\n",
    "            + (\"_test/\" if ds_name == \"eval\" else \"/\")\n",
    "        )\n",
    "        os.makedirs(path_to_dataset, exist_ok=True)\n",
    "        result.save_to_disk(path_to_dataset)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_annotations_dataset(ds_name: str):\n",
    "    path_to_dataset = \"./transformed_datasets/\" + ds_name + \"/annotations\"\n",
    "\n",
    "    if os.path.isdir(path_to_dataset):\n",
    "        return load_from_disk(path_to_dataset)\n",
    "\n",
    "    result = read_annotations_from_path(\n",
    "        \"./SpkAtt-2023/data/\"\n",
    "        + ds_name\n",
    "        + \"/task1\"\n",
    "        + (\"_test/\" if ds_name == \"eval\" else \"/\")\n",
    "    )\n",
    "    os.makedirs(path_to_dataset, exist_ok=True)\n",
    "    result.save_to_disk(path_to_dataset)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences_dataset = read_sentences_dataset(\"train\")\n",
    "val_sentences_dataset = read_sentences_dataset(\"dev\")\n",
    "test_sentences_dataset = read_sentences_dataset(\"eval\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_annotations_dataset = read_annotations_dataset(\"train\")\n",
    "val_annotations_dataset = read_annotations_dataset(\"dev\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format datasets for usage in langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_from_label(train_sentences_dataset, row, annotations):\n",
    "    tokens = []\n",
    "    for anno in annotations:\n",
    "        if int(anno.split(\":\")[0]) == row[\"SentenceId\"]:\n",
    "            tokens.append(row[\"Tokens\"][int(anno.split(\":\")[1])])\n",
    "        else:\n",
    "            temp_row = train_sentences_dataset.filter(\n",
    "                lambda r: r[\"FileName\"] == row[\"FileName\"]\n",
    "                          and r[\"SentenceId\"] == int(anno.split(\":\")[0])\n",
    "            )[0]\n",
    "            tokens.append(temp_row[\"Tokens\"][int(anno.split(\":\")[1])])\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_complete_dataset(sentences_dataset, annotations_dataset, dataset_name):\n",
    "    path_to_dataset = \"./transformed_datasets/\" + dataset_name + \"/complete\"\n",
    "    if os.path.isdir(path_to_dataset):\n",
    "        return load_from_disk(path_to_dataset)\n",
    "\n",
    "    ptc, ptc_temp, ptc_mapped, ptc_mapped_temp = [], [], [], []\n",
    "    evidence, evidence_temp, evidence_mapped, evidence_mapped_temp = [], [], [], []\n",
    "    medium, medium_temp, medium_mapped, medium_mapped_temp = [], [], [], []\n",
    "    topic, topic_temp, topic_mapped, topic_mapped_temp = [], [], [], []\n",
    "    cue, cue_temp, cue_mapped, cue_mapped_temp = [], [], [], []\n",
    "    addr, addr_temp, addr_mapped, addr_mapped_temp = [], [], [], []\n",
    "    message, message_temp, message_mapped, message_mapped_temp = [], [], [], []\n",
    "    source, source_temp, source_mapped, source_mapped_temp = [], [], [], []\n",
    "    (\n",
    "        sentence_extended,\n",
    "        tokens_extended,\n",
    "        sentence_extended_ids,\n",
    "    ) = (\n",
    "        [],\n",
    "        [],\n",
    "        [],\n",
    "    )\n",
    "\n",
    "    index_in_anno_ds = 0\n",
    "\n",
    "    for i, row in tqdm(enumerate(sentences_dataset)):\n",
    "        context = row[\"Sentence\"]\n",
    "        tokens = row[\"Tokens\"]\n",
    "        ids = [row[\"SentenceId\"]] * len(row[\"Tokens\"])\n",
    "        if (\n",
    "                i + 1 < len(sentences_dataset)\n",
    "                and sentences_dataset[i + 1][\"FileName\"] == row[\"FileName\"]\n",
    "        ):\n",
    "            context = context + \" \" + sentences_dataset[i + 1][\"Sentence\"]\n",
    "            tokens.extend(sentences_dataset[i + 1][\"Tokens\"])\n",
    "            ids.extend(\n",
    "                [sentences_dataset[i + 1][\"SentenceId\"]]\n",
    "                * len(sentences_dataset[i + 1][\"Tokens\"])\n",
    "            )\n",
    "        if (\n",
    "                i + 2 < len(sentences_dataset)\n",
    "                and sentences_dataset[i + 2][\"FileName\"] == row[\"FileName\"]\n",
    "        ):\n",
    "            context = context + \" \" + sentences_dataset[i + 2][\"Sentence\"]\n",
    "            tokens.extend(sentences_dataset[i + 2][\"Tokens\"])\n",
    "            ids.extend(\n",
    "                [sentences_dataset[i + 2][\"SentenceId\"]]\n",
    "                * len(sentences_dataset[i + 2][\"Tokens\"])\n",
    "            )\n",
    "        sentence_extended.append(context)\n",
    "        tokens_extended.append(tokens)\n",
    "        sentence_extended_ids.append(ids)\n",
    "\n",
    "        if annotations_dataset is not None:\n",
    "            id_of_next_sentence_with_annotation = (\n",
    "                int(annotations_dataset[index_in_anno_ds][\"Cue\"][0].split(\":\")[0])\n",
    "                if index_in_anno_ds != len(annotations_dataset)\n",
    "                else -1\n",
    "            )\n",
    "\n",
    "            if row[\"SentenceId\"] != id_of_next_sentence_with_annotation:\n",
    "                ptc.append([])\n",
    "                ptc_mapped.append([])\n",
    "                evidence.append([])\n",
    "                evidence_mapped.append([])\n",
    "                medium.append([])\n",
    "                medium_mapped.append([])\n",
    "                topic.append([])\n",
    "                topic_mapped.append([])\n",
    "                cue.append([])\n",
    "                cue_mapped.append([])\n",
    "                addr.append([])\n",
    "                addr_mapped.append([])\n",
    "                message.append([])\n",
    "                message_mapped.append([])\n",
    "                source.append([])\n",
    "                source_mapped.append([])\n",
    "                continue\n",
    "\n",
    "            while row[\"SentenceId\"] == id_of_next_sentence_with_annotation:\n",
    "                ptc_temp.append(annotations_dataset[index_in_anno_ds][\"PTC\"])\n",
    "                evidence_temp.append(annotations_dataset[index_in_anno_ds][\"Evidence\"])\n",
    "                medium_temp.append(annotations_dataset[index_in_anno_ds][\"Medium\"])\n",
    "                topic_temp.append(annotations_dataset[index_in_anno_ds][\"Topic\"])\n",
    "                cue_temp.append(annotations_dataset[index_in_anno_ds][\"Cue\"])\n",
    "                addr_temp.append(annotations_dataset[index_in_anno_ds][\"Addr\"])\n",
    "                message_temp.append(annotations_dataset[index_in_anno_ds][\"Message\"])\n",
    "                source_temp.append(annotations_dataset[index_in_anno_ds][\"Source\"])\n",
    "\n",
    "                ptc_mapped_temp.append(\n",
    "                    get_text_from_label(sentences_dataset, row, ptc_temp[-1])\n",
    "                )\n",
    "                evidence_mapped_temp.append(\n",
    "                    get_text_from_label(sentences_dataset, row, evidence_temp[-1])\n",
    "                )\n",
    "                medium_mapped_temp.append(\n",
    "                    get_text_from_label(sentences_dataset, row, medium_temp[-1])\n",
    "                )\n",
    "                topic_mapped_temp.append(\n",
    "                    get_text_from_label(sentences_dataset, row, topic_temp[-1])\n",
    "                )\n",
    "                cue_mapped_temp.append(\n",
    "                    get_text_from_label(sentences_dataset, row, cue_temp[-1])\n",
    "                )\n",
    "                addr_mapped_temp.append(\n",
    "                    get_text_from_label(sentences_dataset, row, addr_temp[-1])\n",
    "                )\n",
    "                message_mapped_temp.append(\n",
    "                    get_text_from_label(sentences_dataset, row, message_temp[-1])\n",
    "                )\n",
    "                source_mapped_temp.append(\n",
    "                    get_text_from_label(sentences_dataset, row, source_temp[-1])\n",
    "                )\n",
    "\n",
    "                index_in_anno_ds += 1\n",
    "                if index_in_anno_ds == len(annotations_dataset):\n",
    "                    break\n",
    "                id_of_next_sentence_with_annotation = int(\n",
    "                    annotations_dataset[index_in_anno_ds][\"Cue\"][0].split(\":\")[0]\n",
    "                )\n",
    "\n",
    "            ptc.append(ptc_temp)\n",
    "            ptc_mapped.append(ptc_mapped_temp)\n",
    "            evidence.append(evidence_temp)\n",
    "            evidence_mapped.append(evidence_mapped_temp)\n",
    "            medium.append(medium_temp)\n",
    "            medium_mapped.append(medium_mapped_temp)\n",
    "            topic.append(topic_temp)\n",
    "            topic_mapped.append(topic_mapped_temp)\n",
    "            cue.append(cue_temp)\n",
    "            cue_mapped.append(cue_mapped_temp)\n",
    "            addr.append(addr_temp)\n",
    "            addr_mapped.append(addr_mapped_temp)\n",
    "            message.append(message_temp)\n",
    "            message_mapped.append(message_mapped_temp)\n",
    "            source.append(source_temp)\n",
    "            source_mapped.append(source_mapped_temp)\n",
    "\n",
    "            ptc_temp, ptc_mapped_temp = [], []\n",
    "            evidence_temp, evidence_mapped_temp = [], []\n",
    "            medium_temp, medium_mapped_temp = [], []\n",
    "            topic_temp, topic_mapped_temp = [], []\n",
    "            cue_temp, cue_mapped_temp = [], []\n",
    "            addr_temp, addr_mapped_temp = [], []\n",
    "            message_temp, message_mapped_temp = [], []\n",
    "            source_temp, source_mapped_temp = [], []\n",
    "\n",
    "    res = sentences_dataset.add_column(\"sentence_extended\", sentence_extended)\n",
    "    res = res.add_column(\"tokens_extended\", tokens_extended)\n",
    "    res = res.add_column(\"sentence_extended_ids\", sentence_extended_ids)\n",
    "\n",
    "    if annotations_dataset is not None:\n",
    "        res = res.add_column(\"ptc\", ptc)\n",
    "        res = res.add_column(\"ptc_mapped\", ptc_mapped)\n",
    "        res = res.add_column(\"evidence\", evidence)\n",
    "        res = res.add_column(\"evidence_mapped\", evidence_mapped)\n",
    "        res = res.add_column(\"medium\", medium)\n",
    "        res = res.add_column(\"medium_mapped\", medium_mapped)\n",
    "        res = res.add_column(\"topic\", topic)\n",
    "        res = res.add_column(\"topic_mapped\", topic_mapped)\n",
    "        res = res.add_column(\"cue\", cue)\n",
    "        res = res.add_column(\"cue_mapped\", cue_mapped)\n",
    "        res = res.add_column(\"addr\", addr)\n",
    "        res = res.add_column(\"addr_mapped\", addr_mapped)\n",
    "        res = res.add_column(\"message\", message)\n",
    "        res = res.add_column(\"message_mapped\", message_mapped)\n",
    "        res = res.add_column(\"source\", source)\n",
    "        res = res.add_column(\"source_mapped\", source_mapped)\n",
    "\n",
    "    os.makedirs(path_to_dataset, exist_ok=True)\n",
    "    res.save_to_disk(path_to_dataset)\n",
    "\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = build_complete_dataset(\n",
    "    train_sentences_dataset, train_annotations_dataset, \"train\"\n",
    ")\n",
    "val_ds = build_complete_dataset(val_sentences_dataset, val_annotations_dataset, \"dev\")\n",
    "test_ds = build_complete_dataset(test_sentences_dataset, None, \"eval\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = test_sentences_dataset.rename_column(\"Sentence\", \"Satz\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Showcase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds[52]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds[15]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build lmsys format json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_cues_to_string(mapped):\n",
    "    if mapped == []:\n",
    "        return \"#UNK#\"\n",
    "    return \", \".join([\"[\" + \", \".join(val) + \"]\" for val in mapped])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_roles_to_string(mapped):\n",
    "    if mapped == []:\n",
    "        return \"#UNK#\"\n",
    "    return \", \".join(mapped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmsys_data_path = './lmsys.json'\n",
    "\n",
    "\n",
    "def build_lmsys_format(train_ds, val_ds):\n",
    "    result = []\n",
    "\n",
    "    index = 0\n",
    "    for row in concatenate_datasets([train_ds, val_ds]):\n",
    "        if len(row[\"cue_mapped\"]) == 0:\n",
    "            element = {\"id\": \"identity_\" + str(index)}\n",
    "            index += 1\n",
    "            conversations = [\n",
    "                {\n",
    "                    \"from\": \"human\",\n",
    "                    \"value\": 'A cue is the lexical items in a sentence that indicate that speech, writing, or thought is being reproduced.\\nI want you to extract all cues in the text below.\\nIf you find multiple words for one cue, you output them separated by commas.\\nIf no cue can be found in the given text, you output the string #UNK# as cue.\\nNow extract all cues from the following sentence.\\nUse the prefix \"Cues: \".\\nSentence: '\n",
    "                             + row[\"Sentence\"],\n",
    "                },\n",
    "                {\n",
    "                    \"from\": \"gpt\",\n",
    "                    \"value\": \"Cues: \" + map_cues_to_string(row[\"cue_mapped\"]),\n",
    "                },\n",
    "            ]\n",
    "            element[\"conversations\"] = conversations\n",
    "            result.append(element)\n",
    "            continue\n",
    "        for i, cue in enumerate(row[\"cue_mapped\"]):\n",
    "            element = {\"id\": \"identity_\" + str(index)}\n",
    "            index += 1\n",
    "            conversations = [\n",
    "                {\n",
    "                    \"from\": \"human\",\n",
    "                    \"value\": 'A cue is the lexical items in a sentence that indicate that speech, writing, or thought is being reproduced.\\nI want you to extract all cues in the text below.\\nIf you find multiple words for one cue, you output them separated by commas.\\nIf no cue can be found in the given text, you output the string #UNK# as cue.\\nNow extract all cues from the following sentence.\\nUse the prefix \"Cues: \".\\nSentence: '\n",
    "                             + row[\"Sentence\"],\n",
    "                },\n",
    "                {\n",
    "                    \"from\": \"gpt\",\n",
    "                    \"value\": \"Cues: \" + map_cues_to_string(row[\"cue_mapped\"]),\n",
    "                },\n",
    "                {\n",
    "                    \"from\": \"human\",\n",
    "                    \"value\": \"Now I give you again the sentence only in addition with the two following sentences, because the roles can be partially contained in the following sentences.\\nText: \"\n",
    "                             + row[\"sentence_extended\"]\n",
    "                             + \"\\n\\nNow find all roles in the sentence associated with the cue '\"\n",
    "                             + \", \".join(cue)\n",
    "                             + \"' you found in the beginning sentence.\",\n",
    "                },\n",
    "                {\n",
    "                    \"from\": \"gpt\",\n",
    "                    \"value\": \"cue: \"\n",
    "                             + \", \".join(cue)\n",
    "                             + \"\\nptc: \"\n",
    "                             + map_roles_to_string(row[\"ptc_mapped\"][i])\n",
    "                             + \"\\nevidence: \"\n",
    "                             + map_roles_to_string(row[\"evidence_mapped\"][i])\n",
    "                             + \"\\nmedium: \"\n",
    "                             + map_roles_to_string(row[\"medium_mapped\"][i])\n",
    "                             + \"\\ntopic: \"\n",
    "                             + map_roles_to_string(row[\"topic_mapped\"][i])\n",
    "                             + \"\\naddr: \"\n",
    "                             + map_roles_to_string(row[\"addr_mapped\"][i])\n",
    "                             + \"\\nmessage: \"\n",
    "                             + map_roles_to_string(row[\"message_mapped\"][i])\n",
    "                             + \"\\nsource: \"\n",
    "                             + map_roles_to_string(row[\"source_mapped\"][i]),\n",
    "                },\n",
    "            ]\n",
    "            element[\"conversations\"] = conversations\n",
    "            result.append(element)\n",
    "\n",
    "    with open(lmsys_data_path, \"w\", encoding=\"utf8\") as outfile:\n",
    "        json.dump(result, outfile, indent=3, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_lmsys_format(train_ds, val_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QLoRA Fine-Tuning\n",
    "## Parse data into required format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "parsed_cues_file = './transformed_datasets/prompts_training/parsed_data_cues.jsonl'\n",
    "parsed_roles_file = './transformed_datasets/prompts_training/parsed_data_roles.jsonl'\n",
    "os.makedirs(os.path.dirname(parsed_cues_file), exist_ok=True)\n",
    "os.makedirs(os.path.dirname(parsed_roles_file), exist_ok=True)\n",
    "\n",
    "# token to signal the end of the assistant's response\n",
    "separator = '</s>'\n",
    "\n",
    "# reload parsed data\n",
    "with open(lmsys_data_path) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# save parsed prompts separately\n",
    "all_prompts_cues = []\n",
    "all_prompts_roles = []\n",
    "for conversation in data:\n",
    "    # keep track of the complete conversation in order to generate the input of the prompts\n",
    "    complete_prompt = ''\n",
    "\n",
    "    for i, turn in enumerate(conversation['conversations']):\n",
    "        if turn['from'] == 'human':\n",
    "            complete_prompt += 'User: '\n",
    "            complete_prompt += turn['value']\n",
    "        elif turn['from'] == 'gpt':\n",
    "            complete_prompt += 'Assistant: '\n",
    "\n",
    "            # idea\n",
    "            # turn 0: user prompt for cues\n",
    "            # turn 1: assistant response with cues \n",
    "            #   --> create sample with the conversation up to this point as input and the cues as output\n",
    "            # turn 2: user prompt for roles for one specific cue\n",
    "            # turn 3: assistant response with roles \n",
    "            #   --> create sample with the conversation up to this point as input and the roles as output\n",
    "            # there should be no further turns because we split all conversations with multiple cues into separate conversations\n",
    "\n",
    "            sample = json.dumps({'input': complete_prompt, 'output': turn['value'] + separator})\n",
    "\n",
    "            if i == 1 and sample not in all_prompts_cues:\n",
    "                # turn 1: assistant response with cues\n",
    "                all_prompts_cues.append(sample)\n",
    "            elif i == 3 and sample not in all_prompts_cues:\n",
    "                # turn 3: assistant response with roles\n",
    "                all_prompts_roles.append(sample)\n",
    "            elif i != 1 and i != 3:\n",
    "                print('ERROR: each conversation should maximally contain 4 turns'\n",
    "                      ' and only turn 1 and 3 should be responses by the assistant')\n",
    "\n",
    "            complete_prompt += turn['value'] + separator\n",
    "        complete_prompt += '\\n'\n",
    "\n",
    "# write parsed prompts to files\n",
    "with open(parsed_cues_file, 'w') as f:\n",
    "    f.write('\\n'.join(all_prompts_cues))\n",
    "\n",
    "with open(parsed_roles_file, 'w') as f:\n",
    "    f.write('\\n'.join(all_prompts_roles))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# check that the file with the cue prompts was written correctly\n",
    "with open(parsed_cues_file) as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "print(f'Number of samples: {len(lines)}\\n')\n",
    "\n",
    "print('First 5 samples:')\n",
    "for l in lines[:5]:\n",
    "    print('=== in: ===\\n' + json.loads(l)['input'] + '\\n')\n",
    "    print('=== out: ===\\n' + json.loads(l)['output'] + '\\n')\n",
    "    print()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# check that the file with the role prompts was written correctly\n",
    "with open(parsed_roles_file) as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "print(f'Number of samples: {len(lines)}\\n')\n",
    "\n",
    "print('First 5 samples:')\n",
    "for l in lines[:5]:\n",
    "    print('=== in: ===\\n' + json.loads(l)['input'] + '\\n')\n",
    "    print('=== out: ===\\n' + json.loads(l)['output'] + '\\n')\n",
    "    print()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Check optimal source and target lengths\n",
    "\n",
    "This step is only required if you want to use your own data. If you use the original GermEval 2023 task 1 data, you can skip this step and use the source and target lengths that are already defined in the config files in the `configs` folder (parameters `source_max_len` and `target_max_len`).\n",
    "\n",
    "If you want to change the maximum source or target lengths, keep in mind that longer prompts mean longer training times and more memory requirements. While it would be best to set the maximum source/target lengths to the maximum lengths of the inputs/outputs, this is not always feasible due to memory constraints. In this case, we recommend choosing maximum lengths that only truncate few samples. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# encode all prompt inputs with the Llama 1 tokenizer (same as the Llama 2 tokenizer)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'huggyllama/llama-7b',\n",
    "    padding_side=\"right\",\n",
    "    use_fast=False,\n",
    "    tokenizer_type='llama'\n",
    ")\n",
    "\n",
    "encoded_inputs_cues = []\n",
    "encoded_inputs_roles = []\n",
    "encoded_outputs_cues = []\n",
    "encoded_outputs_roles = []\n",
    "with open(parsed_cues_file) as f:\n",
    "    for l in f.readlines():\n",
    "        enc_in = tokenizer.encode(json.loads(l)['input'])\n",
    "        encoded_inputs_cues.append(enc_in)\n",
    "        enc_out = tokenizer.encode(json.loads(l)['output'])\n",
    "        encoded_outputs_cues.append(enc_out)\n",
    "with open(parsed_roles_file) as f:\n",
    "    for l in f.readlines():\n",
    "        enc_in = tokenizer.encode(json.loads(l)['input'])\n",
    "        encoded_inputs_roles.append(enc_in)\n",
    "        enc_out = tokenizer.encode(json.loads(l)['output'])\n",
    "        encoded_outputs_roles.append(enc_out)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# maximum source lengths taken from the config files\n",
    "max_length_source_cues = 256\n",
    "max_length_source_roles = 640\n",
    "\n",
    "print('cues source lengths')\n",
    "len_enc = [len(e) for e in encoded_inputs_cues]\n",
    "print(f'max length: {max(len_enc)}')\n",
    "print(f'mean length: {np.mean(len_enc)}')\n",
    "print(f'number of samples longer than {max_length_source_cues}: {sum(np.array(len_enc) > max_length_source_cues)}')\n",
    "print()\n",
    "\n",
    "print('roles source lengths')\n",
    "len_enc = [len(e) for e in encoded_inputs_roles]\n",
    "print(f'max length: {max(len_enc)}')\n",
    "print(f'mean length: {np.mean(len_enc)}')\n",
    "print(f'number of samples longer than {max_length_source_roles}: {sum(np.array(len_enc) > max_length_source_roles)}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# maximum target lengths taken from the config files\n",
    "max_length_target_cues = 64\n",
    "max_length_target_roles = 256\n",
    "\n",
    "print('cues target lengths')\n",
    "len_enc = [len(e) for e in encoded_outputs_cues]\n",
    "print(f'max length: {max(len_enc)}')\n",
    "print(f'mean length: {np.mean(len_enc)}')\n",
    "print(f'number of samples longer than {max_length_target_cues}: {sum(np.array(len_enc) > max_length_target_cues)}')\n",
    "print()\n",
    "\n",
    "print('roles target lengths')\n",
    "len_enc = [len(e) for e in encoded_outputs_roles]\n",
    "print(f'max length: {max(len_enc)}')\n",
    "print(f'mean length: {np.mean(len_enc)}')\n",
    "print(f'number of samples longer than {max_length_target_roles}: {sum(np.array(len_enc) > max_length_target_roles)}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train models\n",
    "\n",
    "This step can be skipped if you already have trained models.\n",
    "\n",
    "For training, you first have to prepare the Llama 2 models and adapt the configuration. To prepare the Llama 2 models, you will have to make them accessible in HF (Huggingface) format. You can either use the models directly from Huggingface or prepare them yourself by first downloading the model weights from [the official Llama repo](https://github.com/facebookresearch/llama) and then converting these weights using their [conversion manual](https://github.com/facebookresearch/llama-recipes/#model-conversion-to-hugging-face). If you don't want to use the models from Huggingface, once you have prepared the models yourself, update the path to the models in the config files (parameter `model_name_or_path`) in the `configs` folder so the paths point to the folder containing the `pytorch_model-000xx-of-00015.bin` files.\n",
    "\n",
    "Further configuration parameters:\n",
    "* `per_device_train_batch_size` and `gradient_accumulation_steps`: With these two parameters you can control the batch size and the number of accumulation steps when calculating the gradients during training. Larger batch sizes should speed up training, but increase memory requirements considerably. We recommend choosing the parameters so that their product `per_device_train_batch_size * gradient_accumulation_steps` is a multiple of 16.\n",
    "* `save_steps` and `max_steps`: set `max_steps` to control the length of training (`save_steps` determines when checkpoints are created)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optionally set CUDA devices that should be used for training\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2\"\n",
    "\n",
    "# define config files for training\n",
    "# 7B models\n",
    "cues_training_config = \"./configs/7b_cues.args\"\n",
    "roles_training_config = \"./configs/7b_roles.args\"\n",
    "\n",
    "# 70B models\n",
    "# cues_training_config = \"./configs/70b_cues.args\"\n",
    "# roles_training_config = \"./configs/70b_roles.args\"\n",
    "\n",
    "train(cues_training_config)\n",
    "train(roles_training_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
