{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set gpus for qlora training\n",
    "import os\n",
    "\n",
    "# os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device map for 7b model\n",
    "device_map = {\n",
    "    \"model.embed_tokens\": 0,\n",
    "    \"model.layers.0\": 0,\n",
    "    \"model.layers.1\": 0,\n",
    "    \"model.layers.2\": 0,\n",
    "    \"model.layers.3\": 0,\n",
    "    \"model.layers.4\": 0,\n",
    "    \"model.layers.5\": 0,\n",
    "    \"model.layers.6\": 0,\n",
    "    \"model.layers.7\": 0,\n",
    "    \"model.layers.8\": 0,\n",
    "    \"model.layers.9\": 0,\n",
    "    \"model.layers.10\": 0,\n",
    "    \"model.layers.11\": 0,\n",
    "    \"model.layers.12\": 0,\n",
    "    \"model.layers.13\": 0,\n",
    "    \"model.layers.14\": 0,\n",
    "    \"model.layers.15\": 0,\n",
    "    \"model.layers.16\": 0,\n",
    "    \"model.layers.17\": 0,\n",
    "    \"model.layers.18\": 0,\n",
    "    \"model.layers.19\": 0,\n",
    "    \"model.layers.20\": 0,\n",
    "    \"model.layers.21\": 0,\n",
    "    \"model.layers.22\": 0,\n",
    "    \"model.layers.23\": 0,\n",
    "    \"model.layers.24\": 0,\n",
    "    \"model.layers.25\": 0,\n",
    "    \"model.layers.26\": 0,\n",
    "    \"model.layers.27\": 0,\n",
    "    \"model.layers.28\": 0,\n",
    "    \"model.layers.29\": 0,\n",
    "    \"model.layers.30\": 0,\n",
    "    \"model.layers.31\": 0,\n",
    "    \"model.norm\": 0,\n",
    "    \"lm_head\": 0,\n",
    "}\n",
    "\n",
    "# device map for 70b model\n",
    "# device_map = {\n",
    "#     \"model.embed_tokens\": 0,\n",
    "#     \"model.layers.0\": 0,\n",
    "#     \"model.layers.1\": 0,\n",
    "#     \"model.layers.2\": 0,\n",
    "#     \"model.layers.3\": 0,\n",
    "#     \"model.layers.4\": 0,\n",
    "#     \"model.layers.5\": 0,\n",
    "#     \"model.layers.6\": 0,\n",
    "#     \"model.layers.7\": 0,\n",
    "#     \"model.layers.8\": 0,\n",
    "#     \"model.layers.9\": 0,\n",
    "#     \"model.layers.10\": 0,\n",
    "#     \"model.layers.11\": 0,\n",
    "#     \"model.layers.12\": 0,\n",
    "#     \"model.layers.13\": 0,\n",
    "#     \"model.layers.14\": 0,\n",
    "#     \"model.layers.15\": 0,\n",
    "#     \"model.layers.16\": 0,\n",
    "#     \"model.layers.17\": 0,\n",
    "#     \"model.layers.18\": 1,\n",
    "#     \"model.layers.19\": 1,\n",
    "#     \"model.layers.20\": 1,\n",
    "#     \"model.layers.21\": 1,\n",
    "#     \"model.layers.22\": 1,\n",
    "#     \"model.layers.23\": 1,\n",
    "#     \"model.layers.24\": 1,\n",
    "#     \"model.layers.25\": 1,\n",
    "#     \"model.layers.26\": 1,\n",
    "#     \"model.layers.27\": 1,\n",
    "#     \"model.layers.28\": 1,\n",
    "#     \"model.layers.29\": 1,\n",
    "#     \"model.layers.30\": 1,\n",
    "#     \"model.layers.31\": 1,\n",
    "#     \"model.layers.32\": 1,\n",
    "#     \"model.layers.33\": 1,\n",
    "#     \"model.layers.34\": 1,\n",
    "#     \"model.layers.35\": 1,\n",
    "#     \"model.layers.36\": 1,\n",
    "#     \"model.layers.37\": 1,\n",
    "#     \"model.layers.38\": 1,\n",
    "#     \"model.layers.39\": 2,\n",
    "#     \"model.layers.40\": 2,\n",
    "#     \"model.layers.41\": 2,\n",
    "#     \"model.layers.42\": 2,\n",
    "#     \"model.layers.43\": 2,\n",
    "#     \"model.layers.44\": 2,\n",
    "#     \"model.layers.45\": 2,\n",
    "#     \"model.layers.46\": 2,\n",
    "#     \"model.layers.47\": 2,\n",
    "#     \"model.layers.48\": 2,\n",
    "#     \"model.layers.49\": 2,\n",
    "#     \"model.layers.50\": 2,\n",
    "#     \"model.layers.51\": 2,\n",
    "#     \"model.layers.52\": 2,\n",
    "#     \"model.layers.53\": 2,\n",
    "#     \"model.layers.54\": 2,\n",
    "#     \"model.layers.55\": 2,\n",
    "#     \"model.layers.56\": 2,\n",
    "#     \"model.layers.57\": 2,\n",
    "#     \"model.layers.58\": 2,\n",
    "#     \"model.layers.59\": 2,\n",
    "#     \"model.layers.60\": 3,\n",
    "#     \"model.layers.61\": 3,\n",
    "#     \"model.layers.62\": 3,\n",
    "#     \"model.layers.63\": 3,\n",
    "#     \"model.layers.64\": 3,\n",
    "#     \"model.layers.65\": 3,\n",
    "#     \"model.layers.66\": 3,\n",
    "#     \"model.layers.67\": 3,\n",
    "#     \"model.layers.68\": 3,\n",
    "#     \"model.layers.69\": 3,\n",
    "#     \"model.layers.70\": 3,\n",
    "#     \"model.layers.71\": 3,\n",
    "#     \"model.layers.72\": 3,\n",
    "#     \"model.layers.73\": 3,\n",
    "#     \"model.layers.74\": 3,\n",
    "#     \"model.layers.75\": 3,\n",
    "#     \"model.layers.76\": 3,\n",
    "#     \"model.layers.77\": 3,\n",
    "#     \"model.layers.78\": 3,\n",
    "#     \"model.layers.79\": 3,\n",
    "#     \"model.norm\": 3,\n",
    "#     \"lm_head\": 3,\n",
    "# }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import warnings\n",
    "import gc\n",
    "import torch\n",
    "import json\n",
    "import Levenshtein\n",
    "import shutil\n",
    "\n",
    "from datasets import (\n",
    "    load_dataset,\n",
    "    concatenate_datasets,\n",
    "    load_from_disk,\n",
    "    Features,\n",
    "    Sequence,\n",
    "    Value,\n",
    ")\n",
    "from transformers import AutoTokenizer, pipeline, AutoModelForCausalLM, LlamaTokenizer\n",
    "from langchain import HuggingFacePipeline, PromptTemplate, LLMChain\n",
    "from peft import PeftModel\n",
    "\n",
    "from datasets import logging as ds_logging\n",
    "from transformers import logging as trans_logging\n",
    "\n",
    "from qlora import train\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_logging.set_verbosity_error()\n",
    "ds_logging.disable_progress_bar()\n",
    "trans_logging.set_verbosity_error()\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Load datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_annotations_from_file(path: str, file: str):\n",
    "    features = Features(\n",
    "        {\n",
    "            \"PTC\": Sequence(feature=Value(dtype=\"string\", id=None), length=-1, id=None),\n",
    "            \"Evidence\": Sequence(\n",
    "                feature=Value(dtype=\"string\", id=None), length=-1, id=None\n",
    "            ),\n",
    "            \"Medium\": Sequence(\n",
    "                feature=Value(dtype=\"string\", id=None), length=-1, id=None\n",
    "            ),\n",
    "            \"Topic\": Sequence(\n",
    "                feature=Value(dtype=\"string\", id=None), length=-1, id=None\n",
    "            ),\n",
    "            \"Cue\": Sequence(feature=Value(dtype=\"string\", id=None), length=-1, id=None),\n",
    "            \"Addr\": Sequence(\n",
    "                feature=Value(dtype=\"string\", id=None), length=-1, id=None\n",
    "            ),\n",
    "            \"Message\": Sequence(\n",
    "                feature=Value(dtype=\"string\", id=None), length=-1, id=None\n",
    "            ),\n",
    "            \"Source\": Sequence(\n",
    "                feature=Value(dtype=\"string\", id=None), length=-1, id=None\n",
    "            ),\n",
    "        }\n",
    "    )\n",
    "    ds = load_dataset(\n",
    "        \"json\",\n",
    "        data_files=os.path.join(path, file),\n",
    "        field=\"Annotations\",\n",
    "        split=\"train\",\n",
    "        features=features,\n",
    "    )\n",
    "    ds = ds.add_column(\"FileName\", [file] * len(ds))\n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sentences_from_file(path: str, file: str):\n",
    "    ds = load_dataset(\n",
    "        \"json\", data_files=os.path.join(path, file), field=\"Sentences\", split=\"train\"\n",
    "    )\n",
    "    ds = ds.add_column(\"FileName\", [file] * len(ds))\n",
    "    ds = ds.add_column(\"Sentence\", [\" \".join(t) for t in ds[\"Tokens\"]])\n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_annotations_from_path(path: str):\n",
    "    dataset = None\n",
    "\n",
    "    for file in tqdm(sorted(os.listdir(path))):\n",
    "        if not dataset:\n",
    "            dataset = read_annotations_from_file(path, file)\n",
    "        else:\n",
    "            dataset = concatenate_datasets(\n",
    "                [dataset, read_annotations_from_file(path, file)]\n",
    "            )\n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sentences_from_path(path: str):\n",
    "    dataset = None\n",
    "\n",
    "    for file in tqdm(sorted(os.listdir(path))):\n",
    "        if not dataset:\n",
    "            dataset = read_sentences_from_file(path, file)\n",
    "        else:\n",
    "            dataset = concatenate_datasets(\n",
    "                [dataset, read_sentences_from_file(path, file)]\n",
    "            )\n",
    "\n",
    "    dataset = dataset.add_column(\"id\", range(len(dataset)))\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sentences_dataset(ds_name: str):\n",
    "    path_to_dataset = \"./transformed_datasets/\" + ds_name + \"/sentences\"\n",
    "\n",
    "    if os.path.isdir(path_to_dataset):\n",
    "        result = load_from_disk(path_to_dataset)\n",
    "    else:\n",
    "        result = read_sentences_from_path(\n",
    "            \"./SpkAtt-2023/data/\"\n",
    "            + ds_name\n",
    "            + \"/task1\"\n",
    "            + (\"_test/\" if ds_name == \"eval\" else \"/\")\n",
    "        )\n",
    "        os.makedirs(path_to_dataset, exist_ok=True)\n",
    "        result.save_to_disk(path_to_dataset)\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_annotations_dataset(ds_name: str):\n",
    "    path_to_dataset = \"./transformed_datasets/\" + ds_name + \"/annotations\"\n",
    "\n",
    "    if os.path.isdir(path_to_dataset):\n",
    "        return load_from_disk(path_to_dataset)\n",
    "\n",
    "    result = read_annotations_from_path(\n",
    "        \"./SpkAtt-2023/data/\"\n",
    "        + ds_name\n",
    "        + \"/task1\"\n",
    "        + (\"_test/\" if ds_name == \"eval\" else \"/\")\n",
    "    )\n",
    "    os.makedirs(path_to_dataset, exist_ok=True)\n",
    "    result.save_to_disk(path_to_dataset)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences_dataset = read_sentences_dataset(\"train\")\n",
    "val_sentences_dataset = read_sentences_dataset(\"dev\")\n",
    "test_sentences_dataset = read_sentences_dataset(\"eval\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_annotations_dataset = read_annotations_dataset(\"train\")\n",
    "val_annotations_dataset = read_annotations_dataset(\"dev\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Format datasets for usage in langchain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_from_label(train_sentences_dataset, row, annotations):\n",
    "    tokens = []\n",
    "    for anno in annotations:\n",
    "        if int(anno.split(\":\")[0]) == row[\"SentenceId\"]:\n",
    "            tokens.append(row[\"Tokens\"][int(anno.split(\":\")[1])])\n",
    "        else:\n",
    "            temp_row = train_sentences_dataset.filter(\n",
    "                lambda r: r[\"FileName\"] == row[\"FileName\"]\n",
    "                and r[\"SentenceId\"] == int(anno.split(\":\")[0])\n",
    "            )[0]\n",
    "            tokens.append(temp_row[\"Tokens\"][int(anno.split(\":\")[1])])\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_complete_dataset(sentences_dataset, annotations_dataset, dataset_name):\n",
    "    path_to_dataset = \"./transformed_datasets/\" + dataset_name + \"/complete\"\n",
    "    if os.path.isdir(path_to_dataset):\n",
    "        return load_from_disk(path_to_dataset)\n",
    "\n",
    "    ptc, ptc_temp, ptc_mapped, ptc_mapped_temp = [], [], [], []\n",
    "    evidence, evidence_temp, evidence_mapped, evidence_mapped_temp = [], [], [], []\n",
    "    medium, medium_temp, medium_mapped, medium_mapped_temp = [], [], [], []\n",
    "    topic, topic_temp, topic_mapped, topic_mapped_temp = [], [], [], []\n",
    "    cue, cue_temp, cue_mapped, cue_mapped_temp = [], [], [], []\n",
    "    addr, addr_temp, addr_mapped, addr_mapped_temp = [], [], [], []\n",
    "    message, message_temp, message_mapped, message_mapped_temp = [], [], [], []\n",
    "    source, source_temp, source_mapped, source_mapped_temp = [], [], [], []\n",
    "    (\n",
    "        sentence_extended,\n",
    "        tokens_extended,\n",
    "        sentence_extended_ids,\n",
    "    ) = (\n",
    "        [],\n",
    "        [],\n",
    "        [],\n",
    "    )\n",
    "\n",
    "    index_in_anno_ds = 0\n",
    "\n",
    "    for i, row in tqdm(enumerate(sentences_dataset)):\n",
    "        context = row[\"Sentence\"]\n",
    "        tokens = row[\"Tokens\"]\n",
    "        ids = [row[\"SentenceId\"]] * len(row[\"Tokens\"])\n",
    "        if (\n",
    "            i + 1 < len(sentences_dataset)\n",
    "            and sentences_dataset[i + 1][\"FileName\"] == row[\"FileName\"]\n",
    "        ):\n",
    "            context = context + \" \" + sentences_dataset[i + 1][\"Sentence\"]\n",
    "            tokens.extend(sentences_dataset[i + 1][\"Tokens\"])\n",
    "            ids.extend(\n",
    "                [sentences_dataset[i + 1][\"SentenceId\"]]\n",
    "                * len(sentences_dataset[i + 1][\"Tokens\"])\n",
    "            )\n",
    "        if (\n",
    "            i + 2 < len(sentences_dataset)\n",
    "            and sentences_dataset[i + 2][\"FileName\"] == row[\"FileName\"]\n",
    "        ):\n",
    "            context = context + \" \" + sentences_dataset[i + 2][\"Sentence\"]\n",
    "            tokens.extend(sentences_dataset[i + 2][\"Tokens\"])\n",
    "            ids.extend(\n",
    "                [sentences_dataset[i + 2][\"SentenceId\"]]\n",
    "                * len(sentences_dataset[i + 2][\"Tokens\"])\n",
    "            )\n",
    "        sentence_extended.append(context)\n",
    "        tokens_extended.append(tokens)\n",
    "        sentence_extended_ids.append(ids)\n",
    "\n",
    "        if annotations_dataset is not None:\n",
    "            id_of_next_sentence_with_annotation = (\n",
    "                int(annotations_dataset[index_in_anno_ds][\"Cue\"][0].split(\":\")[0])\n",
    "                if index_in_anno_ds != len(annotations_dataset)\n",
    "                else -1\n",
    "            )\n",
    "\n",
    "            if row[\"SentenceId\"] != id_of_next_sentence_with_annotation:\n",
    "                ptc.append([])\n",
    "                ptc_mapped.append([])\n",
    "                evidence.append([])\n",
    "                evidence_mapped.append([])\n",
    "                medium.append([])\n",
    "                medium_mapped.append([])\n",
    "                topic.append([])\n",
    "                topic_mapped.append([])\n",
    "                cue.append([])\n",
    "                cue_mapped.append([])\n",
    "                addr.append([])\n",
    "                addr_mapped.append([])\n",
    "                message.append([])\n",
    "                message_mapped.append([])\n",
    "                source.append([])\n",
    "                source_mapped.append([])\n",
    "                continue\n",
    "\n",
    "            while row[\"SentenceId\"] == id_of_next_sentence_with_annotation:\n",
    "                ptc_temp.append(annotations_dataset[index_in_anno_ds][\"PTC\"])\n",
    "                evidence_temp.append(annotations_dataset[index_in_anno_ds][\"Evidence\"])\n",
    "                medium_temp.append(annotations_dataset[index_in_anno_ds][\"Medium\"])\n",
    "                topic_temp.append(annotations_dataset[index_in_anno_ds][\"Topic\"])\n",
    "                cue_temp.append(annotations_dataset[index_in_anno_ds][\"Cue\"])\n",
    "                addr_temp.append(annotations_dataset[index_in_anno_ds][\"Addr\"])\n",
    "                message_temp.append(annotations_dataset[index_in_anno_ds][\"Message\"])\n",
    "                source_temp.append(annotations_dataset[index_in_anno_ds][\"Source\"])\n",
    "\n",
    "                ptc_mapped_temp.append(\n",
    "                    get_text_from_label(sentences_dataset, row, ptc_temp[-1])\n",
    "                )\n",
    "                evidence_mapped_temp.append(\n",
    "                    get_text_from_label(sentences_dataset, row, evidence_temp[-1])\n",
    "                )\n",
    "                medium_mapped_temp.append(\n",
    "                    get_text_from_label(sentences_dataset, row, medium_temp[-1])\n",
    "                )\n",
    "                topic_mapped_temp.append(\n",
    "                    get_text_from_label(sentences_dataset, row, topic_temp[-1])\n",
    "                )\n",
    "                cue_mapped_temp.append(\n",
    "                    get_text_from_label(sentences_dataset, row, cue_temp[-1])\n",
    "                )\n",
    "                addr_mapped_temp.append(\n",
    "                    get_text_from_label(sentences_dataset, row, addr_temp[-1])\n",
    "                )\n",
    "                message_mapped_temp.append(\n",
    "                    get_text_from_label(sentences_dataset, row, message_temp[-1])\n",
    "                )\n",
    "                source_mapped_temp.append(\n",
    "                    get_text_from_label(sentences_dataset, row, source_temp[-1])\n",
    "                )\n",
    "\n",
    "                index_in_anno_ds += 1\n",
    "                if index_in_anno_ds == len(annotations_dataset):\n",
    "                    break\n",
    "                id_of_next_sentence_with_annotation = int(\n",
    "                    annotations_dataset[index_in_anno_ds][\"Cue\"][0].split(\":\")[0]\n",
    "                )\n",
    "\n",
    "            ptc.append(ptc_temp)\n",
    "            ptc_mapped.append(ptc_mapped_temp)\n",
    "            evidence.append(evidence_temp)\n",
    "            evidence_mapped.append(evidence_mapped_temp)\n",
    "            medium.append(medium_temp)\n",
    "            medium_mapped.append(medium_mapped_temp)\n",
    "            topic.append(topic_temp)\n",
    "            topic_mapped.append(topic_mapped_temp)\n",
    "            cue.append(cue_temp)\n",
    "            cue_mapped.append(cue_mapped_temp)\n",
    "            addr.append(addr_temp)\n",
    "            addr_mapped.append(addr_mapped_temp)\n",
    "            message.append(message_temp)\n",
    "            message_mapped.append(message_mapped_temp)\n",
    "            source.append(source_temp)\n",
    "            source_mapped.append(source_mapped_temp)\n",
    "\n",
    "            ptc_temp, ptc_mapped_temp = [], []\n",
    "            evidence_temp, evidence_mapped_temp = [], []\n",
    "            medium_temp, medium_mapped_temp = [], []\n",
    "            topic_temp, topic_mapped_temp = [], []\n",
    "            cue_temp, cue_mapped_temp = [], []\n",
    "            addr_temp, addr_mapped_temp = [], []\n",
    "            message_temp, message_mapped_temp = [], []\n",
    "            source_temp, source_mapped_temp = [], []\n",
    "\n",
    "    res = sentences_dataset.add_column(\"sentence_extended\", sentence_extended)\n",
    "    res = res.add_column(\"tokens_extended\", tokens_extended)\n",
    "    res = res.add_column(\"sentence_extended_ids\", sentence_extended_ids)\n",
    "\n",
    "    if annotations_dataset is not None:\n",
    "        res = res.add_column(\"ptc\", ptc)\n",
    "        res = res.add_column(\"ptc_mapped\", ptc_mapped)\n",
    "        res = res.add_column(\"evidence\", evidence)\n",
    "        res = res.add_column(\"evidence_mapped\", evidence_mapped)\n",
    "        res = res.add_column(\"medium\", medium)\n",
    "        res = res.add_column(\"medium_mapped\", medium_mapped)\n",
    "        res = res.add_column(\"topic\", topic)\n",
    "        res = res.add_column(\"topic_mapped\", topic_mapped)\n",
    "        res = res.add_column(\"cue\", cue)\n",
    "        res = res.add_column(\"cue_mapped\", cue_mapped)\n",
    "        res = res.add_column(\"addr\", addr)\n",
    "        res = res.add_column(\"addr_mapped\", addr_mapped)\n",
    "        res = res.add_column(\"message\", message)\n",
    "        res = res.add_column(\"message_mapped\", message_mapped)\n",
    "        res = res.add_column(\"source\", source)\n",
    "        res = res.add_column(\"source_mapped\", source_mapped)\n",
    "\n",
    "    os.makedirs(path_to_dataset, exist_ok=True)\n",
    "    res.save_to_disk(path_to_dataset)\n",
    "\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = build_complete_dataset(\n",
    "    train_sentences_dataset, train_annotations_dataset, \"train\"\n",
    ")\n",
    "val_ds = build_complete_dataset(val_sentences_dataset, val_annotations_dataset, \"dev\") # USE FOR INFERENCE\n",
    "test_ds = build_complete_dataset(test_sentences_dataset, None, \"eval\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = test_sentences_dataset.rename_column(\"Sentence\", \"Satz\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Dataset Showcase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Tokens': ['-',\n",
       "  'Letzter',\n",
       "  'Redner',\n",
       "  'in',\n",
       "  'der',\n",
       "  'Debatte',\n",
       "  ':',\n",
       "  'Bernd',\n",
       "  'Westphal',\n",
       "  'für',\n",
       "  'die',\n",
       "  'SPD-Fraktion',\n",
       "  '.'],\n",
       " 'SentenceId': 52,\n",
       " 'FileName': '19002_Zusatzpunkt_3_CDUCSU_Jung_ID19209800_21.11.2017.json',\n",
       " 'Sentence': '- Letzter Redner in der Debatte : Bernd Westphal für die SPD-Fraktion .',\n",
       " 'id': 52,\n",
       " 'sentence_extended': '- Letzter Redner in der Debatte : Bernd Westphal für die SPD-Fraktion .',\n",
       " 'tokens_extended': ['-',\n",
       "  'Letzter',\n",
       "  'Redner',\n",
       "  'in',\n",
       "  'der',\n",
       "  'Debatte',\n",
       "  ':',\n",
       "  'Bernd',\n",
       "  'Westphal',\n",
       "  'für',\n",
       "  'die',\n",
       "  'SPD-Fraktion',\n",
       "  '.'],\n",
       " 'sentence_extended_ids': [52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52],\n",
       " 'ptc': [[]],\n",
       " 'ptc_mapped': [[]],\n",
       " 'evidence': [[]],\n",
       " 'evidence_mapped': [[]],\n",
       " 'medium': [[]],\n",
       " 'medium_mapped': [[]],\n",
       " 'topic': [[]],\n",
       " 'topic_mapped': [[]],\n",
       " 'cue': [['52:5']],\n",
       " 'cue_mapped': [['Debatte']],\n",
       " 'addr': [[]],\n",
       " 'addr_mapped': [[]],\n",
       " 'message': [[]],\n",
       " 'message_mapped': [[]],\n",
       " 'source': [[]],\n",
       " 'source_mapped': [[]]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[52]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Tokens': ['Dazu',\n",
       "  'muss',\n",
       "  'man',\n",
       "  'nur',\n",
       "  'mit',\n",
       "  'den',\n",
       "  'Landwirten',\n",
       "  'sprechen',\n",
       "  ',',\n",
       "  'die',\n",
       "  'sagen',\n",
       "  ':',\n",
       "  'Ja',\n",
       "  ',',\n",
       "  'auch',\n",
       "  'früher',\n",
       "  'gab',\n",
       "  'es',\n",
       "  'extreme',\n",
       "  'Ereignisse',\n",
       "  ',',\n",
       "  'auch',\n",
       "  'früher',\n",
       "  'gab',\n",
       "  'es',\n",
       "  'Naturkatastrophen',\n",
       "  ',',\n",
       "  'aber',\n",
       "  'in',\n",
       "  'einem',\n",
       "  'Jahr',\n",
       "  'den',\n",
       "  'Hagel',\n",
       "  ',',\n",
       "  'im',\n",
       "  'anderen',\n",
       "  'Jahr',\n",
       "  'eine',\n",
       "  'Dürre',\n",
       "  'und',\n",
       "  'im',\n",
       "  'dritten',\n",
       "  'Jahr',\n",
       "  ',',\n",
       "  'wie',\n",
       "  'in',\n",
       "  'diesem',\n",
       "  'Jahr',\n",
       "  ',',\n",
       "  'die',\n",
       "  'Frostschäden',\n",
       "  ',',\n",
       "  'unter',\n",
       "  'denen',\n",
       "  'die',\n",
       "  'Obstbauern',\n",
       "  'zu',\n",
       "  'leiden',\n",
       "  'hatten',\n",
       "  ',',\n",
       "  'diese',\n",
       "  'Häufung',\n",
       "  'hatten',\n",
       "  'wir',\n",
       "  'früher',\n",
       "  'so',\n",
       "  'nicht',\n",
       "  ',',\n",
       "  'also',\n",
       "  'tut',\n",
       "  'etwas',\n",
       "  'gegen',\n",
       "  'den',\n",
       "  'Klimawandel',\n",
       "  '.'],\n",
       " 'SentenceId': 15,\n",
       " 'FileName': '19002_Zusatzpunkt_3_CDUCSU_Jung_ID19209800_21.11.2017.json',\n",
       " 'Sentence': 'Dazu muss man nur mit den Landwirten sprechen , die sagen : Ja , auch früher gab es extreme Ereignisse , auch früher gab es Naturkatastrophen , aber in einem Jahr den Hagel , im anderen Jahr eine Dürre und im dritten Jahr , wie in diesem Jahr , die Frostschäden , unter denen die Obstbauern zu leiden hatten , diese Häufung hatten wir früher so nicht , also tut etwas gegen den Klimawandel .',\n",
       " 'id': 15,\n",
       " 'sentence_extended': 'Dazu muss man nur mit den Landwirten sprechen , die sagen : Ja , auch früher gab es extreme Ereignisse , auch früher gab es Naturkatastrophen , aber in einem Jahr den Hagel , im anderen Jahr eine Dürre und im dritten Jahr , wie in diesem Jahr , die Frostschäden , unter denen die Obstbauern zu leiden hatten , diese Häufung hatten wir früher so nicht , also tut etwas gegen den Klimawandel . Es geht um unsere wirtschaftlichen Existenzen . Damit ist die Aufgabe , vor der wir stehen , beschrieben .',\n",
       " 'tokens_extended': ['Dazu',\n",
       "  'muss',\n",
       "  'man',\n",
       "  'nur',\n",
       "  'mit',\n",
       "  'den',\n",
       "  'Landwirten',\n",
       "  'sprechen',\n",
       "  ',',\n",
       "  'die',\n",
       "  'sagen',\n",
       "  ':',\n",
       "  'Ja',\n",
       "  ',',\n",
       "  'auch',\n",
       "  'früher',\n",
       "  'gab',\n",
       "  'es',\n",
       "  'extreme',\n",
       "  'Ereignisse',\n",
       "  ',',\n",
       "  'auch',\n",
       "  'früher',\n",
       "  'gab',\n",
       "  'es',\n",
       "  'Naturkatastrophen',\n",
       "  ',',\n",
       "  'aber',\n",
       "  'in',\n",
       "  'einem',\n",
       "  'Jahr',\n",
       "  'den',\n",
       "  'Hagel',\n",
       "  ',',\n",
       "  'im',\n",
       "  'anderen',\n",
       "  'Jahr',\n",
       "  'eine',\n",
       "  'Dürre',\n",
       "  'und',\n",
       "  'im',\n",
       "  'dritten',\n",
       "  'Jahr',\n",
       "  ',',\n",
       "  'wie',\n",
       "  'in',\n",
       "  'diesem',\n",
       "  'Jahr',\n",
       "  ',',\n",
       "  'die',\n",
       "  'Frostschäden',\n",
       "  ',',\n",
       "  'unter',\n",
       "  'denen',\n",
       "  'die',\n",
       "  'Obstbauern',\n",
       "  'zu',\n",
       "  'leiden',\n",
       "  'hatten',\n",
       "  ',',\n",
       "  'diese',\n",
       "  'Häufung',\n",
       "  'hatten',\n",
       "  'wir',\n",
       "  'früher',\n",
       "  'so',\n",
       "  'nicht',\n",
       "  ',',\n",
       "  'also',\n",
       "  'tut',\n",
       "  'etwas',\n",
       "  'gegen',\n",
       "  'den',\n",
       "  'Klimawandel',\n",
       "  '.',\n",
       "  'Es',\n",
       "  'geht',\n",
       "  'um',\n",
       "  'unsere',\n",
       "  'wirtschaftlichen',\n",
       "  'Existenzen',\n",
       "  '.',\n",
       "  'Damit',\n",
       "  'ist',\n",
       "  'die',\n",
       "  'Aufgabe',\n",
       "  ',',\n",
       "  'vor',\n",
       "  'der',\n",
       "  'wir',\n",
       "  'stehen',\n",
       "  ',',\n",
       "  'beschrieben',\n",
       "  '.'],\n",
       " 'sentence_extended_ids': [15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  16,\n",
       "  16,\n",
       "  16,\n",
       "  16,\n",
       "  16,\n",
       "  16,\n",
       "  16,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17,\n",
       "  17],\n",
       " 'ptc': [[], []],\n",
       " 'ptc_mapped': [[], []],\n",
       " 'evidence': [[], []],\n",
       " 'evidence_mapped': [[], []],\n",
       " 'medium': [[], []],\n",
       " 'medium_mapped': [[], []],\n",
       " 'topic': [[], []],\n",
       " 'topic_mapped': [[], []],\n",
       " 'cue': [['15:7'], ['15:10']],\n",
       " 'cue_mapped': [['sprechen'], ['sagen']],\n",
       " 'addr': [['15:4', '15:5', '15:6'], []],\n",
       " 'addr_mapped': [['mit', 'den', 'Landwirten'], []],\n",
       " 'message': [[],\n",
       "  ['15:12',\n",
       "   '15:13',\n",
       "   '15:14',\n",
       "   '15:15',\n",
       "   '15:16',\n",
       "   '15:17',\n",
       "   '15:18',\n",
       "   '15:19',\n",
       "   '15:20',\n",
       "   '15:21',\n",
       "   '15:22',\n",
       "   '15:23',\n",
       "   '15:24',\n",
       "   '15:25',\n",
       "   '15:26',\n",
       "   '15:27',\n",
       "   '15:28',\n",
       "   '15:29',\n",
       "   '15:30',\n",
       "   '15:31',\n",
       "   '15:32',\n",
       "   '15:33',\n",
       "   '15:34',\n",
       "   '15:35',\n",
       "   '15:36',\n",
       "   '15:37',\n",
       "   '15:38',\n",
       "   '15:39',\n",
       "   '15:40',\n",
       "   '15:41',\n",
       "   '15:42',\n",
       "   '15:43',\n",
       "   '15:44',\n",
       "   '15:45',\n",
       "   '15:46',\n",
       "   '15:47',\n",
       "   '15:48',\n",
       "   '15:49',\n",
       "   '15:50',\n",
       "   '15:51',\n",
       "   '15:52',\n",
       "   '15:53',\n",
       "   '15:54',\n",
       "   '15:55',\n",
       "   '15:56',\n",
       "   '15:57',\n",
       "   '15:58',\n",
       "   '15:59',\n",
       "   '15:60',\n",
       "   '15:61',\n",
       "   '15:62',\n",
       "   '15:63',\n",
       "   '15:64',\n",
       "   '15:65',\n",
       "   '15:66',\n",
       "   '15:67',\n",
       "   '15:68',\n",
       "   '15:69',\n",
       "   '15:70',\n",
       "   '15:71',\n",
       "   '15:72',\n",
       "   '15:73',\n",
       "   '15:74',\n",
       "   '16:0',\n",
       "   '16:1',\n",
       "   '16:2',\n",
       "   '16:3',\n",
       "   '16:4',\n",
       "   '16:5']],\n",
       " 'message_mapped': [[],\n",
       "  ['Ja',\n",
       "   ',',\n",
       "   'auch',\n",
       "   'früher',\n",
       "   'gab',\n",
       "   'es',\n",
       "   'extreme',\n",
       "   'Ereignisse',\n",
       "   ',',\n",
       "   'auch',\n",
       "   'früher',\n",
       "   'gab',\n",
       "   'es',\n",
       "   'Naturkatastrophen',\n",
       "   ',',\n",
       "   'aber',\n",
       "   'in',\n",
       "   'einem',\n",
       "   'Jahr',\n",
       "   'den',\n",
       "   'Hagel',\n",
       "   ',',\n",
       "   'im',\n",
       "   'anderen',\n",
       "   'Jahr',\n",
       "   'eine',\n",
       "   'Dürre',\n",
       "   'und',\n",
       "   'im',\n",
       "   'dritten',\n",
       "   'Jahr',\n",
       "   ',',\n",
       "   'wie',\n",
       "   'in',\n",
       "   'diesem',\n",
       "   'Jahr',\n",
       "   ',',\n",
       "   'die',\n",
       "   'Frostschäden',\n",
       "   ',',\n",
       "   'unter',\n",
       "   'denen',\n",
       "   'die',\n",
       "   'Obstbauern',\n",
       "   'zu',\n",
       "   'leiden',\n",
       "   'hatten',\n",
       "   ',',\n",
       "   'diese',\n",
       "   'Häufung',\n",
       "   'hatten',\n",
       "   'wir',\n",
       "   'früher',\n",
       "   'so',\n",
       "   'nicht',\n",
       "   ',',\n",
       "   'also',\n",
       "   'tut',\n",
       "   'etwas',\n",
       "   'gegen',\n",
       "   'den',\n",
       "   'Klimawandel',\n",
       "   '.',\n",
       "   'Es',\n",
       "   'geht',\n",
       "   'um',\n",
       "   'unsere',\n",
       "   'wirtschaftlichen',\n",
       "   'Existenzen']],\n",
       " 'source': [['15:2'], ['15:9']],\n",
       " 'source_mapped': [['man'], ['die']]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[15]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Build lmsys format json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_cues_to_string(mapped):\n",
    "    if mapped == []:\n",
    "        return \"#UNK#\"\n",
    "    return \", \".join([\"[\" + \", \".join(val) + \"]\" for val in mapped])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_roles_to_string(mapped):\n",
    "    if mapped == []:\n",
    "        return \"#UNK#\"\n",
    "    return \", \".join(mapped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmsys_data_path = \"./lmsys.json\"\n",
    "\n",
    "\n",
    "def build_lmsys_format(train_ds, val_ds):\n",
    "    result = []\n",
    "\n",
    "    index = 0\n",
    "    for row in train_ds: # CHANGED \n",
    "        if len(row[\"cue_mapped\"]) == 0:\n",
    "            element = {\"id\": \"identity_\" + str(index)}\n",
    "            index += 1\n",
    "            conversations = [\n",
    "                {\n",
    "                    \"from\": \"human\",\n",
    "                    \"value\": 'A cue is the lexical items in a sentence that indicate that speech, writing, or thought is being reproduced.\\nI want you to extract all cues in the text below.\\nIf you find multiple words for one cue, you output them separated by commas.\\nIf no cue can be found in the given text, you output the string #UNK# as cue.\\nNow extract all cues from the following sentence.\\nUse the prefix \"Cues: \".\\nSentence: '\n",
    "                    + row[\"Sentence\"],\n",
    "                },\n",
    "                {\n",
    "                    \"from\": \"gpt\",\n",
    "                    \"value\": \"Cues: \" + map_cues_to_string(row[\"cue_mapped\"]),\n",
    "                },\n",
    "            ]\n",
    "            element[\"conversations\"] = conversations\n",
    "            result.append(element)\n",
    "            continue\n",
    "        for i, cue in enumerate(row[\"cue_mapped\"]):\n",
    "            element = {\"id\": \"identity_\" + str(index)}\n",
    "            index += 1\n",
    "            conversations = [\n",
    "                {\n",
    "                    \"from\": \"human\",\n",
    "                    \"value\": 'A cue is the lexical items in a sentence that indicate that speech, writing, or thought is being reproduced.\\nI want you to extract all cues in the text below.\\nIf you find multiple words for one cue, you output them separated by commas.\\nIf no cue can be found in the given text, you output the string #UNK# as cue.\\nNow extract all cues from the following sentence.\\nUse the prefix \"Cues: \".\\nSentence: '\n",
    "                    + row[\"Sentence\"],\n",
    "                },\n",
    "                {\n",
    "                    \"from\": \"gpt\",\n",
    "                    \"value\": \"Cues: \" + map_cues_to_string(row[\"cue_mapped\"]),\n",
    "                },\n",
    "                {\n",
    "                    \"from\": \"human\",\n",
    "                    \"value\": \"Now I give you again the sentence only in addition with the two following sentences, because the roles can be partially contained in the following sentences.\\nText: \"\n",
    "                    + row[\"sentence_extended\"]\n",
    "                    + \"\\n\\nNow find all roles in the sentence associated with the cue '\"\n",
    "                    + \", \".join(cue)\n",
    "                    + \"' you found in the beginning sentence.\",\n",
    "                },\n",
    "                {\n",
    "                    \"from\": \"gpt\",\n",
    "                    \"value\": \"cue: \"\n",
    "                    + \", \".join(cue)\n",
    "                    + \"\\nptc: \"\n",
    "                    + map_roles_to_string(row[\"ptc_mapped\"][i])\n",
    "                    + \"\\nevidence: \"\n",
    "                    + map_roles_to_string(row[\"evidence_mapped\"][i])\n",
    "                    + \"\\nmedium: \"\n",
    "                    + map_roles_to_string(row[\"medium_mapped\"][i])\n",
    "                    + \"\\ntopic: \"\n",
    "                    + map_roles_to_string(row[\"topic_mapped\"][i])\n",
    "                    + \"\\naddr: \"\n",
    "                    + map_roles_to_string(row[\"addr_mapped\"][i])\n",
    "                    + \"\\nmessage: \"\n",
    "                    + map_roles_to_string(row[\"message_mapped\"][i])\n",
    "                    + \"\\nsource: \"\n",
    "                    + map_roles_to_string(row[\"source_mapped\"][i]),\n",
    "                },\n",
    "            ]\n",
    "            element[\"conversations\"] = conversations\n",
    "            result.append(element)\n",
    "\n",
    "    with open(lmsys_data_path, \"w\", encoding=\"utf8\") as outfile:\n",
    "        json.dump(result, outfile, indent=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_lmsys_format(train_ds, val_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # QLoRA Fine-Tuning\n",
    "\n",
    " ## Parse data into required format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_cues_file = \"./transformed_datasets/prompts_training/parsed_data_cues.jsonl\"\n",
    "parsed_roles_file = \"./transformed_datasets/prompts_training/parsed_data_roles.jsonl\"\n",
    "os.makedirs(os.path.dirname(parsed_cues_file), exist_ok=True)\n",
    "os.makedirs(os.path.dirname(parsed_roles_file), exist_ok=True)\n",
    "\n",
    "# token to signal the end of the assistant's response\n",
    "separator = \"</s>\"\n",
    "\n",
    "# reload parsed data\n",
    "with open(lmsys_data_path) as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# save parsed prompts separately\n",
    "all_prompts_cues = []\n",
    "all_prompts_roles = []\n",
    "for conversation in data:\n",
    "    # keep track of the complete conversation in order to generate the input of the prompts\n",
    "    complete_prompt = \"\"\n",
    "\n",
    "    for i, turn in enumerate(conversation[\"conversations\"]):\n",
    "        if turn[\"from\"] == \"human\":\n",
    "            complete_prompt += \"User: \"\n",
    "            complete_prompt += turn[\"value\"]\n",
    "        elif turn[\"from\"] == \"gpt\":\n",
    "            complete_prompt += \"Assistant: \"\n",
    "\n",
    "            # idea\n",
    "            # turn 0: user prompt for cues\n",
    "            # turn 1: assistant response with cues\n",
    "            #   --> create sample with the conversation up to this point as input and the cues as output\n",
    "            # turn 2: user prompt for roles for one specific cue\n",
    "            # turn 3: assistant response with roles\n",
    "            #   --> create sample with the conversation up to this point as input and the roles as output\n",
    "            # there should be no further turns because we split all conversations with multiple cues into separate conversations\n",
    "\n",
    "            sample = json.dumps(\n",
    "                {\"input\": complete_prompt, \"output\": turn[\"value\"] + separator}\n",
    "            )\n",
    "\n",
    "            if i == 1 and sample not in all_prompts_cues:\n",
    "                # turn 1: assistant response with cues\n",
    "                all_prompts_cues.append(sample)\n",
    "            elif i == 3 and sample not in all_prompts_cues:\n",
    "                # turn 3: assistant response with roles\n",
    "                all_prompts_roles.append(sample)\n",
    "            elif i != 1 and i != 3:\n",
    "                print(\n",
    "                    \"ERROR: each conversation should maximally contain 4 turns\"\n",
    "                    \" and only turn 1 and 3 should be responses by the assistant\"\n",
    "                )\n",
    "\n",
    "            complete_prompt += turn[\"value\"] + separator\n",
    "        complete_prompt += \"\\n\"\n",
    "\n",
    "# write parsed prompts to files\n",
    "with open(parsed_cues_file, \"w\") as f:\n",
    "    f.write(\"\\n\".join(all_prompts_cues))\n",
    "\n",
    "with open(parsed_roles_file, \"w\") as f:\n",
    "    f.write(\"\\n\".join(all_prompts_roles))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 8545\n",
      "\n",
      "First 5 samples:\n",
      "=== in: ===\n",
      "User: A cue is the lexical items in a sentence that indicate that speech, writing, or thought is being reproduced.\n",
      "I want you to extract all cues in the text below.\n",
      "If you find multiple words for one cue, you output them separated by commas.\n",
      "If no cue can be found in the given text, you output the string #UNK# as cue.\n",
      "Now extract all cues from the following sentence.\n",
      "Use the prefix \"Cues: \".\n",
      "Sentence: Frau Präsidentin !\n",
      "Assistant: \n",
      "\n",
      "=== out: ===\n",
      "Cues: #UNK#</s>\n",
      "\n",
      "\n",
      "=== in: ===\n",
      "User: A cue is the lexical items in a sentence that indicate that speech, writing, or thought is being reproduced.\n",
      "I want you to extract all cues in the text below.\n",
      "If you find multiple words for one cue, you output them separated by commas.\n",
      "If no cue can be found in the given text, you output the string #UNK# as cue.\n",
      "Now extract all cues from the following sentence.\n",
      "Use the prefix \"Cues: \".\n",
      "Sentence: Liebe Kolleginnen und Kollegen !\n",
      "Assistant: \n",
      "\n",
      "=== out: ===\n",
      "Cues: #UNK#</s>\n",
      "\n",
      "\n",
      "=== in: ===\n",
      "User: A cue is the lexical items in a sentence that indicate that speech, writing, or thought is being reproduced.\n",
      "I want you to extract all cues in the text below.\n",
      "If you find multiple words for one cue, you output them separated by commas.\n",
      "If no cue can be found in the given text, you output the string #UNK# as cue.\n",
      "Now extract all cues from the following sentence.\n",
      "Use the prefix \"Cues: \".\n",
      "Sentence: Bundeskanzlerin Angela Merkel hat auf der Klimakonferenz in Bonn gesprochen .\n",
      "Assistant: \n",
      "\n",
      "=== out: ===\n",
      "Cues: [gesprochen]</s>\n",
      "\n",
      "\n",
      "=== in: ===\n",
      "User: A cue is the lexical items in a sentence that indicate that speech, writing, or thought is being reproduced.\n",
      "I want you to extract all cues in the text below.\n",
      "If you find multiple words for one cue, you output them separated by commas.\n",
      "If no cue can be found in the given text, you output the string #UNK# as cue.\n",
      "Now extract all cues from the following sentence.\n",
      "Use the prefix \"Cues: \".\n",
      "Sentence: Sie hat dort den Klimawandel als eine zentrale Herausforderung für die Menschheit bezeichnet .\n",
      "Assistant: \n",
      "\n",
      "=== out: ===\n",
      "Cues: [bezeichnet]</s>\n",
      "\n",
      "\n",
      "=== in: ===\n",
      "User: A cue is the lexical items in a sentence that indicate that speech, writing, or thought is being reproduced.\n",
      "I want you to extract all cues in the text below.\n",
      "If you find multiple words for one cue, you output them separated by commas.\n",
      "If no cue can be found in the given text, you output the string #UNK# as cue.\n",
      "Now extract all cues from the following sentence.\n",
      "Use the prefix \"Cues: \".\n",
      "Sentence: Sie hat von einer Schicksalsfrage gesprochen .\n",
      "Assistant: \n",
      "\n",
      "=== out: ===\n",
      "Cues: [gesprochen]</s>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check that the file with the cue prompts was written correctly\n",
    "with open(parsed_cues_file) as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "print(f\"Number of samples: {len(lines)}\\n\")\n",
    "\n",
    "print(\"First 5 samples:\")\n",
    "for l in lines[:5]:\n",
    "    print(\"=== in: ===\\n\" + json.loads(l)[\"input\"] + \"\\n\")\n",
    "    print(\"=== out: ===\\n\" + json.loads(l)[\"output\"] + \"\\n\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 5399\n",
      "\n",
      "First 5 samples:\n",
      "=== in: ===\n",
      "User: A cue is the lexical items in a sentence that indicate that speech, writing, or thought is being reproduced.\n",
      "I want you to extract all cues in the text below.\n",
      "If you find multiple words for one cue, you output them separated by commas.\n",
      "If no cue can be found in the given text, you output the string #UNK# as cue.\n",
      "Now extract all cues from the following sentence.\n",
      "Use the prefix \"Cues: \".\n",
      "Sentence: Bundeskanzlerin Angela Merkel hat auf der Klimakonferenz in Bonn gesprochen .\n",
      "Assistant: Cues: [gesprochen]</s>\n",
      "User: Now I give you again the sentence only in addition with the two following sentences, because the roles can be partially contained in the following sentences.\n",
      "Text: Bundeskanzlerin Angela Merkel hat auf der Klimakonferenz in Bonn gesprochen . Sie hat dort den Klimawandel als eine zentrale Herausforderung für die Menschheit bezeichnet . Sie hat von einer Schicksalsfrage gesprochen .\n",
      "\n",
      "Now find all roles in the sentence associated with the cue 'gesprochen' you found in the beginning sentence.\n",
      "Assistant: \n",
      "\n",
      "=== out: ===\n",
      "cue: gesprochen\n",
      "ptc: #UNK#\n",
      "evidence: #UNK#\n",
      "medium: #UNK#\n",
      "topic: #UNK#\n",
      "addr: #UNK#\n",
      "message: #UNK#\n",
      "source: Bundeskanzlerin, Angela, Merkel</s>\n",
      "\n",
      "\n",
      "=== in: ===\n",
      "User: A cue is the lexical items in a sentence that indicate that speech, writing, or thought is being reproduced.\n",
      "I want you to extract all cues in the text below.\n",
      "If you find multiple words for one cue, you output them separated by commas.\n",
      "If no cue can be found in the given text, you output the string #UNK# as cue.\n",
      "Now extract all cues from the following sentence.\n",
      "Use the prefix \"Cues: \".\n",
      "Sentence: Sie hat dort den Klimawandel als eine zentrale Herausforderung für die Menschheit bezeichnet .\n",
      "Assistant: Cues: [bezeichnet]</s>\n",
      "User: Now I give you again the sentence only in addition with the two following sentences, because the roles can be partially contained in the following sentences.\n",
      "Text: Sie hat dort den Klimawandel als eine zentrale Herausforderung für die Menschheit bezeichnet . Sie hat von einer Schicksalsfrage gesprochen . Was sie damit meint , das sieht man , wenn man sich zum Beispiel die Situation der Inselstaaten anschaut .\n",
      "\n",
      "Now find all roles in the sentence associated with the cue 'bezeichnet' you found in the beginning sentence.\n",
      "Assistant: \n",
      "\n",
      "=== out: ===\n",
      "cue: bezeichnet\n",
      "ptc: #UNK#\n",
      "evidence: #UNK#\n",
      "medium: #UNK#\n",
      "topic: den, Klimawandel\n",
      "addr: #UNK#\n",
      "message: als, eine, zentrale, Herausforderung, für, die, Menschheit\n",
      "source: Sie</s>\n",
      "\n",
      "\n",
      "=== in: ===\n",
      "User: A cue is the lexical items in a sentence that indicate that speech, writing, or thought is being reproduced.\n",
      "I want you to extract all cues in the text below.\n",
      "If you find multiple words for one cue, you output them separated by commas.\n",
      "If no cue can be found in the given text, you output the string #UNK# as cue.\n",
      "Now extract all cues from the following sentence.\n",
      "Use the prefix \"Cues: \".\n",
      "Sentence: Sie hat von einer Schicksalsfrage gesprochen .\n",
      "Assistant: Cues: [gesprochen]</s>\n",
      "User: Now I give you again the sentence only in addition with the two following sentences, because the roles can be partially contained in the following sentences.\n",
      "Text: Sie hat von einer Schicksalsfrage gesprochen . Was sie damit meint , das sieht man , wenn man sich zum Beispiel die Situation der Inselstaaten anschaut . Die Fidschi-Inseln standen in besonderer Weise im Mittelpunkt der Konferenz , weil die Fidschis die Präsidentschaft übernommen hatten .\n",
      "\n",
      "Now find all roles in the sentence associated with the cue 'gesprochen' you found in the beginning sentence.\n",
      "Assistant: \n",
      "\n",
      "=== out: ===\n",
      "cue: gesprochen\n",
      "ptc: #UNK#\n",
      "evidence: #UNK#\n",
      "medium: #UNK#\n",
      "topic: #UNK#\n",
      "addr: #UNK#\n",
      "message: von, einer, Schicksalsfrage\n",
      "source: Sie</s>\n",
      "\n",
      "\n",
      "=== in: ===\n",
      "User: A cue is the lexical items in a sentence that indicate that speech, writing, or thought is being reproduced.\n",
      "I want you to extract all cues in the text below.\n",
      "If you find multiple words for one cue, you output them separated by commas.\n",
      "If no cue can be found in the given text, you output the string #UNK# as cue.\n",
      "Now extract all cues from the following sentence.\n",
      "Use the prefix \"Cues: \".\n",
      "Sentence: Was sie damit meint , das sieht man , wenn man sich zum Beispiel die Situation der Inselstaaten anschaut .\n",
      "Assistant: Cues: [meint], [sieht]</s>\n",
      "User: Now I give you again the sentence only in addition with the two following sentences, because the roles can be partially contained in the following sentences.\n",
      "Text: Was sie damit meint , das sieht man , wenn man sich zum Beispiel die Situation der Inselstaaten anschaut . Die Fidschi-Inseln standen in besonderer Weise im Mittelpunkt der Konferenz , weil die Fidschis die Präsidentschaft übernommen hatten . Wenn man bedenkt , dass es Inseln gibt , die von Überflutung bedroht sind , dass es dort Menschen gibt , die schon heute ihre Heimat verloren haben , deren Existenz , deren Inseln , deren Heimat durch den Klimawandel bedroht sind , wenn man sich vergegenwärtigt , dass es Umsiedlungen gibt , wenn man also weiß , dass es Menschen gibt , die wegen des Klimawandels flüchten - es gibt Menschen , die zu Klimaflüchtlingen werden - , dann muss man sagen : Wer etwas für die Bekämpfung der Fluchtursachen tun möchte , der muss stark für Klimaschutz sein .\n",
      "\n",
      "Now find all roles in the sentence associated with the cue 'meint' you found in the beginning sentence.\n",
      "Assistant: \n",
      "\n",
      "=== out: ===\n",
      "cue: meint\n",
      "ptc: #UNK#\n",
      "evidence: #UNK#\n",
      "medium: #UNK#\n",
      "topic: #UNK#\n",
      "addr: #UNK#\n",
      "message: Was\n",
      "source: sie</s>\n",
      "\n",
      "\n",
      "=== in: ===\n",
      "User: A cue is the lexical items in a sentence that indicate that speech, writing, or thought is being reproduced.\n",
      "I want you to extract all cues in the text below.\n",
      "If you find multiple words for one cue, you output them separated by commas.\n",
      "If no cue can be found in the given text, you output the string #UNK# as cue.\n",
      "Now extract all cues from the following sentence.\n",
      "Use the prefix \"Cues: \".\n",
      "Sentence: Was sie damit meint , das sieht man , wenn man sich zum Beispiel die Situation der Inselstaaten anschaut .\n",
      "Assistant: Cues: [meint], [sieht]</s>\n",
      "User: Now I give you again the sentence only in addition with the two following sentences, because the roles can be partially contained in the following sentences.\n",
      "Text: Was sie damit meint , das sieht man , wenn man sich zum Beispiel die Situation der Inselstaaten anschaut . Die Fidschi-Inseln standen in besonderer Weise im Mittelpunkt der Konferenz , weil die Fidschis die Präsidentschaft übernommen hatten . Wenn man bedenkt , dass es Inseln gibt , die von Überflutung bedroht sind , dass es dort Menschen gibt , die schon heute ihre Heimat verloren haben , deren Existenz , deren Inseln , deren Heimat durch den Klimawandel bedroht sind , wenn man sich vergegenwärtigt , dass es Umsiedlungen gibt , wenn man also weiß , dass es Menschen gibt , die wegen des Klimawandels flüchten - es gibt Menschen , die zu Klimaflüchtlingen werden - , dann muss man sagen : Wer etwas für die Bekämpfung der Fluchtursachen tun möchte , der muss stark für Klimaschutz sein .\n",
      "\n",
      "Now find all roles in the sentence associated with the cue 'sieht' you found in the beginning sentence.\n",
      "Assistant: \n",
      "\n",
      "=== out: ===\n",
      "cue: sieht\n",
      "ptc: #UNK#\n",
      "evidence: #UNK#\n",
      "medium: #UNK#\n",
      "topic: #UNK#\n",
      "addr: #UNK#\n",
      "message: das\n",
      "source: man</s>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check that the file with the role prompts was written correctly\n",
    "with open(parsed_roles_file) as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "print(f\"Number of samples: {len(lines)}\\n\")\n",
    "\n",
    "print(\"First 5 samples:\")\n",
    "for l in lines[:5]:\n",
    "    print(\"=== in: ===\\n\" + json.loads(l)[\"input\"] + \"\\n\")\n",
    "    print(\"=== out: ===\\n\" + json.loads(l)[\"output\"] + \"\\n\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Check optimal source and target lengths\n",
    "\n",
    " This step is only required if you want to use your own data. If you use the original GermEval 2023 task 1 data, you can skip this step and use the source and target lengths that are already defined in the configurations below at the start of the training code (parameters `source_max_len` and `target_max_len`).\n",
    "\n",
    " If you want to change the maximum source or target lengths, keep in mind that longer prompts mean longer training times and more memory requirements. While it would be best to set the maximum source/target lengths to the maximum lengths of the inputs/outputs, this is not always feasible due to memory constraints. In this case, we recommend choosing maximum lengths that only truncate few samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode all prompt inputs with the Llama 1 tokenizer (same as the Llama 2 tokenizer)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"huggyllama/llama-7b\", padding_side=\"right\", use_fast=False, tokenizer_type=\"llama\"\n",
    ")\n",
    "\n",
    "encoded_inputs_cues = []\n",
    "encoded_inputs_roles = []\n",
    "encoded_outputs_cues = []\n",
    "encoded_outputs_roles = []\n",
    "with open(parsed_cues_file) as f:\n",
    "    for l in f.readlines():\n",
    "        enc_in = tokenizer.encode(json.loads(l)[\"input\"])\n",
    "        encoded_inputs_cues.append(enc_in)\n",
    "        enc_out = tokenizer.encode(json.loads(l)[\"output\"])\n",
    "        encoded_outputs_cues.append(enc_out)\n",
    "with open(parsed_roles_file) as f:\n",
    "    for l in f.readlines():\n",
    "        enc_in = tokenizer.encode(json.loads(l)[\"input\"])\n",
    "        encoded_inputs_roles.append(enc_in)\n",
    "        enc_out = tokenizer.encode(json.loads(l)[\"output\"])\n",
    "        encoded_outputs_roles.append(enc_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cues source lengths\n",
      "max length: 323\n",
      "mean length: 144.7366881217086\n",
      "number of samples longer than 256: 7\n",
      "\n",
      "roles source lengths\n",
      "max length: 648\n",
      "mean length: 327.5176884608261\n",
      "number of samples longer than 640: 6\n"
     ]
    }
   ],
   "source": [
    "# maximum source lengths taken from the config files\n",
    "max_length_source_cues = 256\n",
    "max_length_source_roles = 640\n",
    "\n",
    "print(\"cues source lengths\")\n",
    "len_enc = [len(e) for e in encoded_inputs_cues]\n",
    "print(f\"max length: {max(len_enc)}\")\n",
    "print(f\"mean length: {np.mean(len_enc)}\")\n",
    "print(\n",
    "    f\"number of samples longer than {max_length_source_cues}: {sum(np.array(len_enc) > max_length_source_cues)}\"\n",
    ")\n",
    "print()\n",
    "\n",
    "print(\"roles source lengths\")\n",
    "len_enc = [len(e) for e in encoded_inputs_roles]\n",
    "print(f\"max length: {max(len_enc)}\")\n",
    "print(f\"mean length: {np.mean(len_enc)}\")\n",
    "print(\n",
    "    f\"number of samples longer than {max_length_source_roles}: {sum(np.array(len_enc) > max_length_source_roles)}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cues target lengths\n",
      "max length: 50\n",
      "mean length: 10.395318899941486\n",
      "number of samples longer than 64: 0\n",
      "\n",
      "roles target lengths\n",
      "max length: 287\n",
      "mean length: 73.25745508427487\n",
      "number of samples longer than 256: 2\n"
     ]
    }
   ],
   "source": [
    "# maximum target lengths taken from the config files\n",
    "max_length_target_cues = 64\n",
    "max_length_target_roles = 256\n",
    "\n",
    "print(\"cues target lengths\")\n",
    "len_enc = [len(e) for e in encoded_outputs_cues]\n",
    "print(f\"max length: {max(len_enc)}\")\n",
    "print(f\"mean length: {np.mean(len_enc)}\")\n",
    "print(\n",
    "    f\"number of samples longer than {max_length_target_cues}: {sum(np.array(len_enc) > max_length_target_cues)}\"\n",
    ")\n",
    "print()\n",
    "\n",
    "print(\"roles target lengths\")\n",
    "len_enc = [len(e) for e in encoded_outputs_roles]\n",
    "print(f\"max length: {max(len_enc)}\")\n",
    "print(f\"mean length: {np.mean(len_enc)}\")\n",
    "print(\n",
    "    f\"number of samples longer than {max_length_target_roles}: {sum(np.array(len_enc) > max_length_target_roles)}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Train models\n",
    "\n",
    " This step can be skipped if you already have trained models.\n",
    "\n",
    " For training, you first have to prepare the Llama 2 models and adapt the configuration. To prepare the Llama 2 models, you will have to make them accessible in HF (Huggingface) format. You can either use the models directly from Huggingface or prepare them yourself by first downloading the model weights from [the official Llama repo](https://github.com/facebookresearch/llama) and then converting these weights using their [conversion manual](https://github.com/facebookresearch/llama-recipes/#model-conversion-to-hugging-face). When using the models from Huggingface, you should add the parameter `use_auth_token` with your Huggingface token to the training configs in the code cell below. If you don't want to use the models from Huggingface, once you have prepared the models yourself, update the path to the models in the config (parameter `model_name_or_path`) so the paths point to the folder containing the `pytorch_model-000xx-of-00015.bin` files.\n",
    "\n",
    " Further configuration parameters:\n",
    "\n",
    " - `per_device_train_batch_size` and `gradient_accumulation_steps`: With these two parameters you can control the batch size and the number of accumulation steps when calculating the gradients during training. Larger batch sizes should speed up training, but increase memory requirements considerably. We recommend choosing the parameters so that their product `per_device_train_batch_size * gradient_accumulation_steps` is a multiple of 16.\n",
    " - `save_steps` and `max_steps`: set `max_steps` to control the length of training (`save_steps` determines when checkpoints are created)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define config files for training\n",
    "# 7B models\n",
    "cues_training_config = {\n",
    "    \"model_name_or_path\": \"LeoLM/leo-hessianai-7b\",\n",
    "    \"output_dir\": \"./output/spkatt-7b-cues-leolm\",\n",
    "    \"data_seed\": 42,\n",
    "    \"save_steps\": 500,\n",
    "    \"evaluation_strategy\": \"no\",\n",
    "    \"dataloader_num_workers\": 4,\n",
    "    \"lora_modules\": \"all\",\n",
    "    \"bf16\": True,\n",
    "    \"dataset\": \"transformed_datasets/prompts_training/parsed_data_cues.jsonl\",\n",
    "    \"dataset_format\": \"input-output\",\n",
    "    \"source_max_len\": 256,\n",
    "    \"target_max_len\": 64,\n",
    "    \"per_device_train_batch_size\": 16,\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"max_steps\": 2000,\n",
    "    \"learning_rate\": 0.0002,\n",
    "    \"lora_dropout\": 0.1,\n",
    "    \"seed\": 0,\n",
    "}\n",
    "roles_training_config = {\n",
    "    \"model_name_or_path\": \"LeoLM/leo-hessianai-7b\",\n",
    "    \"output_dir\": \"./output/spkatt-7b-roles-leolm\",\n",
    "    \"data_seed\": 42,\n",
    "    \"save_steps\": 500,\n",
    "    \"evaluation_strategy\": \"no\",\n",
    "    \"dataloader_num_workers\": 4,\n",
    "    \"lora_modules\": \"all\",\n",
    "    \"bf16\": True,\n",
    "    \"dataset\": \"transformed_datasets/prompts_training/parsed_data_roles.jsonl\",\n",
    "    \"dataset_format\": \"input-output\",\n",
    "    \"source_max_len\": 640,\n",
    "    \"target_max_len\": 256,\n",
    "    \"per_device_train_batch_size\": 16,\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"max_steps\": 2000,\n",
    "    \"learning_rate\": 0.0002,\n",
    "    \"lora_dropout\": 0.1,\n",
    "    \"seed\": 0,\n",
    "}\n",
    "\n",
    "# 70B models\n",
    "# cues_training_config = {\"model_name_or_path\": \"meta-llama/Llama-2-70b-hf\",\n",
    "#                         \"output_dir\": \"./output/spkatt-70b-cues\",\n",
    "#                         \"data_seed\": 42,\n",
    "#                         \"save_steps\": 500,\n",
    "#                         \"evaluation_strategy\": \"no\",\n",
    "#                         \"dataloader_num_workers\": 4,\n",
    "#                         \"lora_modules\": \"all\",\n",
    "#                         \"bf16\": True,\n",
    "#                         \"dataset\": \"transformed_datasets/prompts_training/parsed_data_cues.jsonl\",\n",
    "#                         \"dataset_format\": \"input-output\",\n",
    "#                         \"source_max_len\": 256,\n",
    "#                         \"target_max_len\": 64,\n",
    "#                         \"per_device_train_batch_size\": 16,\n",
    "#                         \"gradient_accumulation_steps\": 1,\n",
    "#                         \"max_steps\": 2000,\n",
    "#                         \"learning_rate\": 0.0001,\n",
    "#                         \"lora_dropout\": 0.05,\n",
    "#                         \"seed\": 0,\n",
    "#                         }\n",
    "# roles_training_config = {\"model_name_or_path\": \"meta-llama/Llama-2-70b-hf\",\n",
    "#                          \"output_dir\": \"./output/spkatt-70b-roles\",\n",
    "#                          \"data_seed\": 42,\n",
    "#                          \"save_steps\": 500,\n",
    "#                          \"evaluation_strategy\": \"no\",\n",
    "#                          \"dataloader_num_workers\": 4,\n",
    "#                          \"lora_modules\": \"all\",\n",
    "#                          \"bf16\": True,\n",
    "#                          \"dataset\": \"transformed_datasets/prompts_training/parsed_data_roles.jsonl\",\n",
    "#                          \"dataset_format\": \"input-output\",\n",
    "#                          \"source_max_len\": 640,\n",
    "#                          \"target_max_len\": 256,\n",
    "#                          \"per_device_train_batch_size\": 8,\n",
    "#                          \"gradient_accumulation_steps\": 2,\n",
    "#                          \"max_steps\": 2500,\n",
    "#                          \"learning_rate\": 0.0001,\n",
    "#                          \"lora_dropout\": 0.05,\n",
    "#                          \"seed\": 0,\n",
    "#                          }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(model_name_or_path='LeoLM/leo-hessianai-7b', trust_remote_code=False, use_auth_token=False, eval_dataset_size=1024, max_train_samples=None, max_eval_samples=None, source_max_len=256, target_max_len=64, dataset='transformed_datasets/prompts_training/parsed_data_cues.jsonl', dataset_format='input-output', output_dir='./output/spkatt-7b-cues-leolm', overwrite_output_dir=False, do_train=True, do_eval=False, do_predict=False, evaluation_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=16, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, eval_delay=0, learning_rate=0.0002, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=0.3, num_train_epochs=3.0, max_steps=2000, lr_scheduler_type=<SchedulerType.CONSTANT: 'constant'>, warmup_ratio=0.03, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='./output/spkatt-7b-cues-leolm/runs/Nov21_17-21-21_ds3', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=10, logging_nan_inf_filter=True, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=500, save_total_limit=40, save_safetensors=False, save_on_each_node=False, no_cuda=False, use_mps_device=False, seed=0, data_seed=42, jit_mode_eval=False, use_ipex=False, bf16=True, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=0, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=None, dataloader_num_workers=4, past_index=-1, run_name='./output/spkatt-7b-cues-leolm', disable_tqdm=True, remove_unused_columns=False, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], fsdp=[], fsdp_min_num_params=0, fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.PAGED_ADAMW: 'paged_adamw_32bit'>, optim_args=None, adafactor=False, group_by_length=True, length_column_name='length', report_to=[], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=False, gradient_checkpointing=True, include_inputs_for_metrics=False, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, xpu_backend=None, sortish_sampler=False, predict_with_generate=False, generation_max_length=None, generation_num_beams=None, generation_config=GenerationConfig {\n",
      "  \"max_new_tokens\": 256,\n",
      "  \"transformers_version\": \"4.31.0\"\n",
      "}\n",
      ", cache_dir=None, train_on_source=False, mmlu_split='eval', mmlu_dataset='mmlu-fs', do_mmlu_eval=False, max_mmlu_samples=None, mmlu_source_max_len=2048, full_finetune=False, adam8bit=False, double_quant=True, quant_type='nf4', bits=4, lora_r=64, lora_alpha=16, lora_dropout=0.1, max_memory_MB=80000, distributed_state=Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      ", _n_gpu=1, __cached__setup_devices=device(type='cuda', index=0), deepspeed_plugin=None)\n",
      "loading base model LeoLM/leo-hessianai-7b...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58986d102f4e496fb3d17d2f8f08638e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5631e5392bd64fb1b5f116432d2a4db2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/183 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "170208613cfe4ae49e5f32598d1a0c54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0601218550a84248aaaca59e5534bf95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "450dcc3299d846c38ffbbb87d7bdde0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding special tokens.\n",
      "adding LoRA modules...\n",
      "loaded model\n",
      "/home/users/blaneck/eth/semaSpeakerAttribution\n",
      "trainable params: 79953920.0 || all params: 3660328960 || trainable: 2.1843370056007205\n",
      "torch.bfloat16 422060032 0.1153065849032323\n",
      "torch.uint8 3238002688 0.8846206784649213\n",
      "torch.float32 266240 7.273663184633547e-05\n",
      "{'loss': 1.4894, 'learning_rate': 0.0002, 'epoch': 0.02}\n",
      "{'loss': 0.2678, 'learning_rate': 0.0002, 'epoch': 0.04}\n",
      "{'loss': 0.1864, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 0.1428, 'learning_rate': 0.0002, 'epoch': 0.07}\n",
      "{'loss': 0.0463, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 0.4323, 'learning_rate': 0.0002, 'epoch': 0.11}\n",
      "{'loss': 0.1962, 'learning_rate': 0.0002, 'epoch': 0.13}\n",
      "{'loss': 0.1238, 'learning_rate': 0.0002, 'epoch': 0.15}\n",
      "{'loss': 0.0638, 'learning_rate': 0.0002, 'epoch': 0.17}\n",
      "{'loss': 0.0315, 'learning_rate': 0.0002, 'epoch': 0.19}\n",
      "{'loss': 0.3294, 'learning_rate': 0.0002, 'epoch': 0.21}\n",
      "{'loss': 0.1757, 'learning_rate': 0.0002, 'epoch': 0.22}\n",
      "{'loss': 0.1042, 'learning_rate': 0.0002, 'epoch': 0.24}\n",
      "{'loss': 0.0512, 'learning_rate': 0.0002, 'epoch': 0.26}\n",
      "{'loss': 0.0366, 'learning_rate': 0.0002, 'epoch': 0.28}\n",
      "{'loss': 0.2578, 'learning_rate': 0.0002, 'epoch': 0.3}\n",
      "{'loss': 0.1234, 'learning_rate': 0.0002, 'epoch': 0.32}\n",
      "{'loss': 0.1262, 'learning_rate': 0.0002, 'epoch': 0.34}\n",
      "{'loss': 0.0648, 'learning_rate': 0.0002, 'epoch': 0.36}\n",
      "{'loss': 0.0322, 'learning_rate': 0.0002, 'epoch': 0.37}\n",
      "{'loss': 0.1768, 'learning_rate': 0.0002, 'epoch': 0.39}\n",
      "{'loss': 0.135, 'learning_rate': 0.0002, 'epoch': 0.41}\n",
      "{'loss': 0.0711, 'learning_rate': 0.0002, 'epoch': 0.43}\n",
      "{'loss': 0.0377, 'learning_rate': 0.0002, 'epoch': 0.45}\n",
      "{'loss': 0.0393, 'learning_rate': 0.0002, 'epoch': 0.47}\n",
      "{'loss': 0.2144, 'learning_rate': 0.0002, 'epoch': 0.49}\n",
      "{'loss': 0.0802, 'learning_rate': 0.0002, 'epoch': 0.51}\n",
      "{'loss': 0.0735, 'learning_rate': 0.0002, 'epoch': 0.52}\n",
      "{'loss': 0.0359, 'learning_rate': 0.0002, 'epoch': 0.54}\n",
      "{'loss': 0.0264, 'learning_rate': 0.0002, 'epoch': 0.56}\n",
      "{'loss': 0.196, 'learning_rate': 0.0002, 'epoch': 0.58}\n",
      "{'loss': 0.0686, 'learning_rate': 0.0002, 'epoch': 0.6}\n",
      "{'loss': 0.0572, 'learning_rate': 0.0002, 'epoch': 0.62}\n",
      "{'loss': 0.0432, 'learning_rate': 0.0002, 'epoch': 0.64}\n",
      "{'loss': 0.0213, 'learning_rate': 0.0002, 'epoch': 0.66}\n",
      "{'loss': 0.1821, 'learning_rate': 0.0002, 'epoch': 0.67}\n",
      "{'loss': 0.0841, 'learning_rate': 0.0002, 'epoch': 0.69}\n",
      "{'loss': 0.0721, 'learning_rate': 0.0002, 'epoch': 0.71}\n",
      "{'loss': 0.0336, 'learning_rate': 0.0002, 'epoch': 0.73}\n",
      "{'loss': 0.0275, 'learning_rate': 0.0002, 'epoch': 0.75}\n",
      "{'loss': 0.0974, 'learning_rate': 0.0002, 'epoch': 0.77}\n",
      "{'loss': 0.0811, 'learning_rate': 0.0002, 'epoch': 0.79}\n",
      "{'loss': 0.0624, 'learning_rate': 0.0002, 'epoch': 0.81}\n",
      "{'loss': 0.0292, 'learning_rate': 0.0002, 'epoch': 0.82}\n",
      "{'loss': 0.0251, 'learning_rate': 0.0002, 'epoch': 0.84}\n",
      "{'loss': 0.149, 'learning_rate': 0.0002, 'epoch': 0.86}\n",
      "{'loss': 0.0869, 'learning_rate': 0.0002, 'epoch': 0.88}\n",
      "{'loss': 0.0536, 'learning_rate': 0.0002, 'epoch': 0.9}\n",
      "{'loss': 0.0263, 'learning_rate': 0.0002, 'epoch': 0.92}\n",
      "{'loss': 0.0172, 'learning_rate': 0.0002, 'epoch': 0.94}\n",
      "Saving PEFT checkpoint...\n",
      "{'loss': 0.1063, 'learning_rate': 0.0002, 'epoch': 0.96}\n",
      "{'loss': 0.0499, 'learning_rate': 0.0002, 'epoch': 0.97}\n",
      "{'loss': 0.0311, 'learning_rate': 0.0002, 'epoch': 0.99}\n",
      "{'loss': 0.0673, 'learning_rate': 0.0002, 'epoch': 1.01}\n",
      "{'loss': 0.0441, 'learning_rate': 0.0002, 'epoch': 1.03}\n",
      "{'loss': 0.0203, 'learning_rate': 0.0002, 'epoch': 1.05}\n",
      "{'loss': 0.027, 'learning_rate': 0.0002, 'epoch': 1.07}\n",
      "{'loss': 0.0148, 'learning_rate': 0.0002, 'epoch': 1.09}\n",
      "{'loss': 0.0681, 'learning_rate': 0.0002, 'epoch': 1.1}\n",
      "{'loss': 0.0402, 'learning_rate': 0.0002, 'epoch': 1.12}\n",
      "{'loss': 0.0473, 'learning_rate': 0.0002, 'epoch': 1.14}\n",
      "{'loss': 0.0208, 'learning_rate': 0.0002, 'epoch': 1.16}\n",
      "{'loss': 0.0217, 'learning_rate': 0.0002, 'epoch': 1.18}\n",
      "{'loss': 0.0576, 'learning_rate': 0.0002, 'epoch': 1.2}\n",
      "{'loss': 0.0485, 'learning_rate': 0.0002, 'epoch': 1.22}\n",
      "{'loss': 0.0403, 'learning_rate': 0.0002, 'epoch': 1.24}\n",
      "{'loss': 0.0243, 'learning_rate': 0.0002, 'epoch': 1.25}\n",
      "{'loss': 0.0104, 'learning_rate': 0.0002, 'epoch': 1.27}\n",
      "{'loss': 0.0401, 'learning_rate': 0.0002, 'epoch': 1.29}\n",
      "{'loss': 0.0382, 'learning_rate': 0.0002, 'epoch': 1.31}\n",
      "{'loss': 0.045, 'learning_rate': 0.0002, 'epoch': 1.33}\n",
      "{'loss': 0.0202, 'learning_rate': 0.0002, 'epoch': 1.35}\n",
      "{'loss': 0.009, 'learning_rate': 0.0002, 'epoch': 1.37}\n",
      "{'loss': 0.0924, 'learning_rate': 0.0002, 'epoch': 1.39}\n",
      "{'loss': 0.0441, 'learning_rate': 0.0002, 'epoch': 1.4}\n",
      "{'loss': 0.0449, 'learning_rate': 0.0002, 'epoch': 1.42}\n",
      "{'loss': 0.0201, 'learning_rate': 0.0002, 'epoch': 1.44}\n",
      "{'loss': 0.0107, 'learning_rate': 0.0002, 'epoch': 1.46}\n",
      "{'loss': 0.0536, 'learning_rate': 0.0002, 'epoch': 1.48}\n",
      "{'loss': 0.0577, 'learning_rate': 0.0002, 'epoch': 1.5}\n",
      "{'loss': 0.0426, 'learning_rate': 0.0002, 'epoch': 1.52}\n",
      "{'loss': 0.0285, 'learning_rate': 0.0002, 'epoch': 1.54}\n",
      "{'loss': 0.0157, 'learning_rate': 0.0002, 'epoch': 1.55}\n",
      "{'loss': 0.0442, 'learning_rate': 0.0002, 'epoch': 1.57}\n",
      "{'loss': 0.0343, 'learning_rate': 0.0002, 'epoch': 1.59}\n",
      "{'loss': 0.0442, 'learning_rate': 0.0002, 'epoch': 1.61}\n",
      "{'loss': 0.0293, 'learning_rate': 0.0002, 'epoch': 1.63}\n",
      "{'loss': 0.0279, 'learning_rate': 0.0002, 'epoch': 1.65}\n",
      "{'loss': 0.0663, 'learning_rate': 0.0002, 'epoch': 1.67}\n",
      "{'loss': 0.0503, 'learning_rate': 0.0002, 'epoch': 1.69}\n",
      "{'loss': 0.0268, 'learning_rate': 0.0002, 'epoch': 1.7}\n",
      "{'loss': 0.0194, 'learning_rate': 0.0002, 'epoch': 1.72}\n",
      "{'loss': 0.0176, 'learning_rate': 0.0002, 'epoch': 1.74}\n",
      "{'loss': 0.0478, 'learning_rate': 0.0002, 'epoch': 1.76}\n",
      "{'loss': 0.0373, 'learning_rate': 0.0002, 'epoch': 1.78}\n",
      "{'loss': 0.0265, 'learning_rate': 0.0002, 'epoch': 1.8}\n",
      "{'loss': 0.0171, 'learning_rate': 0.0002, 'epoch': 1.82}\n",
      "{'loss': 0.0213, 'learning_rate': 0.0002, 'epoch': 1.84}\n",
      "{'loss': 0.0585, 'learning_rate': 0.0002, 'epoch': 1.85}\n",
      "{'loss': 0.0436, 'learning_rate': 0.0002, 'epoch': 1.87}\n",
      "Saving PEFT checkpoint...\n",
      "{'loss': 0.0442, 'learning_rate': 0.0002, 'epoch': 1.89}\n",
      "{'loss': 0.0225, 'learning_rate': 0.0002, 'epoch': 1.91}\n",
      "{'loss': 0.0186, 'learning_rate': 0.0002, 'epoch': 1.93}\n",
      "{'loss': 0.0535, 'learning_rate': 0.0002, 'epoch': 1.95}\n",
      "{'loss': 0.0352, 'learning_rate': 0.0002, 'epoch': 1.97}\n",
      "{'loss': 0.0273, 'learning_rate': 0.0002, 'epoch': 1.99}\n",
      "{'loss': 0.0285, 'learning_rate': 0.0002, 'epoch': 2.0}\n",
      "{'loss': 0.0254, 'learning_rate': 0.0002, 'epoch': 2.02}\n",
      "{'loss': 0.0159, 'learning_rate': 0.0002, 'epoch': 2.04}\n",
      "{'loss': 0.0148, 'learning_rate': 0.0002, 'epoch': 2.06}\n",
      "{'loss': 0.0072, 'learning_rate': 0.0002, 'epoch': 2.08}\n",
      "{'loss': 0.02, 'learning_rate': 0.0002, 'epoch': 2.1}\n",
      "{'loss': 0.0328, 'learning_rate': 0.0002, 'epoch': 2.12}\n",
      "{'loss': 0.0205, 'learning_rate': 0.0002, 'epoch': 2.13}\n",
      "{'loss': 0.0171, 'learning_rate': 0.0002, 'epoch': 2.15}\n",
      "{'loss': 0.0123, 'learning_rate': 0.0002, 'epoch': 2.17}\n",
      "{'loss': 0.0243, 'learning_rate': 0.0002, 'epoch': 2.19}\n",
      "{'loss': 0.0282, 'learning_rate': 0.0002, 'epoch': 2.21}\n",
      "{'loss': 0.0284, 'learning_rate': 0.0002, 'epoch': 2.23}\n",
      "{'loss': 0.0233, 'learning_rate': 0.0002, 'epoch': 2.25}\n",
      "{'loss': 0.0105, 'learning_rate': 0.0002, 'epoch': 2.27}\n",
      "{'loss': 0.0235, 'learning_rate': 0.0002, 'epoch': 2.28}\n",
      "{'loss': 0.0224, 'learning_rate': 0.0002, 'epoch': 2.3}\n",
      "{'loss': 0.028, 'learning_rate': 0.0002, 'epoch': 2.32}\n",
      "{'loss': 0.012, 'learning_rate': 0.0002, 'epoch': 2.34}\n",
      "{'loss': 0.0029, 'learning_rate': 0.0002, 'epoch': 2.36}\n",
      "{'loss': 0.0135, 'learning_rate': 0.0002, 'epoch': 2.38}\n",
      "{'loss': 0.0117, 'learning_rate': 0.0002, 'epoch': 2.4}\n",
      "{'loss': 0.0213, 'learning_rate': 0.0002, 'epoch': 2.42}\n",
      "{'loss': 0.0111, 'learning_rate': 0.0002, 'epoch': 2.43}\n",
      "{'loss': 0.0109, 'learning_rate': 0.0002, 'epoch': 2.45}\n",
      "{'loss': 0.0176, 'learning_rate': 0.0002, 'epoch': 2.47}\n",
      "{'loss': 0.0227, 'learning_rate': 0.0002, 'epoch': 2.49}\n",
      "{'loss': 0.02, 'learning_rate': 0.0002, 'epoch': 2.51}\n",
      "{'loss': 0.018, 'learning_rate': 0.0002, 'epoch': 2.53}\n",
      "{'loss': 0.0107, 'learning_rate': 0.0002, 'epoch': 2.55}\n",
      "{'loss': 0.0148, 'learning_rate': 0.0002, 'epoch': 2.57}\n",
      "{'loss': 0.0262, 'learning_rate': 0.0002, 'epoch': 2.58}\n",
      "{'loss': 0.03, 'learning_rate': 0.0002, 'epoch': 2.6}\n",
      "{'loss': 0.0175, 'learning_rate': 0.0002, 'epoch': 2.62}\n",
      "{'loss': 0.0095, 'learning_rate': 0.0002, 'epoch': 2.64}\n",
      "{'loss': 0.0239, 'learning_rate': 0.0002, 'epoch': 2.66}\n",
      "{'loss': 0.0418, 'learning_rate': 0.0002, 'epoch': 2.68}\n",
      "{'loss': 0.0227, 'learning_rate': 0.0002, 'epoch': 2.7}\n",
      "{'loss': 0.0129, 'learning_rate': 0.0002, 'epoch': 2.72}\n",
      "{'loss': 0.0276, 'learning_rate': 0.0002, 'epoch': 2.73}\n",
      "{'loss': 0.0326, 'learning_rate': 0.0002, 'epoch': 2.75}\n",
      "{'loss': 0.0288, 'learning_rate': 0.0002, 'epoch': 2.77}\n",
      "{'loss': 0.0367, 'learning_rate': 0.0002, 'epoch': 2.79}\n",
      "{'loss': 0.0332, 'learning_rate': 0.0002, 'epoch': 2.81}\n",
      "Saving PEFT checkpoint...\n",
      "{'loss': 0.0138, 'learning_rate': 0.0002, 'epoch': 2.83}\n",
      "{'loss': 0.0193, 'learning_rate': 0.0002, 'epoch': 2.85}\n",
      "{'loss': 0.0377, 'learning_rate': 0.0002, 'epoch': 2.87}\n",
      "{'loss': 0.0212, 'learning_rate': 0.0002, 'epoch': 2.88}\n",
      "{'loss': 0.0092, 'learning_rate': 0.0002, 'epoch': 2.9}\n",
      "{'loss': 0.0058, 'learning_rate': 0.0002, 'epoch': 2.92}\n",
      "{'loss': 0.0233, 'learning_rate': 0.0002, 'epoch': 2.94}\n",
      "{'loss': 0.0237, 'learning_rate': 0.0002, 'epoch': 2.96}\n",
      "{'loss': 0.015, 'learning_rate': 0.0002, 'epoch': 2.98}\n",
      "{'loss': 0.0047, 'learning_rate': 0.0002, 'epoch': 3.0}\n",
      "{'loss': 0.0178, 'learning_rate': 0.0002, 'epoch': 3.01}\n",
      "{'loss': 0.0135, 'learning_rate': 0.0002, 'epoch': 3.03}\n",
      "{'loss': 0.0059, 'learning_rate': 0.0002, 'epoch': 3.05}\n",
      "{'loss': 0.0064, 'learning_rate': 0.0002, 'epoch': 3.07}\n",
      "{'loss': 0.0117, 'learning_rate': 0.0002, 'epoch': 3.09}\n",
      "{'loss': 0.0183, 'learning_rate': 0.0002, 'epoch': 3.11}\n",
      "{'loss': 0.0152, 'learning_rate': 0.0002, 'epoch': 3.13}\n",
      "{'loss': 0.0176, 'learning_rate': 0.0002, 'epoch': 3.15}\n",
      "{'loss': 0.0108, 'learning_rate': 0.0002, 'epoch': 3.16}\n",
      "{'loss': 0.0085, 'learning_rate': 0.0002, 'epoch': 3.18}\n",
      "{'loss': 0.0225, 'learning_rate': 0.0002, 'epoch': 3.2}\n",
      "{'loss': 0.0142, 'learning_rate': 0.0002, 'epoch': 3.22}\n",
      "{'loss': 0.0156, 'learning_rate': 0.0002, 'epoch': 3.24}\n",
      "{'loss': 0.0046, 'learning_rate': 0.0002, 'epoch': 3.26}\n",
      "{'loss': 0.0027, 'learning_rate': 0.0002, 'epoch': 3.28}\n",
      "{'loss': 0.0203, 'learning_rate': 0.0002, 'epoch': 3.3}\n",
      "{'loss': 0.0136, 'learning_rate': 0.0002, 'epoch': 3.31}\n",
      "{'loss': 0.0151, 'learning_rate': 0.0002, 'epoch': 3.33}\n",
      "{'loss': 0.0128, 'learning_rate': 0.0002, 'epoch': 3.35}\n",
      "{'loss': 0.0064, 'learning_rate': 0.0002, 'epoch': 3.37}\n",
      "{'loss': 0.0241, 'learning_rate': 0.0002, 'epoch': 3.39}\n",
      "{'loss': 0.016, 'learning_rate': 0.0002, 'epoch': 3.41}\n",
      "{'loss': 0.012, 'learning_rate': 0.0002, 'epoch': 3.43}\n",
      "{'loss': 0.0055, 'learning_rate': 0.0002, 'epoch': 3.45}\n",
      "{'loss': 0.0071, 'learning_rate': 0.0002, 'epoch': 3.46}\n",
      "{'loss': 0.0282, 'learning_rate': 0.0002, 'epoch': 3.48}\n",
      "{'loss': 0.0264, 'learning_rate': 0.0002, 'epoch': 3.5}\n",
      "{'loss': 0.0118, 'learning_rate': 0.0002, 'epoch': 3.52}\n",
      "{'loss': 0.0124, 'learning_rate': 0.0002, 'epoch': 3.54}\n",
      "{'loss': 0.011, 'learning_rate': 0.0002, 'epoch': 3.56}\n",
      "{'loss': 0.0184, 'learning_rate': 0.0002, 'epoch': 3.58}\n",
      "{'loss': 0.0213, 'learning_rate': 0.0002, 'epoch': 3.6}\n",
      "{'loss': 0.0131, 'learning_rate': 0.0002, 'epoch': 3.61}\n",
      "{'loss': 0.0133, 'learning_rate': 0.0002, 'epoch': 3.63}\n",
      "{'loss': 0.0079, 'learning_rate': 0.0002, 'epoch': 3.65}\n",
      "{'loss': 0.02, 'learning_rate': 0.0002, 'epoch': 3.67}\n",
      "{'loss': 0.0177, 'learning_rate': 0.0002, 'epoch': 3.69}\n",
      "{'loss': 0.0132, 'learning_rate': 0.0002, 'epoch': 3.71}\n",
      "{'loss': 0.0096, 'learning_rate': 0.0002, 'epoch': 3.73}\n",
      "{'loss': 0.0023, 'learning_rate': 0.0002, 'epoch': 3.75}\n",
      "Saving PEFT checkpoint...\n",
      "{'train_runtime': 5309.2159, 'train_samples_per_second': 6.027, 'train_steps_per_second': 0.377, 'train_loss': 0.051672196897678074, 'epoch': 3.75}\n",
      "Saving PEFT checkpoint...\n",
      "***** train metrics *****\n",
      "  epoch                    =       3.75\n",
      "  train_loss               =     0.0517\n",
      "  train_runtime            = 1:28:29.21\n",
      "  train_samples_per_second =      6.027\n",
      "  train_steps_per_second   =      0.377\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(cues_training_config)\n",
    "\n",
    "# free vram after training\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(model_name_or_path='LeoLM/leo-hessianai-7b', trust_remote_code=False, use_auth_token=False, eval_dataset_size=1024, max_train_samples=None, max_eval_samples=None, source_max_len=640, target_max_len=256, dataset='transformed_datasets/prompts_training/parsed_data_roles.jsonl', dataset_format='input-output', output_dir='./output/spkatt-7b-roles-leolm', overwrite_output_dir=False, do_train=True, do_eval=False, do_predict=False, evaluation_strategy=<IntervalStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=16, per_device_eval_batch_size=8, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, eval_delay=0, learning_rate=0.0002, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=0.3, num_train_epochs=3.0, max_steps=2000, lr_scheduler_type=<SchedulerType.CONSTANT: 'constant'>, warmup_ratio=0.03, warmup_steps=0, log_level='passive', log_level_replica='warning', log_on_each_node=True, logging_dir='./output/spkatt-7b-roles-leolm/runs/Nov21_18-51-41_ds3', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=10, logging_nan_inf_filter=True, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=500, save_total_limit=40, save_safetensors=False, save_on_each_node=False, no_cuda=False, use_mps_device=False, seed=0, data_seed=42, jit_mode_eval=False, use_ipex=False, bf16=True, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=0, ddp_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=None, dataloader_num_workers=4, past_index=-1, run_name='./output/spkatt-7b-roles-leolm', disable_tqdm=True, remove_unused_columns=False, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], fsdp=[], fsdp_min_num_params=0, fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False}, fsdp_transformer_layer_cls_to_wrap=None, deepspeed=None, label_smoothing_factor=0.0, optim=<OptimizerNames.PAGED_ADAMW: 'paged_adamw_32bit'>, optim_args=None, adafactor=False, group_by_length=True, length_column_name='length', report_to=[], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, ddp_broadcast_buffers=None, dataloader_pin_memory=True, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, hub_private_repo=False, gradient_checkpointing=True, include_inputs_for_metrics=False, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', auto_find_batch_size=False, full_determinism=False, torchdynamo=None, ray_scope='last', ddp_timeout=1800, torch_compile=False, torch_compile_backend=None, torch_compile_mode=None, xpu_backend=None, sortish_sampler=False, predict_with_generate=False, generation_max_length=None, generation_num_beams=None, generation_config=GenerationConfig {\n",
      "  \"max_new_tokens\": 256,\n",
      "  \"transformers_version\": \"4.31.0\"\n",
      "}\n",
      ", cache_dir=None, train_on_source=False, mmlu_split='eval', mmlu_dataset='mmlu-fs', do_mmlu_eval=False, max_mmlu_samples=None, mmlu_source_max_len=2048, full_finetune=False, adam8bit=False, double_quant=True, quant_type='nf4', bits=4, lora_r=64, lora_alpha=16, lora_dropout=0.1, max_memory_MB=80000, distributed_state=Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      ", _n_gpu=1, __cached__setup_devices=device(type='cuda', index=0), deepspeed_plugin=None)\n",
      "loading base model LeoLM/leo-hessianai-7b...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e89d387ec6fe449ebad762d879971b6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding special tokens.\n",
      "adding LoRA modules...\n",
      "loaded model\n",
      "/home/users/blaneck/eth/semaSpeakerAttribution\n",
      "trainable params: 79953920.0 || all params: 3660328960 || trainable: 2.1843370056007205\n",
      "torch.bfloat16 422060032 0.1153065849032323\n",
      "torch.uint8 3238002688 0.8846206784649213\n",
      "torch.float32 266240 7.273663184633547e-05\n",
      "{'loss': 0.791, 'learning_rate': 0.0002, 'epoch': 0.03}\n",
      "{'loss': 0.127, 'learning_rate': 0.0002, 'epoch': 0.06}\n",
      "{'loss': 0.0706, 'learning_rate': 0.0002, 'epoch': 0.09}\n",
      "{'loss': 0.0481, 'learning_rate': 0.0002, 'epoch': 0.12}\n",
      "{'loss': 0.0475, 'learning_rate': 0.0002, 'epoch': 0.15}\n",
      "{'loss': 0.0689, 'learning_rate': 0.0002, 'epoch': 0.18}\n",
      "{'loss': 0.043, 'learning_rate': 0.0002, 'epoch': 0.21}\n",
      "{'loss': 0.0383, 'learning_rate': 0.0002, 'epoch': 0.24}\n",
      "{'loss': 0.0351, 'learning_rate': 0.0002, 'epoch': 0.27}\n",
      "{'loss': 0.0255, 'learning_rate': 0.0002, 'epoch': 0.3}\n",
      "{'loss': 0.0423, 'learning_rate': 0.0002, 'epoch': 0.33}\n",
      "{'loss': 0.0444, 'learning_rate': 0.0002, 'epoch': 0.36}\n",
      "{'loss': 0.0309, 'learning_rate': 0.0002, 'epoch': 0.38}\n",
      "{'loss': 0.0386, 'learning_rate': 0.0002, 'epoch': 0.41}\n",
      "{'loss': 0.0249, 'learning_rate': 0.0002, 'epoch': 0.44}\n",
      "{'loss': 0.0316, 'learning_rate': 0.0002, 'epoch': 0.47}\n",
      "{'loss': 0.0332, 'learning_rate': 0.0002, 'epoch': 0.5}\n",
      "{'loss': 0.0286, 'learning_rate': 0.0002, 'epoch': 0.53}\n",
      "{'loss': 0.021, 'learning_rate': 0.0002, 'epoch': 0.56}\n",
      "{'loss': 0.0232, 'learning_rate': 0.0002, 'epoch': 0.59}\n",
      "{'loss': 0.0285, 'learning_rate': 0.0002, 'epoch': 0.62}\n",
      "{'loss': 0.0252, 'learning_rate': 0.0002, 'epoch': 0.65}\n",
      "{'loss': 0.0276, 'learning_rate': 0.0002, 'epoch': 0.68}\n",
      "{'loss': 0.0293, 'learning_rate': 0.0002, 'epoch': 0.71}\n",
      "{'loss': 0.0188, 'learning_rate': 0.0002, 'epoch': 0.74}\n",
      "{'loss': 0.0355, 'learning_rate': 0.0002, 'epoch': 0.77}\n",
      "{'loss': 0.0267, 'learning_rate': 0.0002, 'epoch': 0.8}\n",
      "{'loss': 0.0235, 'learning_rate': 0.0002, 'epoch': 0.83}\n",
      "{'loss': 0.0277, 'learning_rate': 0.0002, 'epoch': 0.86}\n",
      "{'loss': 0.0264, 'learning_rate': 0.0002, 'epoch': 0.89}\n",
      "{'loss': 0.029, 'learning_rate': 0.0002, 'epoch': 0.92}\n",
      "{'loss': 0.0372, 'learning_rate': 0.0002, 'epoch': 0.95}\n",
      "{'loss': 0.0246, 'learning_rate': 0.0002, 'epoch': 0.98}\n",
      "{'loss': 0.018, 'learning_rate': 0.0002, 'epoch': 1.01}\n",
      "{'loss': 0.0268, 'learning_rate': 0.0002, 'epoch': 1.04}\n",
      "{'loss': 0.0182, 'learning_rate': 0.0002, 'epoch': 1.07}\n",
      "{'loss': 0.0122, 'learning_rate': 0.0002, 'epoch': 1.09}\n",
      "{'loss': 0.0152, 'learning_rate': 0.0002, 'epoch': 1.12}\n",
      "{'loss': 0.0118, 'learning_rate': 0.0002, 'epoch': 1.15}\n",
      "{'loss': 0.0143, 'learning_rate': 0.0002, 'epoch': 1.18}\n",
      "{'loss': 0.018, 'learning_rate': 0.0002, 'epoch': 1.21}\n",
      "{'loss': 0.0212, 'learning_rate': 0.0002, 'epoch': 1.24}\n",
      "{'loss': 0.0151, 'learning_rate': 0.0002, 'epoch': 1.27}\n",
      "{'loss': 0.0161, 'learning_rate': 0.0002, 'epoch': 1.3}\n",
      "{'loss': 0.0178, 'learning_rate': 0.0002, 'epoch': 1.33}\n",
      "{'loss': 0.0185, 'learning_rate': 0.0002, 'epoch': 1.36}\n",
      "{'loss': 0.0157, 'learning_rate': 0.0002, 'epoch': 1.39}\n",
      "{'loss': 0.0125, 'learning_rate': 0.0002, 'epoch': 1.42}\n",
      "{'loss': 0.0131, 'learning_rate': 0.0002, 'epoch': 1.45}\n",
      "{'loss': 0.0303, 'learning_rate': 0.0002, 'epoch': 1.48}\n",
      "Saving PEFT checkpoint...\n",
      "{'loss': 0.0146, 'learning_rate': 0.0002, 'epoch': 1.51}\n",
      "{'loss': 0.0236, 'learning_rate': 0.0002, 'epoch': 1.54}\n",
      "{'loss': 0.0143, 'learning_rate': 0.0002, 'epoch': 1.57}\n",
      "{'loss': 0.0185, 'learning_rate': 0.0002, 'epoch': 1.6}\n",
      "{'loss': 0.0207, 'learning_rate': 0.0002, 'epoch': 1.63}\n",
      "{'loss': 0.0252, 'learning_rate': 0.0002, 'epoch': 1.66}\n",
      "{'loss': 0.0212, 'learning_rate': 0.0002, 'epoch': 1.69}\n",
      "{'loss': 0.0147, 'learning_rate': 0.0002, 'epoch': 1.72}\n",
      "{'loss': 0.0127, 'learning_rate': 0.0002, 'epoch': 1.75}\n",
      "{'loss': 0.0188, 'learning_rate': 0.0002, 'epoch': 1.78}\n",
      "{'loss': 0.0163, 'learning_rate': 0.0002, 'epoch': 1.8}\n",
      "{'loss': 0.0149, 'learning_rate': 0.0002, 'epoch': 1.83}\n",
      "{'loss': 0.0123, 'learning_rate': 0.0002, 'epoch': 1.86}\n",
      "{'loss': 0.0205, 'learning_rate': 0.0002, 'epoch': 1.89}\n",
      "{'loss': 0.0171, 'learning_rate': 0.0002, 'epoch': 1.92}\n",
      "{'loss': 0.0218, 'learning_rate': 0.0002, 'epoch': 1.95}\n",
      "{'loss': 0.0136, 'learning_rate': 0.0002, 'epoch': 1.98}\n",
      "{'loss': 0.0183, 'learning_rate': 0.0002, 'epoch': 2.01}\n",
      "{'loss': 0.0083, 'learning_rate': 0.0002, 'epoch': 2.04}\n",
      "{'loss': 0.0114, 'learning_rate': 0.0002, 'epoch': 2.07}\n",
      "{'loss': 0.0106, 'learning_rate': 0.0002, 'epoch': 2.1}\n",
      "{'loss': 0.0107, 'learning_rate': 0.0002, 'epoch': 2.13}\n",
      "{'loss': 0.0096, 'learning_rate': 0.0002, 'epoch': 2.16}\n",
      "{'loss': 0.0144, 'learning_rate': 0.0002, 'epoch': 2.19}\n",
      "{'loss': 0.0093, 'learning_rate': 0.0002, 'epoch': 2.22}\n",
      "{'loss': 0.0052, 'learning_rate': 0.0002, 'epoch': 2.25}\n",
      "{'loss': 0.0106, 'learning_rate': 0.0002, 'epoch': 2.28}\n",
      "{'loss': 0.0145, 'learning_rate': 0.0002, 'epoch': 2.31}\n",
      "{'loss': 0.0105, 'learning_rate': 0.0002, 'epoch': 2.34}\n",
      "{'loss': 0.0095, 'learning_rate': 0.0002, 'epoch': 2.37}\n",
      "{'loss': 0.0121, 'learning_rate': 0.0002, 'epoch': 2.4}\n",
      "{'loss': 0.0064, 'learning_rate': 0.0002, 'epoch': 2.43}\n",
      "{'loss': 0.0075, 'learning_rate': 0.0002, 'epoch': 2.46}\n",
      "{'loss': 0.0117, 'learning_rate': 0.0002, 'epoch': 2.49}\n",
      "{'loss': 0.0097, 'learning_rate': 0.0002, 'epoch': 2.51}\n",
      "{'loss': 0.0073, 'learning_rate': 0.0002, 'epoch': 2.54}\n",
      "{'loss': 0.005, 'learning_rate': 0.0002, 'epoch': 2.57}\n",
      "{'loss': 0.0116, 'learning_rate': 0.0002, 'epoch': 2.6}\n",
      "{'loss': 0.0136, 'learning_rate': 0.0002, 'epoch': 2.63}\n",
      "{'loss': 0.0115, 'learning_rate': 0.0002, 'epoch': 2.66}\n",
      "{'loss': 0.0133, 'learning_rate': 0.0002, 'epoch': 2.69}\n",
      "{'loss': 0.0112, 'learning_rate': 0.0002, 'epoch': 2.72}\n",
      "{'loss': 0.0112, 'learning_rate': 0.0002, 'epoch': 2.75}\n",
      "{'loss': 0.0135, 'learning_rate': 0.0002, 'epoch': 2.78}\n",
      "{'loss': 0.0122, 'learning_rate': 0.0002, 'epoch': 2.81}\n",
      "{'loss': 0.0128, 'learning_rate': 0.0002, 'epoch': 2.84}\n",
      "{'loss': 0.0082, 'learning_rate': 0.0002, 'epoch': 2.87}\n",
      "{'loss': 0.0087, 'learning_rate': 0.0002, 'epoch': 2.9}\n",
      "{'loss': 0.0129, 'learning_rate': 0.0002, 'epoch': 2.93}\n",
      "{'loss': 0.011, 'learning_rate': 0.0002, 'epoch': 2.96}\n",
      "Saving PEFT checkpoint...\n",
      "{'loss': 0.007, 'learning_rate': 0.0002, 'epoch': 2.99}\n",
      "{'loss': 0.0061, 'learning_rate': 0.0002, 'epoch': 3.02}\n",
      "{'loss': 0.006, 'learning_rate': 0.0002, 'epoch': 3.05}\n",
      "{'loss': 0.0056, 'learning_rate': 0.0002, 'epoch': 3.08}\n",
      "{'loss': 0.009, 'learning_rate': 0.0002, 'epoch': 3.11}\n",
      "{'loss': 0.0057, 'learning_rate': 0.0002, 'epoch': 3.14}\n",
      "{'loss': 0.0087, 'learning_rate': 0.0002, 'epoch': 3.17}\n",
      "{'loss': 0.0067, 'learning_rate': 0.0002, 'epoch': 3.2}\n",
      "{'loss': 0.0074, 'learning_rate': 0.0002, 'epoch': 3.22}\n",
      "{'loss': 0.0047, 'learning_rate': 0.0002, 'epoch': 3.25}\n",
      "{'loss': 0.004, 'learning_rate': 0.0002, 'epoch': 3.28}\n",
      "{'loss': 0.0078, 'learning_rate': 0.0002, 'epoch': 3.31}\n",
      "{'loss': 0.0085, 'learning_rate': 0.0002, 'epoch': 3.34}\n",
      "{'loss': 0.0098, 'learning_rate': 0.0002, 'epoch': 3.37}\n",
      "{'loss': 0.0067, 'learning_rate': 0.0002, 'epoch': 3.4}\n",
      "{'loss': 0.0058, 'learning_rate': 0.0002, 'epoch': 3.43}\n",
      "{'loss': 0.0059, 'learning_rate': 0.0002, 'epoch': 3.46}\n",
      "{'loss': 0.0098, 'learning_rate': 0.0002, 'epoch': 3.49}\n",
      "{'loss': 0.0064, 'learning_rate': 0.0002, 'epoch': 3.52}\n",
      "{'loss': 0.0063, 'learning_rate': 0.0002, 'epoch': 3.55}\n",
      "{'loss': 0.0084, 'learning_rate': 0.0002, 'epoch': 3.58}\n",
      "{'loss': 0.0067, 'learning_rate': 0.0002, 'epoch': 3.61}\n",
      "{'loss': 0.0071, 'learning_rate': 0.0002, 'epoch': 3.64}\n",
      "{'loss': 0.0081, 'learning_rate': 0.0002, 'epoch': 3.67}\n",
      "{'loss': 0.0088, 'learning_rate': 0.0002, 'epoch': 3.7}\n",
      "{'loss': 0.0078, 'learning_rate': 0.0002, 'epoch': 3.73}\n",
      "{'loss': 0.0088, 'learning_rate': 0.0002, 'epoch': 3.76}\n",
      "{'loss': 0.0109, 'learning_rate': 0.0002, 'epoch': 3.79}\n",
      "{'loss': 0.0064, 'learning_rate': 0.0002, 'epoch': 3.82}\n",
      "{'loss': 0.0081, 'learning_rate': 0.0002, 'epoch': 3.85}\n",
      "{'loss': 0.0042, 'learning_rate': 0.0002, 'epoch': 3.88}\n",
      "{'loss': 0.0083, 'learning_rate': 0.0002, 'epoch': 3.91}\n",
      "{'loss': 0.0106, 'learning_rate': 0.0002, 'epoch': 3.93}\n",
      "{'loss': 0.0076, 'learning_rate': 0.0002, 'epoch': 3.96}\n",
      "{'loss': 0.0066, 'learning_rate': 0.0002, 'epoch': 3.99}\n",
      "{'loss': 0.0082, 'learning_rate': 0.0002, 'epoch': 4.02}\n",
      "{'loss': 0.0057, 'learning_rate': 0.0002, 'epoch': 4.05}\n",
      "{'loss': 0.004, 'learning_rate': 0.0002, 'epoch': 4.08}\n",
      "{'loss': 0.0052, 'learning_rate': 0.0002, 'epoch': 4.11}\n",
      "{'loss': 0.005, 'learning_rate': 0.0002, 'epoch': 4.14}\n",
      "{'loss': 0.0067, 'learning_rate': 0.0002, 'epoch': 4.17}\n",
      "{'loss': 0.0051, 'learning_rate': 0.0002, 'epoch': 4.2}\n",
      "{'loss': 0.0026, 'learning_rate': 0.0002, 'epoch': 4.23}\n",
      "{'loss': 0.0066, 'learning_rate': 0.0002, 'epoch': 4.26}\n",
      "{'loss': 0.0022, 'learning_rate': 0.0002, 'epoch': 4.29}\n",
      "{'loss': 0.005, 'learning_rate': 0.0002, 'epoch': 4.32}\n",
      "{'loss': 0.0067, 'learning_rate': 0.0002, 'epoch': 4.35}\n",
      "{'loss': 0.0053, 'learning_rate': 0.0002, 'epoch': 4.38}\n",
      "{'loss': 0.0052, 'learning_rate': 0.0002, 'epoch': 4.41}\n",
      "{'loss': 0.0039, 'learning_rate': 0.0002, 'epoch': 4.44}\n",
      "Saving PEFT checkpoint...\n",
      "{'loss': 0.0048, 'learning_rate': 0.0002, 'epoch': 4.47}\n",
      "{'loss': 0.0077, 'learning_rate': 0.0002, 'epoch': 4.5}\n",
      "{'loss': 0.0081, 'learning_rate': 0.0002, 'epoch': 4.53}\n",
      "{'loss': 0.0066, 'learning_rate': 0.0002, 'epoch': 4.56}\n",
      "{'loss': 0.0065, 'learning_rate': 0.0002, 'epoch': 4.59}\n",
      "{'loss': 0.0058, 'learning_rate': 0.0002, 'epoch': 4.62}\n",
      "{'loss': 0.0048, 'learning_rate': 0.0002, 'epoch': 4.64}\n",
      "{'loss': 0.0068, 'learning_rate': 0.0002, 'epoch': 4.67}\n",
      "{'loss': 0.0063, 'learning_rate': 0.0002, 'epoch': 4.7}\n",
      "{'loss': 0.0042, 'learning_rate': 0.0002, 'epoch': 4.73}\n",
      "{'loss': 0.0104, 'learning_rate': 0.0002, 'epoch': 4.76}\n",
      "{'loss': 0.0064, 'learning_rate': 0.0002, 'epoch': 4.79}\n",
      "{'loss': 0.0079, 'learning_rate': 0.0002, 'epoch': 4.82}\n",
      "{'loss': 0.0036, 'learning_rate': 0.0002, 'epoch': 4.85}\n",
      "{'loss': 0.005, 'learning_rate': 0.0002, 'epoch': 4.88}\n",
      "{'loss': 0.0059, 'learning_rate': 0.0002, 'epoch': 4.91}\n",
      "{'loss': 0.0082, 'learning_rate': 0.0002, 'epoch': 4.94}\n",
      "{'loss': 0.0049, 'learning_rate': 0.0002, 'epoch': 4.97}\n",
      "{'loss': 0.0037, 'learning_rate': 0.0002, 'epoch': 5.0}\n",
      "{'loss': 0.0074, 'learning_rate': 0.0002, 'epoch': 5.03}\n",
      "{'loss': 0.0039, 'learning_rate': 0.0002, 'epoch': 5.06}\n",
      "{'loss': 0.0029, 'learning_rate': 0.0002, 'epoch': 5.09}\n",
      "{'loss': 0.004, 'learning_rate': 0.0002, 'epoch': 5.12}\n",
      "{'loss': 0.0041, 'learning_rate': 0.0002, 'epoch': 5.15}\n",
      "{'loss': 0.0104, 'learning_rate': 0.0002, 'epoch': 5.18}\n",
      "{'loss': 0.006, 'learning_rate': 0.0002, 'epoch': 5.21}\n",
      "{'loss': 0.0033, 'learning_rate': 0.0002, 'epoch': 5.24}\n",
      "{'loss': 0.003, 'learning_rate': 0.0002, 'epoch': 5.27}\n",
      "{'loss': 0.0023, 'learning_rate': 0.0002, 'epoch': 5.3}\n",
      "{'loss': 0.0065, 'learning_rate': 0.0002, 'epoch': 5.33}\n",
      "{'loss': 0.0049, 'learning_rate': 0.0002, 'epoch': 5.36}\n",
      "{'loss': 0.0046, 'learning_rate': 0.0002, 'epoch': 5.38}\n",
      "{'loss': 0.0034, 'learning_rate': 0.0002, 'epoch': 5.41}\n",
      "{'loss': 0.003, 'learning_rate': 0.0002, 'epoch': 5.44}\n",
      "{'loss': 0.0056, 'learning_rate': 0.0002, 'epoch': 5.47}\n",
      "{'loss': 0.004, 'learning_rate': 0.0002, 'epoch': 5.5}\n",
      "{'loss': 0.0036, 'learning_rate': 0.0002, 'epoch': 5.53}\n",
      "{'loss': 0.0031, 'learning_rate': 0.0002, 'epoch': 5.56}\n",
      "{'loss': 0.0032, 'learning_rate': 0.0002, 'epoch': 5.59}\n",
      "{'loss': 0.006, 'learning_rate': 0.0002, 'epoch': 5.62}\n",
      "{'loss': 0.0037, 'learning_rate': 0.0002, 'epoch': 5.65}\n",
      "{'loss': 0.0043, 'learning_rate': 0.0002, 'epoch': 5.68}\n",
      "{'loss': 0.0045, 'learning_rate': 0.0002, 'epoch': 5.71}\n",
      "{'loss': 0.0034, 'learning_rate': 0.0002, 'epoch': 5.74}\n",
      "{'loss': 0.0064, 'learning_rate': 0.0002, 'epoch': 5.77}\n",
      "{'loss': 0.0075, 'learning_rate': 0.0002, 'epoch': 5.8}\n",
      "{'loss': 0.0042, 'learning_rate': 0.0002, 'epoch': 5.83}\n",
      "{'loss': 0.0034, 'learning_rate': 0.0002, 'epoch': 5.86}\n",
      "{'loss': 0.0045, 'learning_rate': 0.0002, 'epoch': 5.89}\n",
      "{'loss': 0.0074, 'learning_rate': 0.0002, 'epoch': 5.92}\n",
      "Saving PEFT checkpoint...\n",
      "{'train_runtime': 12864.1325, 'train_samples_per_second': 2.488, 'train_steps_per_second': 0.155, 'train_loss': 0.017598567144945263, 'epoch': 5.92}\n",
      "Saving PEFT checkpoint...\n",
      "***** train metrics *****\n",
      "  epoch                    =       5.92\n",
      "  train_loss               =     0.0176\n",
      "  train_runtime            = 3:34:24.13\n",
      "  train_samples_per_second =      2.488\n",
      "  train_steps_per_second   =      0.155\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train(roles_training_config)\n",
    "\n",
    "# free vram after training\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Inference for Cues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Load cue model for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08e8cc3e8f364c20be0bd928cf49843f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    cues_training_config[\"model_name_or_path\"],\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "checkpoint_dir = (\n",
    "    cues_training_config[\"output_dir\"] + \"/checkpoint-2000/\" # 2000\n",
    ")  # choose checkpoint\n",
    "model = PeftModel.from_pretrained(model, os.path.join(checkpoint_dir, \"adapter_model\"))\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\n",
    "    cues_training_config[\"model_name_or_path\"], legacy=False\n",
    ")\n",
    "tokenizer.bos_token_id = 1\n",
    "\n",
    "pipe = pipeline(\n",
    "    task=\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=300\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Build Cue-LLM-Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_cues = \"\"\"User: A cue is the lexical items in a sentence that indicate that speech, writing, or thought is being reproduced.\n",
    "I want you to extract all cues in the text below.\n",
    "If you find multiple words for one cue, you output them separated by commas.\n",
    "If no cue can be found in the given text, you output the string #UNK# as cue.\n",
    "Now extract all cues from the following sentence.\n",
    "Use the prefix \\\"Cues: \\\".\n",
    "Sentence: {Satz}\n",
    "Assistant:\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_cues = PromptTemplate(input_variables=[\"Satz\"], template=template_cues)\n",
    "llm_chain_cues = LLMChain(prompt=prompt_cues, llm=llm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cues: 100%|██████████| 3067/3067 [14:32<00:00,  3.52it/s]  \n"
     ]
    }
   ],
   "source": [
    "outputs_cues = []\n",
    "for row in tqdm(inputs, desc=\"Cues\"):\n",
    "    outputs_cues.append(llm_chain_cues.apply([row])[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# free vram after inference for cues\n",
    "del tokenizer\n",
    "del model\n",
    "del pipe\n",
    "del llm\n",
    "del llm_chain_cues\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Save and extract cues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Save data in correct format and insert raw cue outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_outputs_to_output_file_format(inputs, outputs):\n",
    "    result = {}\n",
    "    seen_sentences = []\n",
    "\n",
    "    for i, row in enumerate(inputs):\n",
    "        if row[\"FileName\"] not in result:\n",
    "            result[row[\"FileName\"]] = {\n",
    "                \"Sentences\": [],\n",
    "                \"Annotations\": [],\n",
    "                \"Outputs\": {\"Cues\": {}},\n",
    "            }\n",
    "\n",
    "        if row[\"FileName\"] + \"-\" + str(row[\"SentenceId\"]) not in seen_sentences:\n",
    "            seen_sentences.append(row[\"FileName\"] + \"-\" + str(row[\"SentenceId\"]))\n",
    "            result[row[\"FileName\"]][\"Sentences\"].append(\n",
    "                {\"SentenceId\": row[\"SentenceId\"], \"Tokens\": row[\"Tokens\"]}\n",
    "            )\n",
    "\n",
    "        if row[\"SentenceId\"] not in result[row[\"FileName\"]][\"Outputs\"][\"Cues\"]:\n",
    "            result[row[\"FileName\"]][\"Outputs\"][\"Cues\"][row[\"SentenceId\"]] = []\n",
    "\n",
    "        result[row[\"FileName\"]][\"Outputs\"][\"Cues\"][row[\"SentenceId\"]].append(\n",
    "            outputs[i][\"text\"]\n",
    "        )\n",
    "\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_outputs_to_output_files(inputs, outputs):\n",
    "    path = \"./output/data/\"\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    for key, value in map_outputs_to_output_file_format(inputs, outputs).items():\n",
    "        with open(path + key, \"w\", encoding=\"utf8\") as outfile:\n",
    "            json.dump(value, outfile, indent=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_outputs_to_output_files(inputs, outputs_cues)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Map raw cue outputs to cues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_overlap(cues):\n",
    "    for i, cue in enumerate(cues):\n",
    "        for j in range(i + 1, len(cues)):\n",
    "            if len(list(set(cue) & set(cues[j]))) > 0:\n",
    "                return True, i, j\n",
    "    return False, -1, -1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_cues_from_output(output_string: str):\n",
    "    output_string = output_string.strip().split(\"\\n\")[0].strip()\n",
    "\n",
    "    if output_string.startswith(\"Cues:\"):\n",
    "        output_string = output_string[5:].strip()\n",
    "    else:\n",
    "        raise SystemError\n",
    "\n",
    "    if output_string == \"\" or output_string == \"#UNK#\":\n",
    "        return []\n",
    "\n",
    "    outputs = [v.strip() for v in output_string.strip().split(\"],\")]\n",
    "\n",
    "    cues = []\n",
    "    for i, output in enumerate(outputs):\n",
    "        if i < len(outputs) - 1:\n",
    "            output = output + \"]\"\n",
    "        if not output.startswith(\"[\") or not output.endswith(\"]\"):\n",
    "            raise LookupError\n",
    "        output = output[1:-1]\n",
    "        output = [v.strip().split(\" \")[0].strip() for v in output.strip().split(\",\")]\n",
    "\n",
    "        while \"#UNK#\" in output:\n",
    "            output.pop(output.index(\"#UNK#\"))\n",
    "\n",
    "        cues.append(output)\n",
    "\n",
    "    overlap, i, j = check_for_overlap(cues)\n",
    "    while overlap:\n",
    "        cue_2 = cues.pop(j)\n",
    "        cue_1 = cues.pop(i)\n",
    "        cue_1.extend(cue_2)\n",
    "        cue_1 = list(set(cue_1))\n",
    "        cues.append(cue_1)\n",
    "\n",
    "        overlap, i, j = check_for_overlap(cues)\n",
    "\n",
    "    return cues\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_cues():\n",
    "    path = \"./output/data/\"\n",
    "    count_cues = 0\n",
    "\n",
    "    for file in sorted(os.listdir(path)):\n",
    "        if file.endswith(\".zip\"):\n",
    "            continue\n",
    "        file_content = {}\n",
    "\n",
    "        with open(os.path.join(path, file), \"r\") as f:\n",
    "            file_content = json.load(f)\n",
    "            file_content[\"Outputs\"][\"Cues_text\"] = {}\n",
    "\n",
    "            for id, output in file_content[\"Outputs\"][\"Cues\"].items():\n",
    "                try:\n",
    "                    cues = extract_cues_from_output(output[0])\n",
    "                # output does not start with \"Cues: \"\n",
    "                except SystemError:\n",
    "                    cues = []\n",
    "                # output not in [...] format\n",
    "                except LookupError:\n",
    "                    cues = []\n",
    "\n",
    "                count_cues += len(cues)\n",
    "                file_content[\"Outputs\"][\"Cues_text\"][id] = cues\n",
    "\n",
    "        with open(os.path.join(path, file), \"w\", encoding=\"utf8\") as outfile:\n",
    "            json.dump(file_content, outfile, indent=3)\n",
    "\n",
    "    return count_cues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_cues = extract_cues()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Inference for Roles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Load roles model for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e188db289f854992ae1bb0318021d621",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    roles_training_config[\"model_name_or_path\"],\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "checkpoint_dir = (\n",
    "    roles_training_config[\"output_dir\"] + \"/checkpoint-2000/\"\n",
    ")  # choose checkpoint\n",
    "model = PeftModel.from_pretrained(model, os.path.join(checkpoint_dir, \"adapter_model\"))\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\n",
    "    roles_training_config[\"model_name_or_path\"], legacy=False\n",
    ")\n",
    "tokenizer.bos_token_id = 1\n",
    "\n",
    "pipe = pipeline(\n",
    "    task=\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=300\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_for_roles(ds, prompt_cues, roles_prompt):\n",
    "    path = \"./output/data/\"\n",
    "\n",
    "    pbar = tqdm(total=count_cues, desc=\"Roles\")\n",
    "    for file in sorted(os.listdir(path)):\n",
    "        if file.endswith(\".zip\"):\n",
    "            continue\n",
    "        file_content = {}\n",
    "\n",
    "        with open(os.path.join(path, file), \"r\") as f:\n",
    "            file_content = json.load(f)\n",
    "            file_content[\"Outputs\"][\"Roles\"] = {}\n",
    "\n",
    "            for id, cues in file_content[\"Outputs\"][\"Cues_text\"].items():\n",
    "                file_content[\"Outputs\"][\"Roles\"][id] = []\n",
    "\n",
    "                if cues == []:\n",
    "                    continue\n",
    "\n",
    "                sentence = ds.filter(\n",
    "                    lambda r: r[\"FileName\"] == file and r[\"SentenceId\"] == int(id)\n",
    "                )[0][\"Sentence\"]\n",
    "                text = ds.filter(\n",
    "                    lambda r: r[\"FileName\"] == file and r[\"SentenceId\"] == int(id)\n",
    "                )[0][\"sentence_extended\"]\n",
    "                if sentence.endswith(\":\"):\n",
    "                    sentence = sentence[:-1] + \".\"\n",
    "                if text.endswith(\":\"):\n",
    "                    text = text[:-1] + \".\"\n",
    "                cue_prompt = (\n",
    "                    prompt_cues.format(Satz=sentence)\n",
    "                    + \" Cues: \"\n",
    "                    + \", \".join([\"[\" + \", \".join(cue) + \"]\" for cue in cues])\n",
    "                    + \"</s>\"\n",
    "                )\n",
    "\n",
    "                for cue in cues:\n",
    "                    file_content[\"Outputs\"][\"Roles\"][id].append([])\n",
    "                    prompt = PromptTemplate(\n",
    "                        input_variables=[\"text\", \"cue\"],\n",
    "                        template=cue_prompt + \"\\nUser: \" + roles_prompt,\n",
    "                    )\n",
    "                    llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "                    output = llm_chain.apply([{\"text\": text, \"cue\": \", \".join(cue)}])[\n",
    "                        0\n",
    "                    ][\"text\"]\n",
    "                    file_content[\"Outputs\"][\"Roles\"][id][-1].append(output)\n",
    "                    pbar.update()\n",
    "\n",
    "        with open(os.path.join(path, file), \"w\", encoding=\"utf8\") as outfile:\n",
    "            json.dump(file_content, outfile, indent=3)\n",
    "\n",
    "    pbar.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "roles_prompt = \"Now I give you again the sentence only in addition with the two following sentences, because the roles can be partially contained in the following sentences.\\nText: {text}\\n\\nNow find all roles in the sentence associated with the cue '{cue}' you found in the beginning sentence.\\nAssistant:\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Roles: 100%|██████████| 1831/1831 [1:08:22<00:00,  2.24s/it]\n"
     ]
    }
   ],
   "source": [
    "prompt_for_roles(test_ds, prompt_cues, roles_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# free vram after inference for roles\n",
    "del tokenizer\n",
    "del model\n",
    "del pipe\n",
    "del llm\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Extract Roles and map outputs to tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Extract roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_roles_from_output(output_string: str):\n",
    "    res = {\n",
    "        \"ptc\": \"\",\n",
    "        \"evidence\": \"\",\n",
    "        \"medium\": \"\",\n",
    "        \"topic\": \"\",\n",
    "        \"addr\": \"\",\n",
    "        \"message\": \"\",\n",
    "        \"source\": \"\",\n",
    "    }\n",
    "\n",
    "    output_rows = [v.strip() for v in output_string.strip().split(\"\\n\")]\n",
    "\n",
    "    try:\n",
    "        if output_rows[1].startswith(\"ptc: \"):\n",
    "            res[\"ptc\"] = [\n",
    "                v.strip().split(\" \")[0].strip()\n",
    "                for v in output_rows[1][4:].strip().split(\",\")\n",
    "            ]\n",
    "    except IndexError:\n",
    "        pass\n",
    "    try:\n",
    "        if output_rows[2].startswith(\"evidence: \"):\n",
    "            res[\"evidence\"] = [\n",
    "                v.strip().split(\" \")[0].strip()\n",
    "                for v in output_rows[2][9:].strip().split(\",\")\n",
    "            ]\n",
    "    except IndexError:\n",
    "        pass\n",
    "    try:\n",
    "        if output_rows[3].startswith(\"medium: \"):\n",
    "            res[\"medium\"] = [\n",
    "                v.strip().split(\" \")[0].strip()\n",
    "                for v in output_rows[3][7:].strip().split(\",\")\n",
    "            ]\n",
    "    except IndexError:\n",
    "        pass\n",
    "    try:\n",
    "        if output_rows[4].startswith(\"topic: \"):\n",
    "            res[\"topic\"] = [\n",
    "                v.strip().split(\" \")[0].strip()\n",
    "                for v in output_rows[4][6:].strip().split(\",\")\n",
    "            ]\n",
    "    except IndexError:\n",
    "        pass\n",
    "    try:\n",
    "        if output_rows[5].startswith(\"addr: \"):\n",
    "            res[\"addr\"] = [\n",
    "                v.strip().split(\" \")[0].strip()\n",
    "                for v in output_rows[5][5:].strip().split(\",\")\n",
    "            ]\n",
    "    except IndexError:\n",
    "        pass\n",
    "    try:\n",
    "        if output_rows[6].startswith(\"message: \"):\n",
    "            res[\"message\"] = [\n",
    "                v.strip().split(\" \")[0].strip()\n",
    "                for v in output_rows[6][8:].strip().split(\",\")\n",
    "            ]\n",
    "    except IndexError:\n",
    "        pass\n",
    "    try:\n",
    "        if output_rows[7].startswith(\"source: \"):\n",
    "            res[\"source\"] = [\n",
    "                v.strip().split(\" \")[0].strip()\n",
    "                for v in output_rows[7][7:].strip().split(\",\")\n",
    "            ]\n",
    "    except IndexError:\n",
    "        pass\n",
    "\n",
    "    for key, value in res.items():\n",
    "        if value == [\"\"] or value == [\"#UNK#\"]:\n",
    "            res[key] = \"\"\n",
    "        while \"#UNK#\" in value:\n",
    "            value.pop(value.index(\"#UNK#\"))\n",
    "        while type(value) == list and \"\" in value:\n",
    "            value.pop(value.index(\"\"))\n",
    "        res[key] = value\n",
    "\n",
    "    return res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_roles():\n",
    "    path = \"./output/data/\"\n",
    "\n",
    "    for file in sorted(os.listdir(path)):\n",
    "        if file.endswith(\".zip\"):\n",
    "            continue\n",
    "        file_content = {}\n",
    "\n",
    "        with open(os.path.join(path, file), \"r\") as f:\n",
    "            file_content = json.load(f)\n",
    "            file_content[\"Outputs\"][\"Roles_text\"] = {}\n",
    "\n",
    "            for id, roles_for_sentence in file_content[\"Outputs\"][\"Roles\"].items():\n",
    "                file_content[\"Outputs\"][\"Roles_text\"][id] = []\n",
    "\n",
    "                if roles_for_sentence == []:\n",
    "                    continue\n",
    "\n",
    "                for roles_output in roles_for_sentence:\n",
    "                    file_content[\"Outputs\"][\"Roles_text\"][id].append([])\n",
    "\n",
    "                    roles = extract_roles_from_output(roles_output[0])\n",
    "                    file_content[\"Outputs\"][\"Roles_text\"][id][-1].append(roles)\n",
    "\n",
    "        with open(os.path.join(path, file), \"w\", encoding=\"utf8\") as outfile:\n",
    "            json.dump(file_content, outfile, indent=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_roles()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_neighbors(i, seen, skip_index):\n",
    "    res = 0\n",
    "    if i - 2 >= 0 and i - 2 != skip_index:\n",
    "        res += 1 if seen[i - 2] else 0\n",
    "    if i - 1 >= 0 and i - 1 != skip_index:\n",
    "        res += 1 if seen[i - 1] else 0\n",
    "    if i + 1 < len(seen) and i + 1 != skip_index:\n",
    "        res += 1 if seen[i + 1] else 0\n",
    "    if i + 2 < len(seen) and i + 2 != skip_index:\n",
    "        res += 1 if seen[i + 2] else 0\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_neighborhood_swap(seen, tokens):\n",
    "    for i, v in enumerate(seen):\n",
    "        if not v:\n",
    "            continue\n",
    "\n",
    "        neigh_c_v = count_neighbors(i, seen, -1)\n",
    "        neigh = [\n",
    "            j\n",
    "            for j, t in enumerate(tokens)\n",
    "            if seen[j] == False and Levenshtein.distance(t, tokens[i]) <= 1\n",
    "        ]\n",
    "        neigh_c_other = [count_neighbors(n, seen, i) for n in neigh]\n",
    "        if len(neigh_c_other) > 0:\n",
    "            neigh_c_other_max = max(neigh_c_other)\n",
    "            if neigh_c_other_max > neigh_c_v:\n",
    "                return i, neigh[neigh_c_other.index(neigh_c_other_max)]\n",
    "\n",
    "    return -1, -1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_output_list(output_list: list, ids: list, tokens: list, seen_old=None):\n",
    "    res = []\n",
    "    seen = [False] * len(tokens)\n",
    "    if seen_old == None:\n",
    "        seen_old = [False] * len(tokens)\n",
    "\n",
    "    for output in output_list:\n",
    "        indices = [\n",
    "            i\n",
    "            for i, v in enumerate(tokens)\n",
    "            if v == output and seen[i] == False and seen_old[i] == False\n",
    "        ]\n",
    "        if len(indices) > 0:\n",
    "            seen[indices[0]] = True\n",
    "        if len(indices) == 0:\n",
    "            indices = [\n",
    "                i\n",
    "                for i, v in enumerate(tokens)\n",
    "                if seen[i] == False\n",
    "                and seen_old[i] == False\n",
    "                and Levenshtein.distance(output, v) <= 1\n",
    "            ]\n",
    "            if len(indices) > 0:\n",
    "                seen[indices[0]] = True\n",
    "\n",
    "    changed = True\n",
    "    while changed:\n",
    "        changed = False\n",
    "        i, j = calculate_neighborhood_swap(seen, tokens)\n",
    "        while i != j:\n",
    "            seen[i] = False\n",
    "            seen[j] = True\n",
    "            changed = True\n",
    "            i, j = calculate_neighborhood_swap(seen, tokens)\n",
    "\n",
    "        for i in range(len(seen)):\n",
    "            if (\n",
    "                seen[i] == False\n",
    "                and i != 0\n",
    "                and i != len(seen) - 1\n",
    "                and seen[i - 1]\n",
    "                and seen[i + 1]\n",
    "                and (\n",
    "                    tokens[i] == \",\"\n",
    "                    or tokens[i] == \":\"\n",
    "                    or tokens[i] == \";\"\n",
    "                    or tokens[i] == \"-\"\n",
    "                )\n",
    "            ):\n",
    "                seen[i] = True\n",
    "                changed = True\n",
    "\n",
    "    for i in range(len(seen)):\n",
    "        if seen[i]:\n",
    "            res.append(str(ids[i]) + \":\" + str(i))\n",
    "\n",
    "    return res, [v or seen_old[i] for i, v in enumerate(seen)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_outputs(ds):\n",
    "    path = \"./output/data/\"\n",
    "\n",
    "    for file in sorted(os.listdir(path)):\n",
    "        if file.endswith(\".zip\"):\n",
    "            continue\n",
    "        file_content = {}\n",
    "\n",
    "        with open(os.path.join(path, file), \"r\") as f:\n",
    "            file_content = json.load(f)\n",
    "            file_content[\"Annotations\"] = []\n",
    "\n",
    "            for cues_text, roles_text in zip(\n",
    "                file_content[\"Outputs\"][\"Cues_text\"].items(),\n",
    "                file_content[\"Outputs\"][\"Roles_text\"].items(),\n",
    "            ):\n",
    "                id, cues = cues_text\n",
    "                id, roles_list = roles_text\n",
    "\n",
    "                if cues == []:\n",
    "                    continue\n",
    "\n",
    "                tokens = ds.filter(\n",
    "                    lambda r: r[\"FileName\"] == file and r[\"SentenceId\"] == int(id)\n",
    "                )[0][\"tokens_extended\"]\n",
    "                ids = ds.filter(\n",
    "                    lambda r: r[\"FileName\"] == file and r[\"SentenceId\"] == int(id)\n",
    "                )[0][\"sentence_extended_ids\"]\n",
    "\n",
    "                seen_cues = None\n",
    "                for cue, roles in zip(cues, roles_list):\n",
    "                    roles = roles[0]\n",
    "\n",
    "                    cue, seen_cues = map_output_list(cue, ids, tokens, seen_cues)\n",
    "\n",
    "                    if cue != []:\n",
    "                        addr, _ = map_output_list(\n",
    "                            roles[\"addr\"],\n",
    "                            ids,\n",
    "                            tokens,\n",
    "                        )\n",
    "\n",
    "                        evidence, _ = map_output_list(\n",
    "                            roles[\"evidence\"],\n",
    "                            ids,\n",
    "                            tokens,\n",
    "                        )\n",
    "\n",
    "                        medium, _ = map_output_list(\n",
    "                            roles[\"medium\"],\n",
    "                            ids,\n",
    "                            tokens,\n",
    "                        )\n",
    "\n",
    "                        message, _ = map_output_list(\n",
    "                            roles[\"message\"],\n",
    "                            ids,\n",
    "                            tokens,\n",
    "                        )\n",
    "\n",
    "                        source, _ = map_output_list(\n",
    "                            roles[\"source\"],\n",
    "                            ids,\n",
    "                            tokens,\n",
    "                        )\n",
    "\n",
    "                        topic, _ = map_output_list(\n",
    "                            roles[\"topic\"],\n",
    "                            ids,\n",
    "                            tokens,\n",
    "                        )\n",
    "\n",
    "                        ptc, _ = map_output_list(\n",
    "                            roles[\"ptc\"],\n",
    "                            ids,\n",
    "                            tokens,\n",
    "                        )\n",
    "\n",
    "                        annotation = {\n",
    "                            \"Addr\": addr,\n",
    "                            \"Evidence\": evidence,\n",
    "                            \"Medium\": medium,\n",
    "                            \"Message\": message,\n",
    "                            \"Source\": source,\n",
    "                            \"Topic\": topic,\n",
    "                            \"Cue\": cue,\n",
    "                            \"PTC\": ptc,\n",
    "                        }\n",
    "                        file_content[\"Annotations\"].append(annotation)\n",
    "\n",
    "        with open(os.path.join(path, file), \"w\", encoding=\"utf8\") as outfile:\n",
    "            json.dump(file_content, outfile, indent=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_outputs(test_ds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Prepare zip file for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"./output/data/submission.zip\"):\n",
    "    os.remove(\"./output/data/submission.zip\")\n",
    "\n",
    "temp_path = \"./output/data/temp\"\n",
    "shutil.copytree(\"./output/data\", temp_path)\n",
    "\n",
    "for file in sorted(os.listdir(temp_path)):\n",
    "    file_content = {}\n",
    "\n",
    "    with open(os.path.join(temp_path, file), \"r\") as f:\n",
    "        file_content = json.load(f)\n",
    "        file_content.pop(\"Outputs\")\n",
    "\n",
    "    with open(os.path.join(temp_path, file), \"w\", encoding=\"utf8\") as outfile:\n",
    "        json.dump(file_content, outfile, indent=3)\n",
    "shutil.make_archive(temp_path, \"zip\", temp_path)\n",
    "shutil.move(\n",
    "    temp_path + \".zip\",\n",
    "    \"./output/data/submission.zip\",\n",
    ")\n",
    "shutil.rmtree(temp_path)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
