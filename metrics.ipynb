{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import logging as ds_log\n",
    "from transformers import logging as trans_log\n",
    "import warnings\n",
    "\n",
    "ds_log.set_verbosity_error()\n",
    "ds_log.disable_progress_bar()\n",
    "trans_log.set_verbosity_error()\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Switch val and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_test_dataset = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Features, Sequence, Value\n",
    "\n",
    "\n",
    "def read_annotations_from_file(path: str, file: str):\n",
    "    features = Features(\n",
    "        {\n",
    "            \"PTC\": Sequence(feature=Value(dtype=\"string\", id=None), length=-1, id=None),\n",
    "            \"Evidence\": Sequence(\n",
    "                feature=Value(dtype=\"string\", id=None), length=-1, id=None\n",
    "            ),\n",
    "            \"Medium\": Sequence(\n",
    "                feature=Value(dtype=\"string\", id=None), length=-1, id=None\n",
    "            ),\n",
    "            \"Topic\": Sequence(\n",
    "                feature=Value(dtype=\"string\", id=None), length=-1, id=None\n",
    "            ),\n",
    "            \"Cue\": Sequence(feature=Value(dtype=\"string\", id=None), length=-1, id=None),\n",
    "            \"Addr\": Sequence(\n",
    "                feature=Value(dtype=\"string\", id=None), length=-1, id=None\n",
    "            ),\n",
    "            \"Message\": Sequence(\n",
    "                feature=Value(dtype=\"string\", id=None), length=-1, id=None\n",
    "            ),\n",
    "            \"Source\": Sequence(\n",
    "                feature=Value(dtype=\"string\", id=None), length=-1, id=None\n",
    "            ),\n",
    "        }\n",
    "    )\n",
    "    ds = load_dataset(\n",
    "        \"json\",\n",
    "        data_files=os.path.join(path, file),\n",
    "        field=\"Annotations\",\n",
    "        split=\"train\",\n",
    "        features=features,\n",
    "    )\n",
    "    ds = ds.add_column(\"FileName\", [file] * len(ds))\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sentences_from_file(path: str, file: str):\n",
    "    ds = load_dataset(\n",
    "        \"json\", data_files=os.path.join(path, file), field=\"Sentences\", split=\"train\"\n",
    "    )\n",
    "    ds = ds.add_column(\"FileName\", [file] * len(ds))\n",
    "    ds = ds.add_column(\"Sentence\", [\" \".join(t) for t in ds[\"Tokens\"]])\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import concatenate_datasets\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def read_annotations_from_path(path: str):\n",
    "    dataset = None\n",
    "\n",
    "    for file in tqdm(sorted(os.listdir(path))):\n",
    "        if not dataset:\n",
    "            dataset = read_annotations_from_file(path, file)\n",
    "        else:\n",
    "            dataset = concatenate_datasets(\n",
    "                [dataset, read_annotations_from_file(path, file)]\n",
    "            )\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sentences_from_path(path: str):\n",
    "    dataset = None\n",
    "\n",
    "    for file in tqdm(sorted(os.listdir(path))):\n",
    "        if not dataset:\n",
    "            dataset = read_sentences_from_file(path, file)\n",
    "        else:\n",
    "            dataset = concatenate_datasets(\n",
    "                [dataset, read_sentences_from_file(path, file)]\n",
    "            )\n",
    "\n",
    "    dataset = dataset.add_column(\"id\", range(len(dataset)))\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "\n",
    "def read_annotations_train_dataset():\n",
    "    path_to_train_dataset = \"../../data/transformed_datasets/train/annotations\"\n",
    "\n",
    "    if os.path.isdir(path_to_train_dataset):\n",
    "        result = load_from_disk(path_to_train_dataset)\n",
    "    else:\n",
    "        result = read_annotations_from_path(\"../../data/train/\")\n",
    "        os.makedirs(path_to_train_dataset, exist_ok=True)\n",
    "        result.save_to_disk(path_to_train_dataset)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sentences_train_dataset():\n",
    "    path_to_train_dataset = \"../../data/transformed_datasets/train/sentences\"\n",
    "\n",
    "    if os.path.isdir(path_to_train_dataset):\n",
    "        result = load_from_disk(path_to_train_dataset)\n",
    "    else:\n",
    "        result = read_sentences_from_path(\"../../data/train/\")\n",
    "        os.makedirs(path_to_train_dataset, exist_ok=True)\n",
    "        result.save_to_disk(path_to_train_dataset)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_annotations_val_dataset():\n",
    "    path_to_val_dataset = \"../../data/transformed_datasets/val/annotations\"\n",
    "\n",
    "    if os.path.isdir(path_to_val_dataset):\n",
    "        return load_from_disk(path_to_val_dataset)\n",
    "\n",
    "    result = read_annotations_from_path(\"../../data/dev/\")\n",
    "    os.makedirs(path_to_val_dataset, exist_ok=True)\n",
    "    result.save_to_disk(path_to_val_dataset)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sentences_val_dataset():\n",
    "    path_to_val_dataset = \"../../data/transformed_datasets/val/sentences\"\n",
    "\n",
    "    if os.path.isdir(path_to_val_dataset):\n",
    "        return load_from_disk(path_to_val_dataset)\n",
    "\n",
    "    result = read_sentences_from_path(\"../../data/dev/\")\n",
    "    os.makedirs(path_to_val_dataset, exist_ok=True)\n",
    "    result.save_to_disk(path_to_val_dataset)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sentences_test_dataset():\n",
    "    path_to_test_dataset = \"../../data/transformed_datasets/test/sentences\"\n",
    "\n",
    "    if os.path.isdir(path_to_test_dataset):\n",
    "        return load_from_disk(path_to_test_dataset)\n",
    "\n",
    "    result = read_sentences_from_path(\"../../data/test/\")\n",
    "    os.makedirs(path_to_test_dataset, exist_ok=True)\n",
    "    result.save_to_disk(path_to_test_dataset)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences_dataset = read_sentences_train_dataset()\n",
    "val_sentences_dataset = read_sentences_val_dataset()\n",
    "test_sentences_dataset = read_sentences_test_dataset()\n",
    "train_annotations_dataset = read_annotations_train_dataset()\n",
    "val_annotations_dataset = read_annotations_val_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format datasets for usage in langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_from_label(train_sentences_dataset, row, annotations):\n",
    "    tokens = []\n",
    "    for anno in annotations:\n",
    "        if int(anno.split(\":\")[0]) == row[\"SentenceId\"]:\n",
    "            tokens.append(row[\"Tokens\"][int(anno.split(\":\")[1])])\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_complete_dataset(sentences_dataset, annotations_dataset, dataset_name):\n",
    "    path_to_dataset = (\n",
    "        \"../../data/transformed_datasets/\" + dataset_name + \"/complete-ext-2\"\n",
    "    )\n",
    "    if os.path.isdir(path_to_dataset):\n",
    "        return load_from_disk(path_to_dataset)\n",
    "\n",
    "    ptc, ptc_temp, ptc_mapped, ptc_mapped_temp = [], [], [], []\n",
    "    evidence, evidence_temp, evidence_mapped, evidence_mapped_temp = [], [], [], []\n",
    "    medium, medium_temp, medium_mapped, medium_mapped_temp = [], [], [], []\n",
    "    topic, topic_temp, topic_mapped, topic_mapped_temp = [], [], [], []\n",
    "    cue, cue_temp, cue_mapped, cue_mapped_temp = [], [], [], []\n",
    "    addr, addr_temp, addr_mapped, addr_mapped_temp = [], [], [], []\n",
    "    message, message_temp, message_mapped, message_mapped_temp = [], [], [], []\n",
    "    source, source_temp, source_mapped, source_mapped_temp = [], [], [], []\n",
    "    (\n",
    "        sentence_extended,\n",
    "        tokens_extended,\n",
    "        sentence_extended_ids,\n",
    "    ) = (\n",
    "        [],\n",
    "        [],\n",
    "        [],\n",
    "    )\n",
    "\n",
    "    index_in_anno_ds = 0\n",
    "\n",
    "    for i, row in tqdm(enumerate(sentences_dataset)):\n",
    "        context = row[\"Sentence\"]\n",
    "        tokens = row[\"Tokens\"]\n",
    "        ids = [row[\"SentenceId\"]] * len(row[\"Tokens\"])\n",
    "        if (\n",
    "            i + 1 < len(sentences_dataset)\n",
    "            and sentences_dataset[i + 1][\"FileName\"] == row[\"FileName\"]\n",
    "        ):\n",
    "            context = context + \" \" + sentences_dataset[i + 1][\"Sentence\"]\n",
    "            tokens.extend(sentences_dataset[i + 1][\"Tokens\"])\n",
    "            ids.extend(\n",
    "                [sentences_dataset[i + 1][\"SentenceId\"]]\n",
    "                * len(sentences_dataset[i + 1][\"Tokens\"])\n",
    "            )\n",
    "        if (\n",
    "            i + 2 < len(sentences_dataset)\n",
    "            and sentences_dataset[i + 2][\"FileName\"] == row[\"FileName\"]\n",
    "        ):\n",
    "            context = context + \" \" + sentences_dataset[i + 2][\"Sentence\"]\n",
    "            tokens.extend(sentences_dataset[i + 2][\"Tokens\"])\n",
    "            ids.extend(\n",
    "                [sentences_dataset[i + 2][\"SentenceId\"]]\n",
    "                * len(sentences_dataset[i + 2][\"Tokens\"])\n",
    "            )\n",
    "        sentence_extended.append(context)\n",
    "        tokens_extended.append(tokens)\n",
    "        sentence_extended_ids.append(ids)\n",
    "\n",
    "        if annotations_dataset is not None:\n",
    "            id_of_next_sentence_with_annotation = (\n",
    "                int(annotations_dataset[index_in_anno_ds][\"Cue\"][0].split(\":\")[0])\n",
    "                if index_in_anno_ds != len(annotations_dataset)\n",
    "                else -1\n",
    "            )\n",
    "\n",
    "            if row[\"SentenceId\"] != id_of_next_sentence_with_annotation:\n",
    "                ptc.append([])\n",
    "                ptc_mapped.append([])\n",
    "                evidence.append([])\n",
    "                evidence_mapped.append([])\n",
    "                medium.append([])\n",
    "                medium_mapped.append([])\n",
    "                topic.append([])\n",
    "                topic_mapped.append([])\n",
    "                cue.append([])\n",
    "                cue_mapped.append([])\n",
    "                addr.append([])\n",
    "                addr_mapped.append([])\n",
    "                message.append([])\n",
    "                message_mapped.append([])\n",
    "                source.append([])\n",
    "                source_mapped.append([])\n",
    "                continue\n",
    "\n",
    "            while row[\"SentenceId\"] == id_of_next_sentence_with_annotation:\n",
    "                ptc_temp.append(annotations_dataset[index_in_anno_ds][\"PTC\"])\n",
    "                evidence_temp.append(annotations_dataset[index_in_anno_ds][\"Evidence\"])\n",
    "                medium_temp.append(annotations_dataset[index_in_anno_ds][\"Medium\"])\n",
    "                topic_temp.append(annotations_dataset[index_in_anno_ds][\"Topic\"])\n",
    "                cue_temp.append(annotations_dataset[index_in_anno_ds][\"Cue\"])\n",
    "                addr_temp.append(annotations_dataset[index_in_anno_ds][\"Addr\"])\n",
    "                message_temp.append(annotations_dataset[index_in_anno_ds][\"Message\"])\n",
    "                source_temp.append(annotations_dataset[index_in_anno_ds][\"Source\"])\n",
    "\n",
    "                ptc_mapped_temp.append(\n",
    "                    get_text_from_label(sentences_dataset, row, ptc_temp[-1])\n",
    "                )\n",
    "                evidence_mapped_temp.append(\n",
    "                    get_text_from_label(sentences_dataset, row, evidence_temp[-1])\n",
    "                )\n",
    "                medium_mapped_temp.append(\n",
    "                    get_text_from_label(sentences_dataset, row, medium_temp[-1])\n",
    "                )\n",
    "                topic_mapped_temp.append(\n",
    "                    get_text_from_label(sentences_dataset, row, topic_temp[-1])\n",
    "                )\n",
    "                cue_mapped_temp.append(\n",
    "                    get_text_from_label(sentences_dataset, row, cue_temp[-1])\n",
    "                )\n",
    "                addr_mapped_temp.append(\n",
    "                    get_text_from_label(sentences_dataset, row, addr_temp[-1])\n",
    "                )\n",
    "                message_mapped_temp.append(\n",
    "                    get_text_from_label(sentences_dataset, row, message_temp[-1])\n",
    "                )\n",
    "                source_mapped_temp.append(\n",
    "                    get_text_from_label(sentences_dataset, row, source_temp[-1])\n",
    "                )\n",
    "\n",
    "                index_in_anno_ds += 1\n",
    "                if index_in_anno_ds == len(annotations_dataset):\n",
    "                    break\n",
    "                id_of_next_sentence_with_annotation = int(\n",
    "                    annotations_dataset[index_in_anno_ds][\"Cue\"][0].split(\":\")[0]\n",
    "                )\n",
    "\n",
    "            ptc.append(ptc_temp)\n",
    "            ptc_mapped.append(ptc_mapped_temp)\n",
    "            evidence.append(evidence_temp)\n",
    "            evidence_mapped.append(evidence_mapped_temp)\n",
    "            medium.append(medium_temp)\n",
    "            medium_mapped.append(medium_mapped_temp)\n",
    "            topic.append(topic_temp)\n",
    "            topic_mapped.append(topic_mapped_temp)\n",
    "            cue.append(cue_temp)\n",
    "            cue_mapped.append(cue_mapped_temp)\n",
    "            addr.append(addr_temp)\n",
    "            addr_mapped.append(addr_mapped_temp)\n",
    "            message.append(message_temp)\n",
    "            message_mapped.append(message_mapped_temp)\n",
    "            source.append(source_temp)\n",
    "            source_mapped.append(source_mapped_temp)\n",
    "\n",
    "            ptc_temp, ptc_mapped_temp = [], []\n",
    "            evidence_temp, evidence_mapped_temp = [], []\n",
    "            medium_temp, medium_mapped_temp = [], []\n",
    "            topic_temp, topic_mapped_temp = [], []\n",
    "            cue_temp, cue_mapped_temp = [], []\n",
    "            addr_temp, addr_mapped_temp = [], []\n",
    "            message_temp, message_mapped_temp = [], []\n",
    "            source_temp, source_mapped_temp = [], []\n",
    "\n",
    "    res = sentences_dataset.add_column(\"sentence_extended\", sentence_extended)\n",
    "    res = res.add_column(\"tokens_extended\", tokens_extended)\n",
    "    res = res.add_column(\"sentence_extended_ids\", sentence_extended_ids)\n",
    "\n",
    "    if annotations_dataset is not None:\n",
    "        res = res.add_column(\"ptc\", ptc)\n",
    "        res = res.add_column(\"ptc_mapped\", ptc_mapped)\n",
    "        res = res.add_column(\"evidence\", evidence)\n",
    "        res = res.add_column(\"evidence_mapped\", evidence_mapped)\n",
    "        res = res.add_column(\"medium\", medium)\n",
    "        res = res.add_column(\"medium_mapped\", medium_mapped)\n",
    "        res = res.add_column(\"topic\", topic)\n",
    "        res = res.add_column(\"topic_mapped\", topic_mapped)\n",
    "        res = res.add_column(\"cue\", cue)\n",
    "        res = res.add_column(\"cue_mapped\", cue_mapped)\n",
    "        res = res.add_column(\"addr\", addr)\n",
    "        res = res.add_column(\"addr_mapped\", addr_mapped)\n",
    "        res = res.add_column(\"message\", message)\n",
    "        res = res.add_column(\"message_mapped\", message_mapped)\n",
    "        res = res.add_column(\"source\", source)\n",
    "        res = res.add_column(\"source_mapped\", source_mapped)\n",
    "\n",
    "    os.makedirs(path_to_dataset, exist_ok=True)\n",
    "    res.save_to_disk(path_to_dataset)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9093it [00:12, 707.50it/s]\n",
      "927it [00:01, 708.27it/s]\n",
      "3067it [00:02, 1276.82it/s]\n"
     ]
    }
   ],
   "source": [
    "train_ds = build_complete_dataset(\n",
    "    train_sentences_dataset, train_annotations_dataset, \"train\"\n",
    ")\n",
    "val_ds = build_complete_dataset(val_sentences_dataset, val_annotations_dataset, \"val\")\n",
    "test_ds = build_complete_dataset(test_sentences_dataset, None, \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Role Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_prefix = \"qlora-exp008g-llama2-70b-specialized-models-external-context-2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_roles_from_output(output_string: str):\n",
    "    res = {\n",
    "        \"ptc\": \"\",\n",
    "        \"evidence\": \"\",\n",
    "        \"medium\": \"\",\n",
    "        \"topic\": \"\",\n",
    "        \"addr\": \"\",\n",
    "        \"message\": \"\",\n",
    "        \"source\": \"\",\n",
    "    }\n",
    "\n",
    "    output_rows = [v.strip() for v in output_string.strip().split(\"\\n\")]\n",
    "    error = False\n",
    "\n",
    "    try:\n",
    "        if not (\n",
    "            output_rows[1].startswith(\"ptc: \")\n",
    "            and output_rows[2].startswith(\"evidence: \")\n",
    "            and output_rows[3].startswith(\"medium: \")\n",
    "            and output_rows[4].startswith(\"topic: \")\n",
    "            and output_rows[5].startswith(\"addr: \")\n",
    "            and output_rows[6].startswith(\"message: \")\n",
    "            and output_rows[7].startswith(\"source: \")\n",
    "        ):\n",
    "            error = True\n",
    "    except IndexError:\n",
    "        error = True\n",
    "\n",
    "    try:\n",
    "        if output_rows[1].startswith(\"ptc: \"):\n",
    "            res[\"ptc\"] = [\n",
    "                v.strip().split(\" \")[0].strip()\n",
    "                for v in output_rows[1][4:].strip().split(\",\")\n",
    "            ]\n",
    "    except IndexError:\n",
    "        pass\n",
    "    try:\n",
    "        if output_rows[2].startswith(\"evidence: \"):\n",
    "            res[\"evidence\"] = [\n",
    "                v.strip().split(\" \")[0].strip()\n",
    "                for v in output_rows[2][9:].strip().split(\",\")\n",
    "            ]\n",
    "    except IndexError:\n",
    "        pass\n",
    "    try:\n",
    "        if output_rows[3].startswith(\"medium: \"):\n",
    "            res[\"medium\"] = [\n",
    "                v.strip().split(\" \")[0].strip()\n",
    "                for v in output_rows[3][7:].strip().split(\",\")\n",
    "            ]\n",
    "    except IndexError:\n",
    "        pass\n",
    "    try:\n",
    "        if output_rows[4].startswith(\"topic: \"):\n",
    "            res[\"topic\"] = [\n",
    "                v.strip().split(\" \")[0].strip()\n",
    "                for v in output_rows[4][6:].strip().split(\",\")\n",
    "            ]\n",
    "    except IndexError:\n",
    "        pass\n",
    "    try:\n",
    "        if output_rows[5].startswith(\"addr: \"):\n",
    "            res[\"addr\"] = [\n",
    "                v.strip().split(\" \")[0].strip()\n",
    "                for v in output_rows[5][5:].strip().split(\",\")\n",
    "            ]\n",
    "    except IndexError:\n",
    "        pass\n",
    "    try:\n",
    "        if output_rows[6].startswith(\"message: \"):\n",
    "            res[\"message\"] = [\n",
    "                v.strip().split(\" \")[0].strip()\n",
    "                for v in output_rows[6][8:].strip().split(\",\")\n",
    "            ]\n",
    "    except IndexError:\n",
    "        pass\n",
    "    try:\n",
    "        if output_rows[7].startswith(\"source: \"):\n",
    "            res[\"source\"] = [\n",
    "                v.strip().split(\" \")[0].strip()\n",
    "                for v in output_rows[7][7:].strip().split(\",\")\n",
    "            ]\n",
    "    except IndexError:\n",
    "        pass\n",
    "\n",
    "    for key, value in res.items():\n",
    "        if value == [\"\"] or value == [\"#UNK#\"]:\n",
    "            res[key] = \"\"\n",
    "        while \"#UNK#\" in value:\n",
    "            value.pop(value.index(\"#UNK#\"))\n",
    "        while type(value) == list and \"\" in value:\n",
    "            value.pop(value.index(\"\"))\n",
    "        res[key] = value\n",
    "\n",
    "    return res, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def extract_roles(output_filename_prefix):\n",
    "    path = \"./\" + output_filename_prefix + \"-outputs/\"\n",
    "    errors = {\"no_roles_prefix\": 0}\n",
    "\n",
    "    for file in sorted(os.listdir(path)):\n",
    "        if file.endswith(\".zip\"):\n",
    "            continue\n",
    "        file_content = {}\n",
    "\n",
    "        with open(os.path.join(path, file), \"r\") as f:\n",
    "            file_content = json.load(f)\n",
    "            file_content[\"Outputs\"][\"Roles_text\"] = {}\n",
    "\n",
    "            for id, roles_for_sentence in file_content[\"Outputs\"][\"Roles\"].items():\n",
    "                file_content[\"Outputs\"][\"Roles_text\"][id] = []\n",
    "\n",
    "                if roles_for_sentence == []:\n",
    "                    continue\n",
    "\n",
    "                for roles_output in roles_for_sentence:\n",
    "                    file_content[\"Outputs\"][\"Roles_text\"][id].append([])\n",
    "\n",
    "                    roles, error = extract_roles_from_output(roles_output[0])\n",
    "                    if error:\n",
    "                        errors[\"no_roles_prefix\"] += 1\n",
    "                    file_content[\"Outputs\"][\"Roles_text\"][id][-1].append(roles)\n",
    "\n",
    "        with open(os.path.join(path, file), \"w\", encoding=\"utf8\") as outfile:\n",
    "            json.dump(file_content, outfile, indent=3, ensure_ascii=False)\n",
    "\n",
    "    with open(output_filename_prefix + \"-errors.json\", \"r\", encoding=\"utf8\") as f:\n",
    "        file_content = json.load(f)\n",
    "        for key, value in file_content.items():\n",
    "            errors[key] = value\n",
    "    with open(output_filename_prefix + \"-errors.json\", \"w\", encoding=\"utf8\") as outfile:\n",
    "        json.dump(errors, outfile, indent=3, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_roles(file_name_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map model outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein\n",
    "\n",
    "\n",
    "def count_neighbors(i, seen, skip_index):\n",
    "    res = 0\n",
    "    if i - 2 >= 0 and i - 2 != skip_index:\n",
    "        res += 1 if seen[i - 2] else 0\n",
    "    if i - 1 >= 0 and i - 1 != skip_index:\n",
    "        res += 1 if seen[i - 1] else 0\n",
    "    if i + 1 < len(seen) and i + 1 != skip_index:\n",
    "        res += 1 if seen[i + 1] else 0\n",
    "    if i + 2 < len(seen) and i + 2 != skip_index:\n",
    "        res += 1 if seen[i + 2] else 0\n",
    "    return res\n",
    "\n",
    "\n",
    "def calculate_neighborhood_swap(seen, tokens):\n",
    "    for i, v in enumerate(seen):\n",
    "        if not v:\n",
    "            continue\n",
    "\n",
    "        neigh_c_v = count_neighbors(i, seen, -1)\n",
    "        neigh = [\n",
    "            j\n",
    "            for j, t in enumerate(tokens)\n",
    "            if seen[j] == False and Levenshtein.distance(t, tokens[i]) <= 1\n",
    "        ]\n",
    "        neigh_c_other = [count_neighbors(n, seen, i) for n in neigh]\n",
    "        if len(neigh_c_other) > 0:\n",
    "            neigh_c_other_max = max(neigh_c_other)\n",
    "            if neigh_c_other_max > neigh_c_v:\n",
    "                return i, neigh[neigh_c_other.index(neigh_c_other_max)]\n",
    "\n",
    "    return -1, -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_output_list(output_list: list, ids: list, tokens: list, seen_old=None):\n",
    "    res = []\n",
    "    seen = [False] * len(tokens)\n",
    "    if seen_old == None:\n",
    "        seen_old = [False] * len(tokens)\n",
    "    error = False\n",
    "\n",
    "    for output in output_list:\n",
    "        indices = [\n",
    "            i\n",
    "            for i, v in enumerate(tokens)\n",
    "            if v == output and seen[i] == False and seen_old[i] == False\n",
    "        ]\n",
    "        if len(indices) > 0:\n",
    "            seen[indices[0]] = True\n",
    "        if len(indices) == 0:\n",
    "            indices = [\n",
    "                i\n",
    "                for i, v in enumerate(tokens)\n",
    "                if seen[i] == False\n",
    "                and seen_old[i] == False\n",
    "                and Levenshtein.distance(output, v) <= 1\n",
    "            ]\n",
    "            if len(indices) > 0:\n",
    "                seen[indices[0]] = True\n",
    "\n",
    "    if sum(seen) != len(output_list):\n",
    "        error = True\n",
    "\n",
    "    changed = True\n",
    "    while changed:\n",
    "        changed = False\n",
    "        i, j = calculate_neighborhood_swap(seen, tokens)\n",
    "        while i != j:\n",
    "            seen[i] = False\n",
    "            seen[j] = True\n",
    "            changed = True\n",
    "            i, j = calculate_neighborhood_swap(seen, tokens)\n",
    "\n",
    "        for i in range(len(seen)):\n",
    "            if (\n",
    "                seen[i] == False\n",
    "                and i != 0\n",
    "                and i != len(seen) - 1\n",
    "                and seen[i - 1]\n",
    "                and seen[i + 1]\n",
    "                and (\n",
    "                    tokens[i] == \",\"\n",
    "                    or tokens[i] == \":\"\n",
    "                    or tokens[i] == \";\"\n",
    "                    or tokens[i] == \"-\"\n",
    "                )\n",
    "            ):\n",
    "                seen[i] = True\n",
    "                changed = True\n",
    "\n",
    "    for i in range(len(seen)):\n",
    "        if seen[i]:\n",
    "            res.append(str(ids[i]) + \":\" + str(i))\n",
    "\n",
    "    return res, error, [v or seen_old[i] for i, v in enumerate(seen)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_outputs(output_filename_prefix, ds):\n",
    "    path = \"./\" + output_filename_prefix + \"-outputs/\"\n",
    "    errors = {\"cue_not_mappable\": 0, \"roles_not_mappable\": 0}\n",
    "\n",
    "    for file in sorted(os.listdir(path)):\n",
    "        if file.endswith(\".zip\"):\n",
    "            continue\n",
    "        file_content = {}\n",
    "\n",
    "        with open(os.path.join(path, file), \"r\") as f:\n",
    "            file_content = json.load(f)\n",
    "            file_content[\"Annotations\"] = []\n",
    "\n",
    "            for cues_text, roles_text in zip(\n",
    "                file_content[\"Outputs\"][\"Cues_text\"].items(),\n",
    "                file_content[\"Outputs\"][\"Roles_text\"].items(),\n",
    "            ):\n",
    "                id, cues = cues_text\n",
    "                id, roles_list = roles_text\n",
    "\n",
    "                if cues == []:\n",
    "                    continue\n",
    "\n",
    "                tokens = ds.filter(\n",
    "                    lambda r: r[\"FileName\"] == file and r[\"SentenceId\"] == int(id)\n",
    "                )[0][\"tokens_extended\"]\n",
    "                ids = ds.filter(\n",
    "                    lambda r: r[\"FileName\"] == file and r[\"SentenceId\"] == int(id)\n",
    "                )[0][\"sentence_extended_ids\"]\n",
    "\n",
    "                seen_cues = None\n",
    "                for cue, roles in zip(cues, roles_list):\n",
    "                    roles = roles[0]\n",
    "\n",
    "                    cue, error, seen_cues = map_output_list(cue, ids, tokens, seen_cues)\n",
    "                    if error:\n",
    "                        errors[\"cue_not_mappable\"] += 1\n",
    "\n",
    "                    if cue != []:\n",
    "                        addr, error, _ = map_output_list(\n",
    "                            roles[\"addr\"],\n",
    "                            ids,\n",
    "                            tokens,\n",
    "                        )\n",
    "                        if error:\n",
    "                            errors[\"roles_not_mappable\"] += 1\n",
    "\n",
    "                        evidence, error, _ = map_output_list(\n",
    "                            roles[\"evidence\"],\n",
    "                            ids,\n",
    "                            tokens,\n",
    "                        )\n",
    "                        if error:\n",
    "                            errors[\"roles_not_mappable\"] += 1\n",
    "\n",
    "                        medium, error, _ = map_output_list(\n",
    "                            roles[\"medium\"],\n",
    "                            ids,\n",
    "                            tokens,\n",
    "                        )\n",
    "                        if error:\n",
    "                            errors[\"roles_not_mappable\"] += 1\n",
    "\n",
    "                        message, error, _ = map_output_list(\n",
    "                            roles[\"message\"],\n",
    "                            ids,\n",
    "                            tokens,\n",
    "                        )\n",
    "                        if error:\n",
    "                            errors[\"roles_not_mappable\"] += 1\n",
    "\n",
    "                        source, error, _ = map_output_list(\n",
    "                            roles[\"source\"],\n",
    "                            ids,\n",
    "                            tokens,\n",
    "                        )\n",
    "                        if error:\n",
    "                            errors[\"roles_not_mappable\"] += 1\n",
    "\n",
    "                        topic, error, _ = map_output_list(\n",
    "                            roles[\"topic\"],\n",
    "                            ids,\n",
    "                            tokens,\n",
    "                        )\n",
    "                        if error:\n",
    "                            errors[\"roles_not_mappable\"] += 1\n",
    "\n",
    "                        ptc, error, _ = map_output_list(\n",
    "                            roles[\"ptc\"],\n",
    "                            ids,\n",
    "                            tokens,\n",
    "                        )\n",
    "                        if error:\n",
    "                            errors[\"roles_not_mappable\"] += 1\n",
    "\n",
    "                        annotation = {\n",
    "                            \"Addr\": addr,\n",
    "                            \"Evidence\": evidence,\n",
    "                            \"Medium\": medium,\n",
    "                            \"Message\": message,\n",
    "                            \"Source\": source,\n",
    "                            \"Topic\": topic,\n",
    "                            \"Cue\": cue,\n",
    "                            \"PTC\": ptc,\n",
    "                        }\n",
    "                        file_content[\"Annotations\"].append(annotation)\n",
    "\n",
    "        with open(os.path.join(path, file), \"w\", encoding=\"utf8\") as outfile:\n",
    "            json.dump(file_content, outfile, indent=3, ensure_ascii=False)\n",
    "\n",
    "    with open(output_filename_prefix + \"-errors.json\", \"r\", encoding=\"utf8\") as f:\n",
    "        file_content = json.load(f)\n",
    "        for key, value in file_content.items():\n",
    "            errors[key] = value\n",
    "    with open(output_filename_prefix + \"-errors.json\", \"w\", encoding=\"utf8\") as outfile:\n",
    "        json.dump(errors, outfile, indent=3, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_outputs(file_name_prefix, test_ds if use_test_dataset else val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare zip file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "if os.path.exists(file_name_prefix + \"-outputs/\" + file_name_prefix + \".zip\"):\n",
    "    os.remove(file_name_prefix + \"-outputs/\" + file_name_prefix + \".zip\")\n",
    "shutil.copytree(\n",
    "    \"./\" + file_name_prefix + \"-outputs\", \"./\" + file_name_prefix + \"-outputs/temp\"\n",
    ")\n",
    "path = \"./\" + file_name_prefix + \"-outputs/temp\"\n",
    "\n",
    "for file in sorted(os.listdir(path)):\n",
    "    file_content = {}\n",
    "\n",
    "    with open(os.path.join(path, file), \"r\") as f:\n",
    "        file_content = json.load(f)\n",
    "        file_content.pop(\"Outputs\")\n",
    "\n",
    "    with open(os.path.join(path, file), \"w\", encoding=\"utf8\") as outfile:\n",
    "        json.dump(file_content, outfile, indent=3, ensure_ascii=False)\n",
    "shutil.make_archive(\n",
    "    file_name_prefix + \"-outputs/temp\", \"zip\", \"./\" + file_name_prefix + \"-outputs/temp\"\n",
    ")\n",
    "shutil.move(\n",
    "    file_name_prefix + \"-outputs/temp.zip\",\n",
    "    file_name_prefix + \"-outputs/\" + file_name_prefix + \".zip\",\n",
    ")\n",
    "shutil.rmtree(file_name_prefix + \"-outputs/temp\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(output_filename_prefix):\n",
    "    assert use_test_dataset == False\n",
    "\n",
    "    path = \"./\" + output_filename_prefix + \"-outputs/\"\n",
    "\n",
    "    result = {\n",
    "        \"f1\": 0,\n",
    "        \"precision\": 0,\n",
    "        \"recall\": 0,\n",
    "        \"f1_cues\": 0,\n",
    "        \"precision_cues\": 0,\n",
    "        \"recall_cues\": 0,\n",
    "        \"f1_roles\": 0,\n",
    "        \"precision_roles\": 0,\n",
    "        \"recall_roles\": 0,\n",
    "        # \"count_gold_cues\": 0,\n",
    "        # \"count_pred_cues\": 0,\n",
    "        # \"count_exact_match\": 0,\n",
    "        # \"count_partly_match\": 0,\n",
    "        # \"count_no_match\": 0,\n",
    "    }\n",
    "\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    tp_cues = 0\n",
    "    fp_cues = 0\n",
    "    fn_cues = 0\n",
    "    tp_roles = 0\n",
    "    fp_roles = 0\n",
    "    fn_roles = 0\n",
    "\n",
    "    roles_names = [\"Addr\", \"Evidence\", \"Medium\", \"Message\", \"Source\", \"Topic\", \"PTC\"]\n",
    "\n",
    "    for file in sorted(os.listdir(path)):\n",
    "        if file.endswith(\".zip\"):\n",
    "            continue\n",
    "        file_content = {}\n",
    "\n",
    "        with open(os.path.join(path, file), \"r\") as f:\n",
    "            file_content = json.load(f)\n",
    "\n",
    "            pred_cue = [val[\"Cue\"] for val in file_content[\"Annotations\"]]\n",
    "            pred_roles = [\n",
    "                [val[role] for val in file_content[\"Annotations\"]]\n",
    "                for role in roles_names\n",
    "            ]\n",
    "            gold_cue = [\n",
    "                val\n",
    "                for val in val_annotations_dataset.filter(\n",
    "                    lambda row: row[\"FileName\"] == file, load_from_cache_file=False\n",
    "                )[\"Cue\"]\n",
    "            ]\n",
    "            gold_roles = [\n",
    "                [\n",
    "                    val\n",
    "                    for val in val_annotations_dataset.filter(\n",
    "                        lambda row: row[\"FileName\"] == file, load_from_cache_file=False\n",
    "                    )[role]\n",
    "                ]\n",
    "                for role in roles_names\n",
    "            ]\n",
    "\n",
    "            # result[\"count_gold_cues\"] += len(gold)\n",
    "            # result[\"count_pred_cues\"] += len(pred)\n",
    "            # count_exact_match = len([p for p in pred if p in gold])\n",
    "            # count_partly_match = len(\n",
    "            #     [\n",
    "            #         list(set(i) & set(j))\n",
    "            #         for i in gold\n",
    "            #         for j in pred\n",
    "            #         if len(list(set(i) & set(j))) > 0\n",
    "            #         and len(list(set(i) & set(j))) != len(i)\n",
    "            #         and len(list(set(i) & set(j))) != len(j)\n",
    "            #     ]\n",
    "            # )\n",
    "            # result[\"count_exact_match\"] += count_exact_match\n",
    "            # result[\"count_partly_match\"] += count_partly_match\n",
    "            # result[\"count_no_match\"] += (\n",
    "            #     len(gold) - count_exact_match - count_partly_match\n",
    "            # )\n",
    "\n",
    "            pred_cue_flattend = [v for val in pred_cue for v in val]\n",
    "            pred_roles_flattend = [\n",
    "                v for role in pred_roles for val in role for v in val\n",
    "            ]\n",
    "            gold_cue_flattend = [v for val in gold_cue for v in val]\n",
    "            gold_roles_flattend = [\n",
    "                v for role in gold_roles for val in role for v in val\n",
    "            ]\n",
    "\n",
    "            tp_cues += len(list(set(pred_cue_flattend) & set(gold_cue_flattend)))\n",
    "            fp_cues += len(list(set(pred_cue_flattend) - set(gold_cue_flattend)))\n",
    "            fn_cues += len(list(set(gold_cue_flattend) - set(pred_cue_flattend)))\n",
    "\n",
    "            tp_roles += len(list(set(pred_roles_flattend) & set(gold_roles_flattend)))\n",
    "            fp_roles += len(list(set(pred_roles_flattend) - set(gold_roles_flattend)))\n",
    "            fn_roles += len(list(set(gold_roles_flattend) - set(pred_roles_flattend)))\n",
    "\n",
    "    tp = tp_cues + tp_roles\n",
    "    fp = fp_cues + fp_roles\n",
    "    fn = fn_cues + fn_roles\n",
    "\n",
    "    result[\"precision_cues\"] = (\n",
    "        tp_cues / (tp_cues + fp_cues) if (tp_cues + fp_cues) > 0 else 0\n",
    "    )\n",
    "    result[\"recall_cues\"] = (\n",
    "        tp_cues / (tp_cues + fn_cues) if (tp_cues + fp_cues) > 0 else 0\n",
    "    )\n",
    "    result[\"f1_cues\"] = (\n",
    "        (2 * result[\"precision_cues\"] * result[\"recall_cues\"])\n",
    "        / (result[\"precision_cues\"] + result[\"recall_cues\"])\n",
    "        if result[\"precision_cues\"] + result[\"recall_cues\"] > 0\n",
    "        else 0\n",
    "    )\n",
    "    result[\"precision_roles\"] = (\n",
    "        tp_roles / (tp_roles + fp_roles) if (tp_roles + fp_roles) > 0 else 0\n",
    "    )\n",
    "    result[\"recall_roles\"] = (\n",
    "        tp_roles / (tp_roles + fn_roles) if (tp_roles + fp_roles) > 0 else 0\n",
    "    )\n",
    "    result[\"f1_roles\"] = (\n",
    "        (2 * result[\"precision_roles\"] * result[\"recall_roles\"])\n",
    "        / (result[\"precision_roles\"] + result[\"recall_roles\"])\n",
    "        if result[\"precision_roles\"] + result[\"recall_roles\"] > 0\n",
    "        else 0\n",
    "    )\n",
    "    result[\"precision\"] = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    result[\"recall\"] = tp / (tp + fn) if (tp + fp) > 0 else 0\n",
    "    result[\"f1\"] = (\n",
    "        (2 * result[\"precision\"] * result[\"recall\"])\n",
    "        / (result[\"precision\"] + result[\"recall\"])\n",
    "        if result[\"precision\"] + result[\"recall\"] > 0\n",
    "        else 0\n",
    "    )\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = compute_metrics(file_name_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_cues:\t 0.8658865886588658\n",
      "precision_cues:\t 0.8635547576301615\n",
      "recall_cues:\t 0.868231046931408\n",
      "\n",
      "f1_roles:\t 0.8447427293064877\n",
      "precision_roles: 0.8740740740740741\n",
      "recall_roles:\t 0.8173160173160173\n",
      "\n",
      "f1:\t\t 0.8477482088024564\n",
      "precision:\t 0.8725309454832763\n",
      "recall:\t\t 0.8243344115451605\n"
     ]
    }
   ],
   "source": [
    "print(\"f1_cues:\\t\", metrics[\"f1_cues\"])\n",
    "print(\"precision_cues:\\t\", metrics[\"precision_cues\"])\n",
    "print(\"recall_cues:\\t\", metrics[\"recall_cues\"])\n",
    "print()\n",
    "print(\"f1_roles:\\t\", metrics[\"f1_roles\"])\n",
    "print(\"precision_roles:\", metrics[\"precision_roles\"])\n",
    "print(\"recall_roles:\\t\", metrics[\"recall_roles\"])\n",
    "print()\n",
    "print(\"f1:\\t\\t\", metrics[\"f1\"])\n",
    "print(\"precision:\\t\", metrics[\"precision\"])\n",
    "print(\"recall:\\t\\t\", metrics[\"recall\"])\n",
    "# print(\"count_gold_cues:\\t\", metrics[\"count_gold_cues\"])\n",
    "# print(\"count_pred_cues:\\t\", metrics[\"count_pred_cues\"])\n",
    "# print(\"count_exact_match:\\t\", metrics[\"count_exact_match\"])\n",
    "# print(\"count_partly_match:\\t\", metrics[\"count_partly_match\"])\n",
    "# print(\"count_no_match:\\t\\t\", metrics[\"count_no_match\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "germeval2023-7cQwAZyP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
